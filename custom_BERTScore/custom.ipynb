{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\orteg\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\orteg\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "c:\\Users\\orteg\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "C:\\Users\\orteg\\AppData\\Roaming\\Python\\Python38\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors\n",
    "from datetime import datetime\n",
    "import bert_score\n",
    "from custom_score.score import score\n",
    "from custom_score.utils import model_load"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I - Custom BERTScore Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = [\"Mon prenom est marius\"]\n",
    "can = [\"Je suis marius\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Token Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_load(model):\n",
    "    assert(type(model) == str)\n",
    "    if model == \"Word2Vec\":\n",
    "        wordvector_path = r'D:\\COURS\\A4\\S8\\Stage\\Documents\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\custom_BERTScore\\GoogleNews-vectors-negative300.bin.gz'\n",
    "        emb = KeyedVectors.load_word2vec_format(wordvector_path, binary=True)\n",
    "    if model == \"Glove\":\n",
    "        glove_path = r'D:\\COURS\\A4\\S8\\Stage\\Documents\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\custom_BERTScore\\glove2word2vec.txt'\n",
    "        emb = KeyedVectors.load_word2vec_format(glove_path)\n",
    "    else:\n",
    "        print(\"Model not currently supported\")\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(corpus, model):\n",
    "    encoded_corpus = []\n",
    "    unknown = 0\n",
    "    for sentence in corpus:\n",
    "        encoded_sentence = []\n",
    "        for word in sentence.split(\" \"):\n",
    "            try:\n",
    "                encoded_sentence.append(model[word])\n",
    "            except:\n",
    "                unknown += 1\n",
    "        encoded_corpus.append(encoded_sentence)\n",
    "    return np.array(encoded_corpus, dtype=object), unknown"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = model_load(\"Word2Vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#references, n_unknown_ref = encode([\"I am Marius\", \"I like trains\"], w2v)\n",
    "#candidates, n_unknown_cand = encode([\"My name is Marius\", \"I enjoy rail vehicules\"], w2v)\n",
    "\n",
    "references, n_unknown_ref = encode([\"I am Marius\"], w2v)\n",
    "candidates, n_unknown_cand = encode([\"My name is Marius\"], w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Reference :  (1, 3, 300) || Unknown Token Reference :  0\n",
      "Shape Candidate :  (1, 4, 300) || Unknown Token Candidate :  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape Reference : \", references.shape, \"||\", \"Unknown Token Reference : \", n_unknown_ref)\n",
    "print(\"Shape Candidate : \", candidates.shape, \"||\", \"Unknown Token Candidate : \", n_unknown_cand)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fasttext"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversition to word2vec format (Do not compile unless necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = fasttext.load_model(r'D:\\COURS\\A4\\S8\\Stage\\Documents\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\custom_BERTScore\\wiki.en\\wiki.en.bin')\n",
    "kv = KeyedVectors(vector_size=ft_model.get_dimension())\n",
    "all_words = ft_model.get_words()\n",
    "all_vectors = [ft_model.get_word_vector(w) for w in all_words]\n",
    "kv.add_vectors(all_words, all_vectors)\n",
    "kv.save_word2vec_format('ftwords.txt', binary=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading of formated fasttext file :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext = KeyedVectors.load_word2vec_format(r'D:\\COURS\\A4\\S8\\Stage\\Documents\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\custom_BERTScore\\ftwords.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext = KeyedVectors.load_word2vec_format(r'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.75779974,  0.5959443 , -0.27542254,  0.24059881, -0.5490175 ,\n",
       "        0.8300641 , -0.41410735,  0.7523287 ,  0.50460875, -0.86938876],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext[\"house\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Glove"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversition to word2vec format (Do not compile unless necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = r\"D:\\COURS\\A4\\S8\\Stage\\Documents\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\custom_BERTScore\\glove.6B\\glove.6B.300d.txt\"\n",
    "glove_temp = KeyedVectors.load_word2vec_format(glove_path, no_header=True)\n",
    "glove_temp.save_word2vec_format(r\"D:\\COURS\\A4\\S8\\Stage\\Documents\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\custom_BERTScore\\glove2word2vec.txt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading of formated glove file :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = KeyedVectors.load_word2vec_format(r\"D:\\COURS\\A4\\S8\\Stage\\Documents\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\custom_BERTScore\\glove2word2vec.txt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Similarity Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SimilarityCandToRef(references, candidates):\n",
    "    proximity = lambda x, y: (np.matmul(np.transpose(x), y))/(norm(x)*norm(y))\n",
    "\n",
    "    all_proximities = []\n",
    "\n",
    "    for candidate, reference in zip(candidates, references):\n",
    "        proximities = []\n",
    "        for c_word in candidate:\n",
    "            sub_proximities = []\n",
    "            for r_word in reference:\n",
    "                sub_proximities.append(proximity(r_word, c_word))\n",
    "            proximities.append(sub_proximities)\n",
    "        all_proximities.append(proximities)\n",
    "    return all_proximities\n",
    "\n",
    "def SimilarityRefToCand(references, candidates):\n",
    "    proximity = lambda x, y: (np.matmul(np.transpose(x), y))/(norm(x)*norm(y))\n",
    "\n",
    "    all_proximities = []\n",
    "\n",
    "    for candidate, reference in zip(candidates, references):\n",
    "        proximities = []\n",
    "        for r_word in reference:\n",
    "            sub_proximities = []\n",
    "            for c_word in candidate:\n",
    "                sub_proximities.append(proximity(r_word, c_word))\n",
    "            proximities.append(sub_proximities)\n",
    "        all_proximities.append(proximities)\n",
    "    return all_proximities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "candToRef = SimilarityCandToRef(references, candidates)\n",
    "refToCand = SimilarityRefToCand(references, candidates)\n",
    "refToCand2 = np.transpose(candToRef) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0.519005613856723, 0.18616442214576517, 0.04520518544899319],\n",
       "  [0.09242192671658575, 0.012498354833030686, 0.025547976598766665],\n",
       "  [0.2098326692090991, 0.3489852938028225, -0.00109822157119661],\n",
       "  [0.0368609406903856, 0.020396980932637056, 1.0]]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candToRef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0.519005613856723,\n",
       "   0.09242192671658575,\n",
       "   0.2098326692090991,\n",
       "   0.0368609406903856],\n",
       "  [0.18616442214576517,\n",
       "   0.012498354833030686,\n",
       "   0.3489852938028225,\n",
       "   0.020396980932637056],\n",
       "  [0.04520518544899319, 0.025547976598766665, -0.00109822157119661, 1.0]]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refToCand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.51900561],\n",
       "        [ 0.09242193],\n",
       "        [ 0.20983267],\n",
       "        [ 0.03686094]],\n",
       "\n",
       "       [[ 0.18616442],\n",
       "        [ 0.01249835],\n",
       "        [ 0.34898529],\n",
       "        [ 0.02039698]],\n",
       "\n",
       "       [[ 0.04520519],\n",
       "        [ 0.02554798],\n",
       "        [-0.00109822],\n",
       "        [ 1.        ]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refToCand2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Calculation of P, R and F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### R calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullSum = []\n",
    "for individualSimilarity in candToRef:\n",
    "    currentSum = 0\n",
    "    for row in individualSimilarity:\n",
    "        currentSum += row[np.argmax(row)]\n",
    "    fullSum.append(currentSum)\n",
    "R = []\n",
    "for sum, reference in zip(fullSum, references):\n",
    "    R.append((1/norm(reference))*sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.42425704559238325]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### P Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullSum = []\n",
    "for individualSimilarity in refToCand:\n",
    "    currentSum = 0\n",
    "    for row in individualSimilarity:\n",
    "        currentSum += row[np.argmax(row)]\n",
    "    fullSum.append(currentSum)\n",
    "P = []\n",
    "for sum, candidate in zip(fullSum, candidates):\n",
    "    P.append((1/norm(candidate))*sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3604158590807159]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### F Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = []\n",
    "\n",
    "for r, p in zip(R, P):\n",
    "    f = 2*((p*r)/(p+r))\n",
    "    F.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.38973938477441966]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Full Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeMetrics(candToRef, refToCand):\n",
    "    # R computation\n",
    "    fullSum = []\n",
    "    for individualSimilarity in candToRef:\n",
    "        currentSum = 0\n",
    "        for row in individualSimilarity:\n",
    "            currentSum += row[np.argmax(row)]\n",
    "        fullSum.append(currentSum)\n",
    "    R = []\n",
    "    for sum, reference in zip(fullSum, references):\n",
    "        R.append((1/norm(reference))*sum)\n",
    "\n",
    "    # P computation\n",
    "    fullSum = []\n",
    "    for individualSimilarity in refToCand:\n",
    "        currentSum = 0\n",
    "        for row in individualSimilarity:\n",
    "            currentSum += row[np.argmax(row)]\n",
    "        fullSum.append(currentSum)\n",
    "    P = []\n",
    "    for sum, candidate in zip(fullSum, candidates):\n",
    "        P.append((1/norm(candidate))*sum)\n",
    "    \n",
    "    # F computation\n",
    "    F = []\n",
    "    for r, p in zip(R, P):\n",
    "        f = 2*((p*r)/(p+r))\n",
    "        F.append(f)\n",
    "    \n",
    "    return (R, R, F)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Basic Test of the package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_score.score import score\n",
    "from custom_score.utils import model_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = model_load(\"Word2Vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.40998854847735094], [0.22880340017752968], [0.2937005518713847])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.03954665010154433], [0.05171590963278553], [0.04481993467824077])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(glove)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II - Billsum Dataset Test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0 - Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DynamicEmbeddingSampleTest(data, limit=3, modelPath = None, model = None, nbLayers = 24):\n",
    "    nbIter = 1\n",
    "    scores = []\n",
    "    init_time = datetime.now()\n",
    "    for row in data.iterrows():\n",
    "        curCand = [row[1][2]]\n",
    "        curRef = row[1][1]\n",
    "        curCand = [\" \".join(row[1][2].split(\"\\n\"))]\n",
    "        curRef = [\" \".join(row[1][1].split(\"\\n\"))]\n",
    "        assert len(curCand) == len(curRef)\n",
    "        if modelPath != None:\n",
    "            (P, R, F), hashname = bert_score.score(curCand, curRef, lang=\"en\", \n",
    "                                        model_type=modelPath, \n",
    "                                        num_layers=nbLayers, return_hash=True)\n",
    "        elif model != None:\n",
    "            (P, R, F), hashname = bert_score.score(curCand, curRef, lang=\"en\", \n",
    "                                        model_type=model, \n",
    "                                        num_layers=nbLayers, return_hash=True)\n",
    "        else:\n",
    "            (P, R, F), hashname = bert_score.score(curCand, curRef, lang=\"en\", return_hash=True)\n",
    "        P = P[0].item()\n",
    "        R = R[0].item()\n",
    "        F = F[0].item()\n",
    "        scores.append((P, R, F))\n",
    "        if nbIter >= limit:\n",
    "            break\n",
    "        nbIter += 1\n",
    "    runtime = (datetime.now() - init_time).total_seconds()\n",
    "    return scores, runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StaticEmbeddingSampleTest(data, model, limit=3):\n",
    "    nbIter = 1\n",
    "    scores = []\n",
    "    init_time = datetime.now()\n",
    "    for row in data.iterrows():\n",
    "        curCand = [row[1][2]]\n",
    "        curRef = row[1][1]\n",
    "        curCand = [\" \".join(row[1][2].split(\"\\n\"))]\n",
    "        curRef = [\" \".join(row[1][1].split(\"\\n\"))]\n",
    "        assert len(curCand) == len(curRef)\n",
    "        \n",
    "        (P, R, F) = score(model, curCand, curRef)\n",
    "        P = P[0]\n",
    "        R = R[0]\n",
    "        F = F[0]\n",
    "        scores.append((P, R, F))\n",
    "\n",
    "        if nbIter >= limit:\n",
    "            break\n",
    "        nbIter += 1\n",
    "    runtime = (datetime.now() - init_time).total_seconds()\n",
    "    return scores, runtime\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Load Benchmarking Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "billsumPath = r\"D:\\COURS\\A4\\S8\\Stage\\Documents\\Datasets\\Summary Evaluation\\BillSum\\corpus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "billsum_train = pd.read_json(path_or_buf = billsumPath + r\"\\us_train_data_final_OFFICIAL.jsonl\", lines=True)\n",
    "billsum_test = pd.read_json(path_or_buf = billsumPath + r\"\\us_test_data_final_OFFICIAL.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bill_id</th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "      <th>text_len</th>\n",
       "      <th>sum_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>110_hr37</td>\n",
       "      <td>SECTION 1. SHORT TITLE.\\n\\n    This Act may be...</td>\n",
       "      <td>National Science Education Tax Incentive for B...</td>\n",
       "      <td>To amend the Internal Revenue Code of 1986 to ...</td>\n",
       "      <td>8494</td>\n",
       "      <td>321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>112_hr2873</td>\n",
       "      <td>SECTION 1. SHORT TITLE.\\n\\n    This Act may be...</td>\n",
       "      <td>Small Business Expansion and Hiring Act of 201...</td>\n",
       "      <td>To amend the Internal Revenue Code of 1986 to ...</td>\n",
       "      <td>6522</td>\n",
       "      <td>1424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>109_s2408</td>\n",
       "      <td>SECTION 1. RELEASE OF DOCUMENTS CAPTURED IN IR...</td>\n",
       "      <td>Requires the Director of National Intelligence...</td>\n",
       "      <td>A bill to require the Director of National Int...</td>\n",
       "      <td>6154</td>\n",
       "      <td>463</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      bill_id                                               text   \n",
       "0    110_hr37  SECTION 1. SHORT TITLE.\\n\\n    This Act may be...  \\\n",
       "1  112_hr2873  SECTION 1. SHORT TITLE.\\n\\n    This Act may be...   \n",
       "2   109_s2408  SECTION 1. RELEASE OF DOCUMENTS CAPTURED IN IR...   \n",
       "\n",
       "                                             summary   \n",
       "0  National Science Education Tax Incentive for B...  \\\n",
       "1  Small Business Expansion and Hiring Act of 201...   \n",
       "2  Requires the Director of National Intelligence...   \n",
       "\n",
       "                                               title  text_len  sum_len  \n",
       "0  To amend the Internal Revenue Code of 1986 to ...      8494      321  \n",
       "1  To amend the Internal Revenue Code of 1986 to ...      6522     1424  \n",
       "2  A bill to require the Director of National Int...      6154      463  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "billsum_test.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Test : RoBERTa Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_scores, bert_runtime = DynamicEmbeddingSampleTest(billsum_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Test : Word2Vec Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = w2v = model_load(\"Word2Vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_scores, word2vec_runtime = StaticEmbeddingSampleTest(billsum_test, w2v)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Test : Fasttext Implementation "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - Test : Glove Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = KeyedVectors.load_word2vec_format(r\"D:\\COURS\\A4\\S8\\Stage\\Documents\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\custom_BERTScore\\glove2word2vec.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_scores, glove_runtime = StaticEmbeddingSampleTest(billsum_test, glove)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 - Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Runtime Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>runtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Roberta-24-layers</th>\n",
       "      <td>38.338537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word2Vec</th>\n",
       "      <td>4.411479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Glove</th>\n",
       "      <td>4.890593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     runtime\n",
       "Roberta-24-layers  38.338537\n",
       "Word2Vec            4.411479\n",
       "Glove               4.890593"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runtimeTable = [bert_runtime, word2vec_runtime, glove_runtime]\n",
    "runtimeDf = pd.DataFrame(runtimeTable, columns=[\"runtime\"], index=[\"Roberta-24-layers\", \"Word2Vec\", \"Glove\"])\n",
    "runtimeDf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bert_P</th>\n",
       "      <th>Bert_R</th>\n",
       "      <th>Bert_F</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.874369</td>\n",
       "      <td>0.704338</td>\n",
       "      <td>0.780197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.846002</td>\n",
       "      <td>0.728316</td>\n",
       "      <td>0.782760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.873844</td>\n",
       "      <td>0.702725</td>\n",
       "      <td>0.778998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Bert_P    Bert_R    Bert_F\n",
       "0  0.874369  0.704338  0.780197\n",
       "1  0.846002  0.728316  0.782760\n",
       "2  0.873844  0.702725  0.778998"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_scores_Df = pd.DataFrame(bert_scores, columns=[\"Bert_P\", \"Bert_R\", \"Bert_F\"])\n",
    "bert_scores_Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>W2V_P</th>\n",
       "      <th>W2V_R</th>\n",
       "      <th>W2V_F</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.470343</td>\n",
       "      <td>0.195813</td>\n",
       "      <td>0.276509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.077914</td>\n",
       "      <td>0.329076</td>\n",
       "      <td>0.594583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.762733</td>\n",
       "      <td>0.193866</td>\n",
       "      <td>0.309154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      W2V_P     W2V_R     W2V_F\n",
       "0  0.470343  0.195813  0.276509\n",
       "1  3.077914  0.329076  0.594583\n",
       "2  0.762733  0.193866  0.309154"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_scores_Df = pd.DataFrame(word2vec_scores, columns=[\"W2V_P\", \"W2V_R\", \"W2V_F\"])\n",
    "word2vec_scores_Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GLV_P</th>\n",
       "      <th>GLV_R</th>\n",
       "      <th>GLV_F</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.193759</td>\n",
       "      <td>-0.170188</td>\n",
       "      <td>-2.798021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.496544</td>\n",
       "      <td>-0.422760</td>\n",
       "      <td>-1.178411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.309721</td>\n",
       "      <td>-0.217137</td>\n",
       "      <td>-1.452770</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      GLV_P     GLV_R     GLV_F\n",
       "0  0.193759 -0.170188 -2.798021\n",
       "1  1.496544 -0.422760 -1.178411\n",
       "2  0.309721 -0.217137 -1.452770"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_scores_Df = pd.DataFrame(glove_scores, columns=[\"GLV_P\", \"GLV_R\", \"GLV_F\"])\n",
    "glove_scores_Df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOLDALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding\n",
    "new_references = []\n",
    "new_candidates = []\n",
    "\n",
    "for reference, candidate in zip(references, candidates):\n",
    "    size_diff = len(reference) - len(candidate)\n",
    "    if size_diff >= 0:\n",
    "        candidate = np.pad(candidate, (0, size_diff))\n",
    "        reference = np.array(reference)\n",
    "    else:\n",
    "        reference = np.pad(reference, [(0, np.abs(size_diff)), (0, 0)], mode=\"constant\")\n",
    "        candidate = np.array(candidate)\n",
    "    new_references.append(reference)\n",
    "    new_candidates.append(candidate)\n",
    "   \n",
    "\n",
    "references = np.array(new_references, dtype=object)\n",
    "candidates = np.array(new_candidates, dtype=object)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
