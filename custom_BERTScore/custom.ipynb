{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\orteg\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\orteg\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "c:\\Users\\orteg\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "C:\\Users\\orteg\\AppData\\Roaming\\Python\\Python38\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors\n",
    "from datetime import datetime\n",
    "import bert_score\n",
    "from custom_score.score import score\n",
    "from custom_score.utils import model_load\n",
    "import pickle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I - Custom BERTScore Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = [\"Mon prenom est marius\"]\n",
    "can = [\"Je suis marius\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Token Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_load(model):\n",
    "    assert(type(model) == str)\n",
    "    if model == \"Word2Vec\":\n",
    "        wordvector_path = r'D:\\COURS\\A4\\S8\\Stage\\Documents\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\custom_BERTScore\\GoogleNews-vectors-negative300.bin.gz'\n",
    "        emb = KeyedVectors.load_word2vec_format(wordvector_path, binary=True)\n",
    "    if model == \"Glove\":\n",
    "        glove_path = r'D:\\COURS\\A4\\S8\\Stage\\Documents\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\custom_BERTScore\\glove2word2vec.txt'\n",
    "        emb = KeyedVectors.load_word2vec_format(glove_path)\n",
    "    else:\n",
    "        print(\"Model not currently supported\")\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(corpus, model):\n",
    "    encoded_corpus = []\n",
    "    unknown = 0\n",
    "    for sentence in corpus:\n",
    "        encoded_sentence = []\n",
    "        for word in sentence.split(\" \"):\n",
    "            try:\n",
    "                encoded_sentence.append(model[word])\n",
    "            except:\n",
    "                unknown += 1\n",
    "        encoded_corpus.append(encoded_sentence)\n",
    "    return np.array(encoded_corpus, dtype=object), unknown"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = model_load(\"Word2Vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#references, n_unknown_ref = encode([\"I am Marius\", \"I like trains\"], w2v)\n",
    "#candidates, n_unknown_cand = encode([\"My name is Marius\", \"I enjoy rail vehicules\"], w2v)\n",
    "\n",
    "references, n_unknown_ref = encode([\"I am Marius\"], w2v)\n",
    "candidates, n_unknown_cand = encode([\"My name is Marius\"], w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Reference :  (1, 3, 300) || Unknown Token Reference :  0\n",
      "Shape Candidate :  (1, 4, 300) || Unknown Token Candidate :  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape Reference : \", references.shape, \"||\", \"Unknown Token Reference : \", n_unknown_ref)\n",
    "print(\"Shape Candidate : \", candidates.shape, \"||\", \"Unknown Token Candidate : \", n_unknown_cand)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fasttext"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversition to word2vec format (Do not compile unless necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = fasttext.load_model(r'D:\\COURS\\A4\\S8\\Stage\\Documents\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\custom_BERTScore\\wiki.en\\wiki.en.bin')\n",
    "kv = KeyedVectors(vector_size=ft_model.get_dimension())\n",
    "all_words = ft_model.get_words()\n",
    "all_vectors = [ft_model.get_word_vector(w) for w in all_words]\n",
    "kv.add_vectors(all_words, all_vectors)\n",
    "kv.save_word2vec_format('ftwords.txt', binary=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading of formated fasttext file :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext = KeyedVectors.load_word2vec_format(r'D:\\COURS\\A4\\S8\\Stage\\Documents\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\custom_BERTScore\\ftwords.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext = KeyedVectors.load_word2vec_format(r'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.75779974,  0.5959443 , -0.27542254,  0.24059881, -0.5490175 ,\n",
       "        0.8300641 , -0.41410735,  0.7523287 ,  0.50460875, -0.86938876],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext[\"house\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Glove"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversition to word2vec format (Do not compile unless necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = r\"D:\\COURS\\A4\\S8\\Stage\\Documents\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\custom_BERTScore\\glove.6B\\glove.6B.300d.txt\"\n",
    "glove_temp = KeyedVectors.load_word2vec_format(glove_path, no_header=True)\n",
    "glove_temp.save_word2vec_format(r\"D:\\COURS\\A4\\S8\\Stage\\Documents\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\custom_BERTScore\\glove2word2vec.txt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading of formated glove file :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = KeyedVectors.load_word2vec_format(r\"D:\\COURS\\A4\\S8\\Stage\\Documents\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\custom_BERTScore\\glove2word2vec.txt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Similarity Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SimilarityCandToRef(references, candidates):\n",
    "    proximity = lambda x, y: (np.matmul(np.transpose(x), y))/(norm(x)*norm(y))\n",
    "\n",
    "    all_proximities = []\n",
    "\n",
    "    for candidate, reference in zip(candidates, references):\n",
    "        proximities = []\n",
    "        for c_word in candidate:\n",
    "            sub_proximities = []\n",
    "            for r_word in reference:\n",
    "                sub_proximities.append(proximity(r_word, c_word))\n",
    "            proximities.append(sub_proximities)\n",
    "        all_proximities.append(proximities)\n",
    "    return all_proximities\n",
    "\n",
    "def SimilarityRefToCand(references, candidates):\n",
    "    proximity = lambda x, y: (np.matmul(np.transpose(x), y))/(norm(x)*norm(y))\n",
    "\n",
    "    all_proximities = []\n",
    "\n",
    "    for candidate, reference in zip(candidates, references):\n",
    "        proximities = []\n",
    "        for r_word in reference:\n",
    "            sub_proximities = []\n",
    "            for c_word in candidate:\n",
    "                sub_proximities.append(proximity(r_word, c_word))\n",
    "            proximities.append(sub_proximities)\n",
    "        all_proximities.append(proximities)\n",
    "    return all_proximities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'references' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m candToRef \u001b[39m=\u001b[39m SimilarityCandToRef(references, candidates)\n\u001b[0;32m      2\u001b[0m refToCand \u001b[39m=\u001b[39m SimilarityRefToCand(references, candidates)\n\u001b[0;32m      3\u001b[0m refToCand2 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mtranspose(candToRef) \n",
      "\u001b[1;31mNameError\u001b[0m: name 'references' is not defined"
     ]
    }
   ],
   "source": [
    "candToRef = SimilarityCandToRef(references, candidates)\n",
    "refToCand = SimilarityRefToCand(references, candidates)\n",
    "refToCand2 = np.transpose(candToRef) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0.519005613856723, 0.18616442214576517, 0.04520518544899319],\n",
       "  [0.09242192671658575, 0.012498354833030686, 0.025547976598766665],\n",
       "  [0.2098326692090991, 0.3489852938028225, -0.00109822157119661],\n",
       "  [0.0368609406903856, 0.020396980932637056, 1.0]]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candToRef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0.519005613856723,\n",
       "   0.09242192671658575,\n",
       "   0.2098326692090991,\n",
       "   0.0368609406903856],\n",
       "  [0.18616442214576517,\n",
       "   0.012498354833030686,\n",
       "   0.3489852938028225,\n",
       "   0.020396980932637056],\n",
       "  [0.04520518544899319, 0.025547976598766665, -0.00109822157119661, 1.0]]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refToCand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.51900561],\n",
       "        [ 0.09242193],\n",
       "        [ 0.20983267],\n",
       "        [ 0.03686094]],\n",
       "\n",
       "       [[ 0.18616442],\n",
       "        [ 0.01249835],\n",
       "        [ 0.34898529],\n",
       "        [ 0.02039698]],\n",
       "\n",
       "       [[ 0.04520519],\n",
       "        [ 0.02554798],\n",
       "        [-0.00109822],\n",
       "        [ 1.        ]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refToCand2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Calculation of P, R and F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### R calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullSum = []\n",
    "for individualSimilarity in candToRef:\n",
    "    currentSum = 0\n",
    "    for row in individualSimilarity:\n",
    "        currentSum += row[np.argmax(row)]\n",
    "    fullSum.append(currentSum)\n",
    "R = []\n",
    "for sum, reference in zip(fullSum, references):\n",
    "    R.append((1/norm(reference))*sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.42425704559238325]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### P Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullSum = []\n",
    "for individualSimilarity in refToCand:\n",
    "    currentSum = 0\n",
    "    for row in individualSimilarity:\n",
    "        currentSum += row[np.argmax(row)]\n",
    "    fullSum.append(currentSum)\n",
    "P = []\n",
    "for sum, candidate in zip(fullSum, candidates):\n",
    "    P.append((1/norm(candidate))*sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3604158590807159]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### F Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = []\n",
    "\n",
    "for r, p in zip(R, P):\n",
    "    f = 2*((p*r)/(p+r))\n",
    "    F.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.38973938477441966]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Full Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeMetrics(candToRef, refToCand):\n",
    "    # R computation\n",
    "    fullSum = []\n",
    "    for individualSimilarity in candToRef:\n",
    "        currentSum = 0\n",
    "        for row in individualSimilarity:\n",
    "            currentSum += row[np.argmax(row)]\n",
    "        fullSum.append(currentSum)\n",
    "    R = []\n",
    "    for sum, reference in zip(fullSum, references):\n",
    "        R.append((1/norm(reference))*sum)\n",
    "\n",
    "    # P computation\n",
    "    fullSum = []\n",
    "    for individualSimilarity in refToCand:\n",
    "        currentSum = 0\n",
    "        for row in individualSimilarity:\n",
    "            currentSum += row[np.argmax(row)]\n",
    "        fullSum.append(currentSum)\n",
    "    P = []\n",
    "    for sum, candidate in zip(fullSum, candidates):\n",
    "        P.append((1/norm(candidate))*sum)\n",
    "    \n",
    "    # F computation\n",
    "    F = []\n",
    "    for r, p in zip(R, P):\n",
    "        f = 2*((p*r)/(p+r))\n",
    "        F.append(f)\n",
    "    \n",
    "    return (R, R, F)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Basic Test of the package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_score.score import score\n",
    "from custom_score.utils import model_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = model_load(\"Word2Vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.40998854847735094], [0.22880340017752968], [0.2937005518713847])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.03954665010154433], [0.05171590963278553], [0.04481993467824077])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(glove)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II - Billsum Dataset Test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0 - Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DynamicEmbeddingSampleTest(data, limit=3, modelPath = None, model = None, nbLayers = 24):\n",
    "    nbIter = 1\n",
    "    scores = []\n",
    "    init_time = datetime.now()\n",
    "    for row in data.iterrows():\n",
    "        curCand = [row[1][2]]\n",
    "        curRef = row[1][1]\n",
    "        curCand = [\" \".join(row[1][2].split(\"\\n\"))]\n",
    "        curRef = [\" \".join(row[1][1].split(\"\\n\"))]\n",
    "        assert len(curCand) == len(curRef)\n",
    "        if modelPath != None:\n",
    "            (P, R, F), hashname = bert_score.score(curCand, curRef, lang=\"en\", \n",
    "                                        model_type=modelPath, \n",
    "                                        num_layers=nbLayers, return_hash=True)\n",
    "        elif model != None:\n",
    "            (P, R, F), hashname = bert_score.score(curCand, curRef, lang=\"en\", \n",
    "                                        model_type=model, \n",
    "                                        num_layers=nbLayers, return_hash=True)\n",
    "        else:\n",
    "            (P, R, F), hashname = bert_score.score(curCand, curRef, lang=\"en\", return_hash=True)\n",
    "        P = P[0].item()\n",
    "        R = R[0].item()\n",
    "        F = F[0].item()\n",
    "        scores.append((P, R, F))\n",
    "        if nbIter >= limit:\n",
    "            break\n",
    "        nbIter += 1\n",
    "    runtime = (datetime.now() - init_time).total_seconds()\n",
    "    return scores, runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StaticEmbeddingSampleTest(data, model, limit=3):\n",
    "    nbIter = 1\n",
    "    scores = []\n",
    "    init_time = datetime.now()\n",
    "    for row in data.iterrows():\n",
    "        curCand = [row[1][2]]\n",
    "        curRef = row[1][1]\n",
    "        curCand = [\" \".join(row[1][2].split(\"\\n\"))]\n",
    "        curRef = [\" \".join(row[1][1].split(\"\\n\"))]\n",
    "        assert len(curCand) == len(curRef)\n",
    "        \n",
    "        (P, R, F) = score(model, curCand, curRef)\n",
    "        P = P[0]\n",
    "        R = R[0]\n",
    "        F = F[0]\n",
    "        scores.append((P, R, F))\n",
    "\n",
    "        if nbIter >= limit:\n",
    "            break\n",
    "        nbIter += 1\n",
    "    runtime = (datetime.now() - init_time).total_seconds()\n",
    "    return scores, runtime\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Load Benchmarking Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "billsumPath = r\"D:\\COURS\\A4\\S8\\Stage\\Documents\\Datasets\\Summary Evaluation\\BillSum\\corpus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#billsum_train = pd.read_json(path_or_buf = billsumPath + r\"\\us_train_data_final_OFFICIAL.jsonl\", lines=True)\n",
    "billsum_test = pd.read_json(path_or_buf = billsumPath + r\"\\us_test_data_final_OFFICIAL.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bill_id</th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "      <th>text_len</th>\n",
       "      <th>sum_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>110_hr37</td>\n",
       "      <td>SECTION 1. SHORT TITLE.\\n\\n    This Act may be...</td>\n",
       "      <td>National Science Education Tax Incentive for B...</td>\n",
       "      <td>To amend the Internal Revenue Code of 1986 to ...</td>\n",
       "      <td>8494</td>\n",
       "      <td>321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>112_hr2873</td>\n",
       "      <td>SECTION 1. SHORT TITLE.\\n\\n    This Act may be...</td>\n",
       "      <td>Small Business Expansion and Hiring Act of 201...</td>\n",
       "      <td>To amend the Internal Revenue Code of 1986 to ...</td>\n",
       "      <td>6522</td>\n",
       "      <td>1424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>109_s2408</td>\n",
       "      <td>SECTION 1. RELEASE OF DOCUMENTS CAPTURED IN IR...</td>\n",
       "      <td>Requires the Director of National Intelligence...</td>\n",
       "      <td>A bill to require the Director of National Int...</td>\n",
       "      <td>6154</td>\n",
       "      <td>463</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      bill_id                                               text   \n",
       "0    110_hr37  SECTION 1. SHORT TITLE.\\n\\n    This Act may be...  \\\n",
       "1  112_hr2873  SECTION 1. SHORT TITLE.\\n\\n    This Act may be...   \n",
       "2   109_s2408  SECTION 1. RELEASE OF DOCUMENTS CAPTURED IN IR...   \n",
       "\n",
       "                                             summary   \n",
       "0  National Science Education Tax Incentive for B...  \\\n",
       "1  Small Business Expansion and Hiring Act of 201...   \n",
       "2  Requires the Director of National Intelligence...   \n",
       "\n",
       "                                               title  text_len  sum_len  \n",
       "0  To amend the Internal Revenue Code of 1986 to ...      8494      321  \n",
       "1  To amend the Internal Revenue Code of 1986 to ...      6522     1424  \n",
       "2  A bill to require the Director of National Int...      6154      463  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "billsum_test.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Test : RoBERTa Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_scores, bert_runtime = DynamicEmbeddingSampleTest(billsum_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Test : Word2Vec Implementation "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### W2V Serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = model_load(\"Word2Vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('serialized_w2v.pkl', 'wb') as f:\n",
    "    pickle.dump(w2v, f)\n",
    "    f.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### W2V test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('serialized_w2v.pkl', 'rb') as f:\n",
    "    w2v = pickle.load(f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_scores, word2vec_runtime = StaticEmbeddingSampleTest(billsum_test, w2v, 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Test : Fasttext Implementation "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - Test : Glove Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = KeyedVectors.load_word2vec_format(r\"D:\\COURS\\A4\\S8\\Stage\\Documents\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\custom_BERTScore\\glove2word2vec.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_scores, glove_runtime = StaticEmbeddingSampleTest(billsum_test, glove)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 - Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Runtime Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>runtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Roberta-24-layers</th>\n",
       "      <td>38.338537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word2Vec</th>\n",
       "      <td>4.411479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Glove</th>\n",
       "      <td>4.890593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     runtime\n",
       "Roberta-24-layers  38.338537\n",
       "Word2Vec            4.411479\n",
       "Glove               4.890593"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runtimeTable = [bert_runtime, word2vec_runtime, glove_runtime]\n",
    "runtimeDf = pd.DataFrame(runtimeTable, columns=[\"runtime\"], index=[\"Roberta-24-layers\", \"Word2Vec\", \"Glove\"])\n",
    "runtimeDf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quality Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bert_P</th>\n",
       "      <th>Bert_R</th>\n",
       "      <th>Bert_F</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.874369</td>\n",
       "      <td>0.704338</td>\n",
       "      <td>0.780197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.846002</td>\n",
       "      <td>0.728316</td>\n",
       "      <td>0.782760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.873844</td>\n",
       "      <td>0.702725</td>\n",
       "      <td>0.778998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Bert_P    Bert_R    Bert_F\n",
       "0  0.874369  0.704338  0.780197\n",
       "1  0.846002  0.728316  0.782760\n",
       "2  0.873844  0.702725  0.778998"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_scores_Df = pd.DataFrame(bert_scores, columns=[\"Bert_P\", \"Bert_R\", \"Bert_F\"])\n",
    "bert_scores_Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>W2V_P</th>\n",
       "      <th>W2V_R</th>\n",
       "      <th>W2V_F</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.542677</td>\n",
       "      <td>0.904048</td>\n",
       "      <td>0.678230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.769379</td>\n",
       "      <td>0.953081</td>\n",
       "      <td>0.851434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.718532</td>\n",
       "      <td>0.909344</td>\n",
       "      <td>0.802755</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      W2V_P     W2V_R     W2V_F\n",
       "0  0.542677  0.904048  0.678230\n",
       "1  0.769379  0.953081  0.851434\n",
       "2  0.718532  0.909344  0.802755"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_scores_Df = pd.DataFrame(word2vec_scores, columns=[\"W2V_P\", \"W2V_R\", \"W2V_F\"])\n",
    "word2vec_scores_Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bill_id</th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "      <th>text_len</th>\n",
       "      <th>sum_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>110_hr37</td>\n",
       "      <td>SECTION 1. SHORT TITLE.\\n\\n    This Act may be...</td>\n",
       "      <td>National Science Education Tax Incentive for B...</td>\n",
       "      <td>To amend the Internal Revenue Code of 1986 to ...</td>\n",
       "      <td>8494</td>\n",
       "      <td>321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>112_hr2873</td>\n",
       "      <td>SECTION 1. SHORT TITLE.\\n\\n    This Act may be...</td>\n",
       "      <td>Small Business Expansion and Hiring Act of 201...</td>\n",
       "      <td>To amend the Internal Revenue Code of 1986 to ...</td>\n",
       "      <td>6522</td>\n",
       "      <td>1424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>109_s2408</td>\n",
       "      <td>SECTION 1. RELEASE OF DOCUMENTS CAPTURED IN IR...</td>\n",
       "      <td>Requires the Director of National Intelligence...</td>\n",
       "      <td>A bill to require the Director of National Int...</td>\n",
       "      <td>6154</td>\n",
       "      <td>463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>108_s1899</td>\n",
       "      <td>SECTION 1. SHORT TITLE.\\n\\n    This Act may be...</td>\n",
       "      <td>National Cancer Act of 2003 - Amends the Publi...</td>\n",
       "      <td>A bill to improve data collection and dissemin...</td>\n",
       "      <td>19853</td>\n",
       "      <td>1400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>107_s1531</td>\n",
       "      <td>SECTION 1. SHORT TITLE.\\n\\n    This Act may be...</td>\n",
       "      <td>Military Call-up Relief Act - Amends the Inter...</td>\n",
       "      <td>A bill to amend the Internal Revenue Code of 1...</td>\n",
       "      <td>6273</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>107_hr4541</td>\n",
       "      <td>SECTION 1. RELIQUIDATION OF CERTAIN ENTRIES PR...</td>\n",
       "      <td>Requires the Customs Service to reliquidate ce...</td>\n",
       "      <td>To provide for reliquidation of entries premat...</td>\n",
       "      <td>11691</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>111_s1495</td>\n",
       "      <td>SECTION 1. SHORT TITLE.\\n\\n    This Act may be...</td>\n",
       "      <td>Service Dogs for Veterans Act of 2009 - Direct...</td>\n",
       "      <td>A bill to require the Secretary of Veterans Af...</td>\n",
       "      <td>5328</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>111_s3885</td>\n",
       "      <td>SECTION 1. SHORT TITLE.\\n\\n    This Act may be...</td>\n",
       "      <td>Race to the Top Act of 2010 - Directs the Secr...</td>\n",
       "      <td>A bill to provide incentives for States and lo...</td>\n",
       "      <td>16668</td>\n",
       "      <td>1525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>113_hr1796</td>\n",
       "      <td>SECTION 1. SHORT TITLE.\\n\\n    This Act may be...</td>\n",
       "      <td>Troop Talent Act of 2013 - Directs the Secreta...</td>\n",
       "      <td>Troop Talent Act of 2013</td>\n",
       "      <td>15352</td>\n",
       "      <td>2151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>103_hr1987</td>\n",
       "      <td>SECTION 1. SHORT TITLE.\\n\\n    This Act may be...</td>\n",
       "      <td>Taxpayer's Right to View Act of 1993 - Amends ...</td>\n",
       "      <td>Taxpayer's Right to View Act of 1993</td>\n",
       "      <td>5633</td>\n",
       "      <td>894</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      bill_id                                               text   \n",
       "0    110_hr37  SECTION 1. SHORT TITLE.\\n\\n    This Act may be...  \\\n",
       "1  112_hr2873  SECTION 1. SHORT TITLE.\\n\\n    This Act may be...   \n",
       "2   109_s2408  SECTION 1. RELEASE OF DOCUMENTS CAPTURED IN IR...   \n",
       "3   108_s1899  SECTION 1. SHORT TITLE.\\n\\n    This Act may be...   \n",
       "4   107_s1531  SECTION 1. SHORT TITLE.\\n\\n    This Act may be...   \n",
       "5  107_hr4541  SECTION 1. RELIQUIDATION OF CERTAIN ENTRIES PR...   \n",
       "6   111_s1495  SECTION 1. SHORT TITLE.\\n\\n    This Act may be...   \n",
       "7   111_s3885  SECTION 1. SHORT TITLE.\\n\\n    This Act may be...   \n",
       "8  113_hr1796  SECTION 1. SHORT TITLE.\\n\\n    This Act may be...   \n",
       "9  103_hr1987  SECTION 1. SHORT TITLE.\\n\\n    This Act may be...   \n",
       "\n",
       "                                             summary   \n",
       "0  National Science Education Tax Incentive for B...  \\\n",
       "1  Small Business Expansion and Hiring Act of 201...   \n",
       "2  Requires the Director of National Intelligence...   \n",
       "3  National Cancer Act of 2003 - Amends the Publi...   \n",
       "4  Military Call-up Relief Act - Amends the Inter...   \n",
       "5  Requires the Customs Service to reliquidate ce...   \n",
       "6  Service Dogs for Veterans Act of 2009 - Direct...   \n",
       "7  Race to the Top Act of 2010 - Directs the Secr...   \n",
       "8  Troop Talent Act of 2013 - Directs the Secreta...   \n",
       "9  Taxpayer's Right to View Act of 1993 - Amends ...   \n",
       "\n",
       "                                               title  text_len  sum_len  \n",
       "0  To amend the Internal Revenue Code of 1986 to ...      8494      321  \n",
       "1  To amend the Internal Revenue Code of 1986 to ...      6522     1424  \n",
       "2  A bill to require the Director of National Int...      6154      463  \n",
       "3  A bill to improve data collection and dissemin...     19853     1400  \n",
       "4  A bill to amend the Internal Revenue Code of 1...      6273      278  \n",
       "5  To provide for reliquidation of entries premat...     11691      114  \n",
       "6  A bill to require the Secretary of Veterans Af...      5328      379  \n",
       "7  A bill to provide incentives for States and lo...     16668     1525  \n",
       "8                           Troop Talent Act of 2013     15352     2151  \n",
       "9               Taxpayer's Right to View Act of 1993      5633      894  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "billsum_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GLV_P</th>\n",
       "      <th>GLV_R</th>\n",
       "      <th>GLV_F</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.193759</td>\n",
       "      <td>-0.170188</td>\n",
       "      <td>-2.798021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.496544</td>\n",
       "      <td>-0.422760</td>\n",
       "      <td>-1.178411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.309721</td>\n",
       "      <td>-0.217137</td>\n",
       "      <td>-1.452770</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      GLV_P     GLV_R     GLV_F\n",
       "0  0.193759 -0.170188 -2.798021\n",
       "1  1.496544 -0.422760 -1.178411\n",
       "2  0.309721 -0.217137 -1.452770"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_scores_Df = pd.DataFrame(glove_scores, columns=[\"GLV_P\", \"GLV_R\", \"GLV_F\"])\n",
    "glove_scores_Df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOLDALL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding\n",
    "new_references = []\n",
    "new_candidates = []\n",
    "\n",
    "for reference, candidate in zip(references, candidates):\n",
    "    size_diff = len(reference) - len(candidate)\n",
    "    if size_diff >= 0:\n",
    "        candidate = np.pad(candidate, (0, size_diff))\n",
    "        reference = np.array(reference)\n",
    "    else:\n",
    "        reference = np.pad(reference, [(0, np.abs(size_diff)), (0, 0)], mode=\"constant\")\n",
    "        candidate = np.array(candidate)\n",
    "    new_references.append(reference)\n",
    "    new_candidates.append(candidate)\n",
    "   \n",
    "\n",
    "references = np.array(new_references, dtype=object)\n",
    "candidates = np.array(new_candidates, dtype=object)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicating billsum summary column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = []\n",
    "for i in range(billsum_test.shape[0]):\n",
    "    tab.append( billsum_test.iloc[i][2] + billsum_test.iloc[i][2])\n",
    "billsum_test[\"summary\"] = tab    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
