{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(r\"C:\\Pro\\Stages\\A4 - DVRC\\Work\\BARTScore\")\n",
    "sys.path.append(r\"C:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\myLibraries\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_score.score import BERTScoreStaticSampleTest, BERTScoreDynamicSampleTest\n",
    "from custom_score.utils import serialized_to_model\n",
    "from bart_score import BARTScorer\n",
    "import torch\n",
    "import tensorflow_datasets as tfds\n",
    "from datetime import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkup and linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Billsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SECTION 1. SHORT TITLE.\\n\\n    This Act may be...</td>\n",
       "      <td>Local Innovation and Coastal Protection Act of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SECTION 1. SHORT TITLE.\\n\\n    This Act may be...</td>\n",
       "      <td>Gun Show Background Check Act of 2008 - Amends...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SECTION 1. SHORT TITLE.\\n\\n    This Act may be...</td>\n",
       "      <td>Recycled Roads Act of 2003 - Directs the Secre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SECTION 1. SHORT TITLE.\\n\\n    This Act may be...</td>\n",
       "      <td>Prosthetic and Custom Orthotic Parity Act of 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SECTION 1. SHORT TITLE.\\n\\n    This Act may be...</td>\n",
       "      <td>Investing in Neighborhood-focused, Vital, Evid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   \n",
       "0  SECTION 1. SHORT TITLE.\\n\\n    This Act may be...  \\\n",
       "1  SECTION 1. SHORT TITLE.\\n\\n    This Act may be...   \n",
       "2  SECTION 1. SHORT TITLE.\\n\\n    This Act may be...   \n",
       "3  SECTION 1. SHORT TITLE.\\n\\n    This Act may be...   \n",
       "4  SECTION 1. SHORT TITLE.\\n\\n    This Act may be...   \n",
       "\n",
       "                                             summary  \n",
       "0  Local Innovation and Coastal Protection Act of...  \n",
       "1  Gun Show Background Check Act of 2008 - Amends...  \n",
       "2  Recycled Roads Act of 2003 - Directs the Secre...  \n",
       "3  Prosthetic and Custom Orthotic Parity Act of 2...  \n",
       "4  Investing in Neighborhood-focused, Vital, Evid...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "billsum = tfds.load('huggingface:billsum')\n",
    "billsum_test = tfds.as_dataframe(billsum[\"test\"])\n",
    "billsum_test = billsum_test.loc[:, [\"text\", \"summary\"]]\n",
    "billsum_test.text = billsum_test.text.str.decode(\"utf-8\")\n",
    "billsum_test.summary = billsum_test.summary.str.decode(\"utf-8\")\n",
    "billsum_test.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opinosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinosis_builder = tfds.builder(\"opinosis\")\n",
    "opinosis_builder.download_and_prepare()\n",
    "opinosis = opinosis_builder.as_dataset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classic BERTScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_scores, bert_runtime = BERTScoreDynamicSampleTest(billsum_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static BERTScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = serialized_to_model(r'C:\\Pro\\Stages\\A4 - DVRC\\Work\\Models\\serialized_w2v.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_scores, word2vec_runtime = BERTScoreStaticSampleTest(billsum_test, w2v, 3, withIdf = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BARTScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BARTScoreDynamicSampleTest(data, limit=3):\n",
    "    \"\"\"\n",
    "    Benchmarking function allowing to compute classical bertscore as well as its runtime.\n",
    "\n",
    "    :param1 data (DataFrame) : Dataframe containing all references and candidates. Required Format : [col0: Reference, col1: Candidate].\n",
    "    :param2 limit (int): Number of individuals to compute.\n",
    "    :param3 modelPath (string): Path to the wanted model in the HuggingFace repository.\n",
    "    :param4 model (object): Model to use directly for computation.\n",
    "    :param5 nbLayers (int): Number of layers in the custom model (has to be filled-in if modelPath!=None or model!=None).  \n",
    "    \n",
    "    :output1 scores (list): List of Precision, Recall and F1score for each computed individual.\n",
    "    :output2 runtime (float): Elasped time between the start and end of score computation for all individuals.\n",
    "    \"\"\"\n",
    "\n",
    "    nbIter = 1\n",
    "    scores = []\n",
    "    bart_scorer = BARTScorer(device='cuda:0', checkpoint='facebook/bart-large-cnn')\n",
    "    init_time = datetime.now()\n",
    "    for row in data.iterrows():\n",
    "        curCand = [\" \".join(row[1][1].split(\"\\n\"))]\n",
    "        curRef = [\" \".join(row[1][0].split(\"\\n\"))]\n",
    "        assert len(curCand) == len(curRef)\n",
    "        \n",
    "        score = bart_scorer.score(curRef, curCand, batch_size=4)\n",
    "        score = score[0]\n",
    "        \n",
    "        scores.append(score)\n",
    "        if nbIter >= limit:\n",
    "            break\n",
    "        nbIter += 1\n",
    "    runtime = (datetime.now() - init_time).total_seconds()\n",
    "    return scores, runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_scores, bart_runtime = BARTScoreDynamicSampleTest(billsum_test, limit=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalutation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtime table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>runtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Roberta-24-layers</th>\n",
       "      <td>8.840713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word2Vec</th>\n",
       "      <td>5.608159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BART-large-CNN</th>\n",
       "      <td>0.349999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    runtime\n",
       "Roberta-24-layers  8.840713\n",
       "Word2Vec           5.608159\n",
       "BART-large-CNN     0.349999"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runtimeTable = [bert_runtime, word2vec_runtime, bart_runtime]\n",
    "runtimeDf = pd.DataFrame(runtimeTable, columns=[\"runtime\"], index=[\"Roberta-24-layers\", \"Word2Vec\", \"BART-large-CNN\"])\n",
    "runtimeDf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bert_P</th>\n",
       "      <th>Bert_R</th>\n",
       "      <th>Bert_F</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.849191</td>\n",
       "      <td>0.781935</td>\n",
       "      <td>0.814176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.818807</td>\n",
       "      <td>0.699549</td>\n",
       "      <td>0.754495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.832273</td>\n",
       "      <td>0.697599</td>\n",
       "      <td>0.759008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Bert_P    Bert_R    Bert_F\n",
       "0  0.849191  0.781935  0.814176\n",
       "1  0.818807  0.699549  0.754495\n",
       "2  0.832273  0.697599  0.759008"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_scores_Df = pd.DataFrame(bert_scores, columns=[\"Bert_P\", \"Bert_R\", \"Bert_F\"])\n",
    "bert_scores_Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>W2V_P</th>\n",
       "      <th>W2V_R</th>\n",
       "      <th>W2V_F</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.646725</td>\n",
       "      <td>0.902619</td>\n",
       "      <td>0.753539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.699015</td>\n",
       "      <td>0.941868</td>\n",
       "      <td>0.802470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.652091</td>\n",
       "      <td>0.959232</td>\n",
       "      <td>0.776389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      W2V_P     W2V_R     W2V_F\n",
       "0  0.646725  0.902619  0.753539\n",
       "1  0.699015  0.941868  0.802470\n",
       "2  0.652091  0.959232  0.776389"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_scores_Df = pd.DataFrame(word2vec_scores, columns=[\"W2V_P\", \"W2V_R\", \"W2V_F\"])\n",
    "word2vec_scores_Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BARTScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.924354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.258326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3.256637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   BARTScore\n",
       "0  -2.924354\n",
       "1  -3.258326\n",
       "2  -3.256637"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_scores_Df = pd.DataFrame(bart_scores, columns=[\"BARTScore\"])\n",
    "bart_scores_Df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
