{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\umap\\distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\umap\\distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\umap\\distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\umap\\umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\myLibraries\")\n",
    "from MARScore.utils import *\n",
    "from MARScore.score import MARSCore\n",
    "\n",
    "from custom_score.utils import cleanString\n",
    "from sklearn.cluster import SpectralClustering, Birch\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_json(r'C:\\Pro\\Stages\\A4 - DVRC\\Work\\Datasets\\pubmed\\test.json', lines=True)\n",
    "dataset = dataset[[\"article_text\", \"abstract_text\"]]\n",
    "cleaner = lambda x: \". \".join(x).replace(\"<S>\", \"\").strip()\n",
    "format_dot = lambda x: x.replace(\" .\", \".\")\n",
    "dataset.loc[:,\"abstract_text\"] = dataset[\"abstract_text\"].replace(regex=r\"\\[[^\\]]*\\]\", value=\"\")\n",
    "dataset.loc[:,\"article_text\"] = dataset[\"article_text\"].replace(regex=r\"\\[[^\\]]*\\]\", value=\"\")\n",
    "dataset.loc[:,\"abstract_text\"] = dataset[\"abstract_text\"].map(cleaner)\n",
    "dataset.loc[:,\"article_text\"] = dataset[\"article_text\"].map(cleaner)\n",
    "dataset.loc[:,\"abstract_text\"] = dataset[\"abstract_text\"].map(cleanString)\n",
    "dataset.loc[:,\"article_text\"] = dataset[\"article_text\"].map(cleanString)\n",
    "dataset.loc[:,\"abstract_text\"] = dataset[\"abstract_text\"].map(format_dot)\n",
    "dataset.loc[:,\"article_text\"] = dataset[\"article_text\"].map(format_dot)\n",
    "dataset = dataset.rename(columns={\"abstract_text\": \"summary\",\n",
    "                        \"article_text\": \"text\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = dataset.iloc[:2, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus to Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpusToSentences(corpus):\n",
    "    corpus_sentences = []\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    for indiv in corpus:\n",
    "        doc = nlp(indiv)\n",
    "        corpus_sentences.append([sent.text for sent in doc.sents])   \n",
    "    return corpus_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = corpusToSentences(subset[\"text\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanMarkers(embs, labels, mode=\"all\", ignore=[\".\"]):\n",
    "    \"\"\"\n",
    "    Removes vectors associated with noisy words such as stop words, punctuation, and BERT separator tokens.\n",
    "\n",
    "    :param1 embs (list): List of words embeddings.\n",
    "    :param2 labels (list): List of text token associated with each embedding.\n",
    "\n",
    "    :output1 new_embs (list): Cleansed list of words embeddings.\n",
    "    :output2 new_labels (list): Cleansed list of tokens.\n",
    "    :output3 -1 (int): Error output.\n",
    "    \"\"\"\n",
    "    token_indexes = [i for i in range(len(labels)) if (labels[i] != \"[PAD]\" and labels[i] != \"[CLS]\" and labels[i] != \"[SEP]\") or labels[i] in ignore]\n",
    "    new_embs = [embs[i] for i in range(len(embs)) if i in token_indexes]\n",
    "    new_labels = [labels[i] for i in range(len(labels)) if i in token_indexes]\n",
    "\n",
    "    if mode == \"all\":\n",
    "        return new_embs, new_labels\n",
    "    elif mode == \"emb\":\n",
    "        return new_embs\n",
    "    elif mode == \"lab\":\n",
    "        return new_labels\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o, l = tokenizeCorpus(subset[\"text\"][0])\n",
    "v = vectorizeCorpus(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v, l = cleanMarkers(v, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" \".join(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenced_tokens = corpusToSentences([\" \".join(l)])\n",
    "print(sentenced_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = [i for i, sent in enumerate(sentenced_tokens[0]) for _ in sent.split(\" \")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentences clusters visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import colormaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 10\u001b[0m\n\u001b[0;32m      1\u001b[0m ms \u001b[39m=\u001b[39m MARSCore(subset[\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto_list(),\n\u001b[0;32m      2\u001b[0m               subset[\u001b[39m\"\u001b[39m\u001b[39msummary\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto_list(),\n\u001b[0;32m      3\u001b[0m               precision_level\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39ms\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m               model\u001b[39m=\u001b[39mBertModel\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39maitslab/biobert_huner_gene_v1\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[0;32m      8\u001b[0m               tokenizer\u001b[39m=\u001b[39mBertTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39maitslab/biobert_huner_gene_v1\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m      9\u001b[0m               \u001b[39m#clusterizer=AffinityPropagation(random_state=0))\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m ms\u001b[39m.\u001b[39;49mcompute()\n",
      "File \u001b[1;32mC:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\myLibraries\\MARScore\\score.py:117\u001b[0m, in \u001b[0;36mMARSCore.compute\u001b[1;34m(self, checkpoints, saveRate)\u001b[0m\n\u001b[0;32m    115\u001b[0m sentences_mask \u001b[39m=\u001b[39m []\n\u001b[0;32m    116\u001b[0m \u001b[39mwhile\u001b[39;00m j \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(l):\n\u001b[1;32m--> 117\u001b[0m     sentences_mask\u001b[39m.\u001b[39mappend(indiv_mask[i])\n\u001b[0;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m#\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m l[j]:\n\u001b[0;32m    119\u001b[0m         i\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "ms = MARSCore(subset[\"text\"].to_list(),\n",
    "              subset[\"summary\"].to_list(),\n",
    "              precision_level=\"s\",\n",
    "              ratio=5,\n",
    "              n_allowed_elements=7,\n",
    "              extraction_method=\"concat_l4\",\n",
    "              model=BertModel.from_pretrained('aitslab/biobert_huner_gene_v1'),\n",
    "              tokenizer=BertTokenizer.from_pretrained('aitslab/biobert_huner_gene_v1'))\n",
    "              #clusterizer=AffinityPropagation(random_state=0))\n",
    "ms.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_map(ms, index):\n",
    "    sentences_mask = [i for i, sent in enumerate(ms.sentenced_labels[index]) for _ in sent.split(\" \")]\n",
    "    sentences_map = {0: []}\n",
    "    for cluster_index, smask in zip(ms.clusters_labels[index], sentences_mask):\n",
    "        if cluster_index in sentences_map.keys():\n",
    "            sentences_map[cluster_index].append(smask)\n",
    "        else:\n",
    "            sentences_map[cluster_index] = [smask]\n",
    "    inv_smap = {}\n",
    "    for k,v in sentences_map.items():\n",
    "        for x in v:\n",
    "            if x in inv_smap.keys():\n",
    "                inv_smap[x].add(k)\n",
    "            else:\n",
    "                inv_smap[x] = set([k])\n",
    "    inv_smap = {k: list(v) for k, v in inv_smap.items()}\n",
    "    return inv_smap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smap = sentences_map(ms, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selector(ms, s_map, index):\n",
    "    selectedClusters = []\n",
    "    for sel_index in ms.selectedIndexes[index]:\n",
    "        selectedClusters.extend(s_map[sel_index])\n",
    "    return list(set(selectedClusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedClusters = selector(ms, smap, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_viz(reductor, embs, labels, selected_clusters, embs_gold=None, labels_gold=None, labels_cluster=None, tf_values=None,):\n",
    "    \n",
    "    def colorize(label=None, glabels=[], clabel=None, cmap=[], mode=\"unclustered\"):\n",
    "        \"\"\"\n",
    "        Colorize vector's projections depending on the context. \n",
    "\n",
    "        :param1 label (string): Single Token.\n",
    "        :param2 glabel (list): List of gold tokens.\n",
    "        :param3 clabel (int): Cluster's index of the current token.\n",
    "        :param4 cmap (color_map): Matplotlib color map.\n",
    "        :param5 mode (string): Equals to <clustered>, <unclustered> to respectively colorize depending on gold's, cluster's belonging.\n",
    "\n",
    "        :output color (string): CSS text color.  \n",
    "        \"\"\"\n",
    "        comp_gold = True if label != None and glabels != [] else False\n",
    "        assert label != None or glabels != [] or clabel != None, \"ERROR: No labels detected\"\n",
    "        if mode == \"unclustered\":\n",
    "            if comp_gold:\n",
    "                color = \"green\" if label in glabels else 'red'\n",
    "            else:\n",
    "                color = \"red\"\n",
    "        elif mode == \"clustered\":\n",
    "            if clabel != None and cmap != []:\n",
    "                color = cmap[clabel]\n",
    "            else:\n",
    "                color = \"black\"\n",
    "        return color\n",
    "    \n",
    "    cmap = colormaps[\"viridis\"].colors\n",
    "    formated_embs = np.array(embs)\n",
    "    formated_embs_gold = np.array(embs_gold)\n",
    "    token_indexes = [i for i in range(len(labels)) if labels[i] != \"[PAD]\" and labels[i] != \"[CLS]\" and labels[i] != \"[SEP]\"]\n",
    "\n",
    "    proj2D = np.transpose(embs)\n",
    "    data = {\"x\": proj2D[0],\n",
    "            \"y\": proj2D[1],\n",
    "            \"labels\": labels,\n",
    "            \"clusters\": labels_cluster}\n",
    "    for k in data.keys():\n",
    "        data[k] = [data[k][i] for i in range(len(data[k])) if i in token_indexes]\n",
    "\n",
    "    token_indexes_gold = [i for i in range(len(labels_gold)) if labels_gold[i] != \"[PAD]\" and labels_gold[i] != \"[CLS]\" and labels_gold[i] != \"[SEP]\"]\n",
    "    proj2D_gold = reductor.transform(formated_embs_gold).T\n",
    "    data_gold = {\"x\": proj2D_gold[0],\n",
    "                 \"y\": proj2D_gold[1],\n",
    "                 \"labels\": labels_gold}\n",
    "    for k in data_gold.keys():\n",
    "        data_gold[k] = [data_gold[k][i] for i in range(len(data_gold[k])) if i in token_indexes_gold]\n",
    "\n",
    "    traces = []\n",
    "    for i in range(len(data['x'])):\n",
    "        if labels_cluster[i] in selected_clusters:\n",
    "            if data[\"clusters\"] != None:\n",
    "                color = colorize(clabel=data[\"clusters\"][i], cmap=cmap, mode=\"clustered\")\n",
    "            else:\n",
    "                color = colorize(labels=data['labels'][i], glabels=data_gold['labels'], mode=\"unclustered\")\n",
    "            trace = go.Scatter(\n",
    "                x=[data['x'][i]],\n",
    "                y=[data['y'][i]],\n",
    "                mode='markers',\n",
    "                marker=dict(size=9, color=color),\n",
    "                line=dict(width=2, color=\"DarkSlateGrey\"),\n",
    "                text=[\"token: \"+str(data['labels'][i]) +\" || \"+\"tf   : \"+str(tf_values[data['labels'][i]])+\" || cluster: \"+str(data[\"clusters\"][i])],\n",
    "                name=data['labels'][i]\n",
    "            )\n",
    "            traces.append(trace)\n",
    "    \n",
    "    for i in range(len(data_gold['x'])):\n",
    "        trace = go.Scatter(\n",
    "            x=[data_gold['x'][i]],\n",
    "            y=[data_gold['y'][i]],\n",
    "            mode='markers',\n",
    "            marker=dict(size=9, color='red'),\n",
    "            marker_symbol=\"diamond\",\n",
    "            line=dict(width=2, color=\"DarkSlateGrey\"),\n",
    "            text=[\"token: \"+str(data_gold['labels'][i])],\n",
    "            name=data_gold['labels'][i]\n",
    "        )\n",
    "        traces.append(trace) \n",
    "    layout = go.Layout(\n",
    "    title='2D Scatter Plot',\n",
    "    scene=dict(\n",
    "        xaxis=dict(title='X'),\n",
    "        yaxis=dict(title='Y')\n",
    "    ),\n",
    "    autosize=False,\n",
    "    width=1000,\n",
    "    height=1000\n",
    "    )\n",
    "    fig = go.Figure(data=traces, layout=layout)\n",
    "    fig.update_yaxes(\n",
    "        scaleanchor=\"x\",\n",
    "        scaleratio=1,\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_gold, l_gold = tokenizeCorpus(ms.gold[0])\n",
    "v_gold = vectorizeCorpus(o_gold, method=ms.extraction_method)\n",
    "v_gold, l_gold = cleanAll(v_gold, l_gold) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_viz(ms.dim_reductor, ms.vectors[0], ms.labels[0], selectedClusters, v_gold, l_gold, ms.clusters_labels[0], ms.tokens_tfs[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
