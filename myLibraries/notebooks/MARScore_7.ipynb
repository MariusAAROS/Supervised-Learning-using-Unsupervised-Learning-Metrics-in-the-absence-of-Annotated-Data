{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\umap\\distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\umap\\distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\umap\\distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\umap\\umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\myLibraries\")\n",
    "from MARScore.utils import *\n",
    "from MARScore.score import MARSCore\n",
    "\n",
    "from custom_score.utils import cleanString\n",
    "from sklearn.cluster import SpectralClustering, Birch\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_json(r'C:\\Pro\\Stages\\A4 - DVRC\\Work\\Datasets\\pubmed\\test.json', lines=True)\n",
    "dataset = dataset[[\"article_text\", \"abstract_text\"]]\n",
    "cleaner = lambda x: \". \".join(x).replace(\"<S>\", \"\").strip()\n",
    "format_dot = lambda x: x.replace(\" .\", \".\")\n",
    "dataset.loc[:,\"abstract_text\"] = dataset[\"abstract_text\"].replace(regex=r\"\\[[^\\]]*\\]\", value=\"\")\n",
    "dataset.loc[:,\"article_text\"] = dataset[\"article_text\"].replace(regex=r\"\\[[^\\]]*\\]\", value=\"\")\n",
    "dataset.loc[:,\"abstract_text\"] = dataset[\"abstract_text\"].map(cleaner)\n",
    "dataset.loc[:,\"article_text\"] = dataset[\"article_text\"].map(cleaner)\n",
    "dataset.loc[:,\"abstract_text\"] = dataset[\"abstract_text\"].map(cleanString)\n",
    "dataset.loc[:,\"article_text\"] = dataset[\"article_text\"].map(cleanString)\n",
    "dataset.loc[:,\"abstract_text\"] = dataset[\"abstract_text\"].map(format_dot)\n",
    "dataset.loc[:,\"article_text\"] = dataset[\"article_text\"].map(format_dot)\n",
    "dataset = dataset.rename(columns={\"abstract_text\": \"summary\",\n",
    "                        \"article_text\": \"text\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = dataset.iloc[:2, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus to Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpusToSentences(corpus):\n",
    "    corpus_sentences = []\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    for indiv in corpus:\n",
    "        doc = nlp(indiv)\n",
    "        corpus_sentences.append([sent.text for sent in doc.sents])   \n",
    "    return corpus_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = corpusToSentences(subset[\"text\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanMarkers(embs, labels, mode=\"all\", ignore=[\".\"]):\n",
    "    \"\"\"\n",
    "    Removes vectors associated with noisy words such as stop words, punctuation, and BERT separator tokens.\n",
    "\n",
    "    :param1 embs (list): List of words embeddings.\n",
    "    :param2 labels (list): List of text token associated with each embedding.\n",
    "\n",
    "    :output1 new_embs (list): Cleansed list of words embeddings.\n",
    "    :output2 new_labels (list): Cleansed list of tokens.\n",
    "    :output3 -1 (int): Error output.\n",
    "    \"\"\"\n",
    "    token_indexes = [i for i in range(len(labels)) if (labels[i] != \"[PAD]\" and labels[i] != \"[CLS]\" and labels[i] != \"[SEP]\") or labels[i] in ignore]\n",
    "    new_embs = [embs[i] for i in range(len(embs)) if i in token_indexes]\n",
    "    new_labels = [labels[i] for i in range(len(labels)) if i in token_indexes]\n",
    "\n",
    "    if mode == \"all\":\n",
    "        return new_embs, new_labels\n",
    "    elif mode == \"emb\":\n",
    "        return new_embs\n",
    "    elif mode == \"lab\":\n",
    "        return new_labels\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o, l = tokenizeCorpus(subset[\"text\"][0])\n",
    "v = vectorizeCorpus(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v, l = cleanMarkers(v, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" \".join(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenced_tokens = corpusToSentences([\" \".join(l)])\n",
    "print(sentenced_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = [i for i, sent in enumerate(sentenced_tokens[0]) for _ in sent.split(\" \")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentences clusters visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = MARSCore(subset[\"text\"].to_list(),\n",
    "              subset[\"summary\"].to_list(),\n",
    "              precision_level=\"s\",\n",
    "              ratio=5,\n",
    "              n_allowed_elements=7,\n",
    "              extraction_method=\"concat_l4\",\n",
    "              model=BertModel.from_pretrained('aitslab/biobert_huner_gene_v1'),\n",
    "              tokenizer=BertTokenizer.from_pretrained('aitslab/biobert_huner_gene_v1'))\n",
    "              #clusterizer=AffinityPropagation(random_state=0))\n",
    "ms.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_map(ms, index):\n",
    "    sentences_mask = [i for i, sent in enumerate(ms.sentenced_labels[index]) for _ in sent.split(\" \")]\n",
    "    sentences_map = {0: []}\n",
    "    for cluster_index, smask in zip(ms.clusters_labels[index], sentences_mask):\n",
    "        if cluster_index in sentences_map.keys():\n",
    "            sentences_map[cluster_index].append(smask)\n",
    "        else:\n",
    "            sentences_map[cluster_index] = [smask]\n",
    "    return sentences_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "smap = sentences_map(ms, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_smap = {}\n",
    "for k,v in smap.items():\n",
    "    for x in v:\n",
    "        inv_smap.setdefault(x,[]).append(k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{55: [0, -1, -1, -1, -1, 4, 4, 9, 9, 50, 36, 78, 48, 82, 66, 32, 17, 17, 29, 75, 28, 28, 25, 25, 14, 14, 14, 13, 13, 1, 1, 16, 19, 19, 31], 56: [0, 0, 4, 4, 4, 49, 9, 9, 9, 50, 51, 59, 48, 56, 66, 33, 17, 17, 76, 75, 28, 28, 25, 25, 14, 13, 13, 1, 1, 16, 16, 19, 19, 31, 31], 62: [0, 4, 53, 52, 48, 17, 28, 23, 25, 14, 13, 1, 18, 142, 16, 31], 67: [0, 0, -1, -1, -1, 4, 4, 9, 9, 9, 71, 51, 51, 51, 122, 159, 38, 177, 91, 26, 26, 26, 131, 131, 131, 42, 42, 128, 77, 77, 80, 127, 61, 102, 160, 160, 160, 48, 48, 149, 149, 149, 149, 149, 37, 46, 11, 17, 17, 132, 132, 132, 132, 132, 76, 76, 75, 75, 28, 28, 23, 25, 25, 14, 14, 13, 13, 1, 1, 16, 16, 19, 19, 31, 31], 71: [0, -1, 4, 126, 24, 143, 9, 50, 36, 150, 48, 97, 37, 46, 11, 17, 23, 134, 151, 25, 14, 13, 1, 18, 16, 31, 135], 72: [0, 0, -1, 4, 4, 9, 9, 100, 51, 159, 38, 27, 160, 48, 48, 32, 32, 17, 17, 28, 23, 23, 65, 65, 65, 72, 72, 72, 25, 25, 14, 14, 13, 13, 1, 1, 18, 18, 16, 16, 31, 31], 73: [0, 4, 9, 71, 53, 52, 160, 160, 48, 37, 46, 17, 28, 25, 14, 13, 1, 16, 19, 31], 0: [117, 117, 117, 117, -1, -1, -1, -1, -1, -1, -1, -1, -1, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 55, 4, 49], 1: [117, 117, 117, 117, 117, 117, 117, -1, -1, -1, -1, -1, -1, -1, -1, -1, 6, 3, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111], 2: [117, 6], 3: [-1, -1, -1, -1, -1, -1, -1, 115, 49, 3, 7, 93, 93, 93, 93, 93, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 170, 120], 4: [-1, -1, -1, 93, 93, 93, 93, 93, 93, 93, 93, 126, 57, 84, 84, 84], 5: [-1, -1, -1, -1, -1, -1, 170, 120, 57, 84, 84, 174, 183, 183, 183, 183, 184, 184, 110, 107, 107], 6: [-1, -1, -1, -1, -1, 4, 6, 3, 7, 184, 184, 24, 143, 62, 62, 62, 62, 62, 62, 62, 62, 9], 7: [-1, -1, 4, 6, 62, 9, 114, 114, 162, 162, 162, 168, 168, 168, 45, 45, 54, 44, 35], 8: [-1, -1, -1, -1, -1, 3, 170, 170, 120, 57, 110, 107, 62, 179, 99], 9: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 4, 6, 3, 7, 170, 170, 170, 170, 170, 120, 57, 174, 183, 110, 110, 62, 179, 179, 179, 179, 179, 100, 145, 144, 144, 144, 144, 101, 141], 10: [-1, -1, 4, 170, 170, 170, 120, 84, 84, 84, 84, 84, 9, 179, 99, 141, 141, 141, 141, 141, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43], 11: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 170, 170, 57, 184, 184, 184, 184, 107, 143, 9, 45, 45, 54, 44, 35, 172, 172, 172, 172, 172, 172, 172, 172, 50, 50], 12: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 4, 4, 4, 49, 170, 170, 170, 170, 170, 170, 170, 57, 57, 174, 174, 174, 110, 110, 110, 110, 9, 162, 162, 179, 179, 99, 99, 99, 100, 144, 144, 144, 144, 144, 144, 144, 172, 172, 172, 50, 50, 86, 86, 86, 86, 86, 86, 86, 86, 71, 71, 71, 71, 71, 71, 71, 146, 146, 10, 10, 53, 52, 67, 140], 14: [-1, 168, 173], 15: [-1, -1, -1, -1, 4, 49, 170, 170, 170, 170, 170, 57, 174, 184, 110, 62, 62, 62, 62, 62, 62, 62, 62, 9, 9, 162, 168, 168, 168, 168, 45, 45, 45, 54, 44, 35, 71, 71], 16: [-1, -1, -1, -1, 4, 170, 57, 84, 84, 84, 84, 84, 183, 184, 184, 184, 110, 107, 62, 62, 62, 62, 62, 62, 62, 162, 162, 162, 168, 45, 54, 44, 35, 173, 173], 17: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 170, 170, 110, 110, 110, 110, 110, 114, 114, 168, 179, 179, 101, 173, 136, 136, 136, 136, 136, 136, 147, 147, 147, 147, 147, 147, 36, 22, 51, 51, 51, 106, 68, 181, 153, 153, 157, 122, 70, 70, 159, 58, 138, 138, 38, 89, 139], 18: [-1, -1, 100, 101, 146, 10, 10, 10, 67, 22, 51, 68, 122, 104, 95, 175, 177, 129, 129, 91, 26, 131, 21, 81, 59, 74, 88], 19: [-1, -1, -1, 49, 114, 145, 146, 68, 157, 70, 159, 159, 176, 79, 79, 79, 158, 158, 158, 158, 158, 42], 20: [-1, -1, -1, -1, 6, 3, 7, 174, 145, 146, 10, 10, 53, 52, 67, 67, 51, 51, 181, 153, 157, 70, 70, 159, 159, 38, 177, 21, 21, 81, 81, 158, 164, 164, 41, 41, 27, 27, 15, 150, 133, 152, 78], 21: [-1, -1, -1, -1, -1, -1, -1, 57, 100, 100, 101, 51, 106, 68, 122, 58, 138, 38, 139, 104, 95, 175, 88, 164, 164, 15, 156, 161, 130], 22: [-1, -1, -1, -1, -1, -1, 10, 10, 67, 67, 51, 21, 21, 81, 81, 74, 128], 23: [-1, -1, -1, -1, 4, 9, 141, 141, 136, 138, 138, 138, 89, 139, 81, 130, 130, 12, 12, 12, 77, 92, 92, 119, 119, 80, 127, 125, 125, 30], 24: [-1, -1, -1, -1, -1, -1, -1, 55, 55, 55, 4, 9, 54, 141, 141, 141, 22, 51, 122, 138, 138, 95, 91, 88, 150, 78, 161, 161, 77, 119, 119, 119, 119, 119, 119, 119, 119, 119, 108, 108, 90, 90, 96, 96, 148, 69], 25: [-1, -1, -1, -1, -1, -1, -1, 9, 9, 141, 141, 141, 141, 141, 141, 141, 141, 141, 10, 67, 140, 89, 91, 21, 81, 81, 42, 42, 78, 78, 130, 92, 119, 127, 30, 148, 148, 83, 83, 83, 83, 61, 102, 160, 160, 154, 48], 26: [-1, 4, 9, 45, 45, 54, 54, 35, 10, 67, 51, 21, 150, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 128, 30], 27: [-1, -1, -1, -1, -1, -1, 4, 114, 114, 141, 50, 81, 161, 161, 161, 161, 161, 128, 108, 108, 108, 108, 90, 90, 102], 28: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 4, 4, 49, 24, 143, 9, 9, 9, 114, 114, 141, 141, 51, 51, 106, 68, 58, 26, 131, 59, 59, 59, 78, 128, 119, 119, 80, 30, 30, 90, 61, 82, 60, 97, 56, 56, 66, 66, 32], 29: [-1, -1, -1, -1, 49, 141, 10, 10, 67, 51, 89, 91, 91, 21, 81, 130, 123, 123, 123, 124, 124, 149], 30: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 101, 101, 140, 68, 122, 104, 95, 175, 74, 88, 88, 42, 78, 78, 78, 130, 77, 77, 119, 119, 119, 119, 119, 119, 80, 56], 32: [-1, -1, -1, -1, -1, -1, -1, 24, 143, 143, 141, 136, 22, 22, 68, 68, 68, 138, 95, 177, 177, 91, 42, 77, 92, 119, 119, 30, 82, 60, 56, 182, 182, 63, 63, 5], 34: [-1, 49, 110, 24, 143, 141, 91, 81, 127, 125, 30, 82, 56, 66, 124, 63, 63, 5, 33, 17], 35: [-1, -1, -1, -1, 4, 4, 9, 9, 114, 10, 53, 67, 130, 77, 30, 96, 82, 63, 63, 5, 37, 46], 36: [-1, -1, -1, -1, -1, 9, 141, 141, 51, 91, 26, 81, 74, 92, 80, 123, 123, 124, 132, 34, 34, 34, 34, 34, 29], 37: [-1, -1, -1, -1, 4, 143, 153, 122, 129, 91, 131, 42, 78, 119, 108, 69, 66, 32, 29, 76], 38: [-1, 55, 55, 4, 9, 130, 90, 102, 102, 102, 102, 97, 66, 76, 75, 109, 109], 39: [-1, 141, 51, 129, 91, 91, 127, 166], 40: [-1, -1, -1, -1, 49, 143, 9, 50, 51, 12, 12, 119, 119, 69, 160, 160, 160, 160, 82, 97, 97, 66, 66, 32, 75, 166, 28, 28, 28, 23, 23, 23], 41: [-1, 143, 159, 129, 91, 152, 12, 127, 69, 61, 97, 109, 155, 134], 42: [-1, 4, 9, 70, 164, 133, 152, 82, 33, 33, 29, 109, 65, 47], 44: [-1, -1, -1, -1, -1, -1, -1, 4, 143, 143, 143, 9, 9, 10, 51, 51, 51, 159, 159, 129, 129, 91, 91, 41, 119, 127, 127, 160, 160, 160, 160, 160, 160, 160, 160, 160, 82, 60, 97, 97, 97, 32, 34, 34, 34, 109, 134, 151, 151, 151], 45: [-1, -1, -1, 4, 82, 29, 109, 47], 46: [-1, -1, -1, -1, -1, -1, 4, 143, 143, 143, 143, 9, 9, 50, 10, 51, 159, 159, 159, 38, 129, 91, 91, 26, 119, 127, 127, 148, 83, 83, 61, 160, 160, 160, 160, 160, 160, 154, 154, 154, 82, 82, 82, 60, 97, 97, 97, 97, 32, 32, 32, 132, 132, 34, 34, 34, 29, 29, 29, 75, 109, 109, 65, 65, 65, 65, 47, 72, 72, 72], 47: [-1, -1, -1, -1, -1, -1, -1, -1, -1, 51, 51, 38, 91, 74, 164, 164, 127, 125, 160, 154, 97, 97, 97, 97, 124, 34, 34, 151, 73, 25], 48: [-1, -1, -1, -1, -1, 141, 89, 26, 131, 131, 164, 164, 77, 5, 132, 14, 13, 1], 49: [-1, -1, -1, 54, 51, 68, 91, 26, 131, 176, 42, 78, 78, 127, 127, 125, 124, 132, 132, 132, 34, 34, 18], 50: [-1, -1, 51, 51, 106, 153, 122, 38, 26, 59, 150, 96, 37, 46, 11, 142], 51: [-1, 126, 24, 143, 9, 68, 95, 48, 56, 66, 182, 5, 33, 25, 14, 13, 16], 52: [-1, -1, -1, 4, 4, 126, 24, 143, 9, 9, 53, 52, 26, 12, 12, 12, 48, 82, 56, 66, 5, 37, 46, 25, 14, 13, 13, 1, 1, 18, 18, 16], 53: [-1, -1, -1, 4, 9, 26, 27, 128, 61, 48, 155, 73, 14, 13, 1, 8, 2, 19], 54: [-1, 4, 9, 50, 90, 17, 28, 155, 47, 25, 13, 1, 19], 57: [-1, -1, -1, -1, 4, 9, 101, 101, 101, 101, 38, 26, 74, 48, 132, 155, 14, 13, 13, 1, 16, 19], 59: [-1, 4, 4, 9, 9, 26, 131, 77, 48, 48, 149, 149, 33, 17, 28, 65, 72, 25, 14, 14, 13, 13, 1, 1, 16, 16, 19, 19, 103, 135], 64: [-1, -1, -1, -1, -1, 4, 4, 9, 9, 9, 51, 159, 159, 26, 131, 27, 27, 161, 61, 61, 160, 160, 160, 160, 160, 48, 48, 97, 32, 17, 132, 109, 28, 47, 73, 73, 25, 14, 14, 14, 13, 13, 13, 13, 13, 1, 1, 1, 1, 1, 16, 16, 16, 8, 8, 2, 2, 19, 19, 19, 19], 66: [-1, -1, 4, 126, 9, 100, 141, 141, 146, 53, 51, 159, 159, 38, 26, 26, 74, 150, 160, 160, 160, 160, 48, 97, 32, 149, 37, 37, 46, 46, 11, 17, 28, 25, 14, 13, 1, 142, 142, 16, 19, 135], 68: [-1, -1, -1, -1, -1, -1, -1, 24, 100, 50, 50, 50, 50, 51, 106, 38, 38, 26, 131, 131, 59, 74, 78, 90, 66, 32, 149, 149, 182, 37, 46, 29, 76, 76, 75, 155, 155], 74: [-1, -1, -1, -1, 145, 146, 10, 10, 67, 51, 51, 68, 122, 70, 89, 175, 129, 91, 21, 81, 74, 164, 15, 133, 152, 78, 130, 130, 130, 130, 132, 64], 75: [-1, -1, -1, 126, 24, 143, 146, 10, 67, 22, 106, 159, 58, 177, 21, 81, 158, 41, 15, 152, 128, 134], 76: [-1, -1, -1, 49, 24, 50, 51, 70, 70, 159, 91, 91, 164, 41, 133, 152, 69, 102, 102, 97, 97, 33, 132, 134, 151], 77: [-1, -1, -1, -1, -1, -1, -1, -1, 126, 24, 143, 145, 146, 10, 53, 52, 67, 22, 51, 68, 181, 153, 159, 159, 159, 177, 91, 21, 81, 158, 158, 42, 130, 160, 160, 160, 97, 109, 109, 64, 163], 78: [-1, -1, -1, -1, 6, 7, 51, 153, 157, 157, 157, 70, 70, 159, 38, 176, 79, 79, 79, 164, 41, 27, 102, 102, 102, 102], 79: [-1, -1, -1, -1, 3, 100, 145, 145, 101, 101, 146, 136, 22, 68, 68, 38, 139, 139, 95, 177, 74, 88, 27, 90, 148, 148, 83, 83, 154], 80: [-1, -1, -1, 181, 181, 157, 157, 158, 164, 161, 161], 81: [-1, 6, 3, 7, 114, 114, 114, 114, 22, 153, 153, 38, 164, 27, 27, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 169], 82: [-1, -1, -1, -1, -1, 9, 114, 45, 45, 54, 54, 35, 51, 153, 157, 164, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 34, 34], 83: [-1, -1, -1, -1, -1, 4, 54, 44, 153, 153, 38, 158, 164, 78, 161, 161, 161, 161, 161, 161, 161, 180, 180], 85: [-1, -1, -1, -1, 114, 114, 114, 181, 181, 153, 153, 158, 164, 164, 164, 161, 161, 161, 161, 161, 161], 87: [-1, -1, -1, -1, -1, 145, 146, 51, 51, 181, 153, 153, 70, 70, 159, 159, 138, 139, 177, 88, 158, 164, 41, 78, 161, 161, 161, 125, 148, 83], 88: [-1, -1, -1, 107, 107, 53, 52, 106, 68, 181, 181, 153, 153, 58, 38, 21, 81, 81, 59, 176, 158, 158, 164, 164, 164, 78, 161, 161, 161, 119, 109, 163, 163, 163, 163, 163, 163], 89: [-1, -1, -1, -1, -1, -1, -1, 174, 101, 146, 10, 67, 136, 136, 136, 147, 51, 181, 153, 153, 157, 122, 70, 159, 177, 21, 21, 81, 81, 176, 176, 158, 42, 164, 41, 161, 161], 90: [-1, -1, -1, 36, 181, 153, 157, 158, 164, 164, 41, 161, 161, 182, 163, 163, 163, 116, 116, 116, 116, 116, 118, 98, 98, 98, 98, 98], 91: [-1, -1, -1, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 118, 118, 118, 98, 113, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105], 92: [-1, -1, -1, 116, 118, 98, 98, 113, 105, 105, 112, 112, 112, 112, 112, 112], 93: [-1, 118, 118, 98, 98, 98, 98, 98, 98, 113, 113, 113, 112, 112, 112, 112, 112, 112, 112, 112, 112], 94: [-1, -1, -1, 135, 118, 118, 113, 113, 113, 39, 39, 39, 39, 39, 39, 39, 87, 121, 121, 121], 95: [-1, -1, -1, -1, -1, -1, -1, 170, 170, 120, 174, 110, 100, 135, 169, 180, 121, 121, 121, 137, 137, 137, 137, 137, 137, 137, 40, 40, 40, 40, 40, 40], 96: [-1, -1, -1, -1, -1, -1, 170, 170, 120, 120, 174, 110, 58, 139, 39, 39, 39, 39, 39, 39, 39, 39, 171, 171, 20, 20, 20, 20], 97: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 49, 6, 3, 7, 170, 120, 120, 174, 174, 174, 110, 145, 145, 145, 169, 171, 171, 171, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 165, 165, 165, 165, 165, 165, 165, 165], 98: [-1, 170, 170, 120, 179, 179, 179, 179, 179, 179, 179, 179, 179, 179, 145, 145, 145, 145, 144, 144, 144, 144, 144, 104, 123, 123, 123], 99: [-1, 170, 120, 174, 110, 110, 179, 179, 179, 179, 179, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 169, 169, 121, 121, 121, 121, 121, 178, 178, 178, 178, 178, 178, 178, 178], 100: [-1, -1, 170, 170, 170, 110, 110, 179, 123, 123, 169, 87, 87, 87, 121, 121, 121, 121, 121], 101: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 170, 170, 120, 84, 84, 84, 84, 84, 84, 84, 174, 174, 110, 179, 179, 99, 145, 146, 53, 139, 123, 166, 166, 166, 169, 169, 169, 87, 87, 87, 85, 85, 85, 85, 85, 85, 85], 102: [-1, -1, -1, -1, -1, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 110, 52, 166, 121, 121, 121, 121, 167], 103: [-1, -1, -1, -1, -1, -1, 170, 170, 170, 170, 120, 174, 174, 174, 174, 110, 99, 175], 104: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 120, 57, 110, 107, 107, 99, 71, 182, 182, 182, 182, 135, 137, 137, 137, 137, 137, 137, 137, 137, 85, 85], 105: [-1, -1, -1, -1, -1, 49, 57, 110, 110, 107, 99, 71, 71, 166, 166, 166, 166, 166, 137, 137, 137, 137, 85, 85, 167, 167, 167, 167, 167, 167], 33: [4, 9, 9, 10, 36, 91, 21, 81, 127, 125, 61, 56, 66, 66, 124, 33, 37, 46, 11], 43: [4, 143, 9, 129, 91, 152, 127, 69, 82, 97, 33, 33, 29, 109, 134, 65, 47, 47, 72], 58: [4, 9, 51, 153, 48, 17, 23, 23, 155, 14, 13, 13, 1, 1, 1, 18, 16, 19, 19], 60: [4, 4, 4, 126, 110, 24, 143, 9, 9, 100, 53, 52, 38, 26, 26, 27, 150, 152, 12, 12, 69, 48, 97, 33, 37, 37, 46, 46, 11, 23, 134, 73, 14, 13, 13, 1, 1, 18, 142, 142, 16, 8, 2, 19, 135], 61: [4, 4, 4, 126, 24, 143, 9, 9, 51, 26, 27, 150, 78, 12, 12, 102, 60, 60, 97, 97, 182, 37, 46, 11, 23, 134, 47, 151, 151, 151, 73, 14, 13, 13, 1, 1, 18, 142, 16, 8, 2, 19, 135], 63: [4, 4, 4, 9, 51, 26, 78, 12, 12, 12, 149, 33, 33, 37, 46, 8, 8], 65: [4, 23, 18], 69: [4, 4, 126, 24, 143, 9, 68, 27, 150, 78, 48, 97, 46, 11, 75, 23, 134, 151, 151, 73, 14, 13, 13, 1, 1, 18, 16, 8, 2, 19, 135], 86: [6, 3, 7, 38, 27, 27, 182, 182, 182, 180], 13: [170, 184, 184, 184, 162, 162, 168, 168, 168, 168, 168, 168, 168, 173, 173, 173], 84: [57, 51, 153, 157, 164, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 108], 70: [110, 71, 53, 52, 38, 46, 75], 31: [127, 125, 66, 124]}\n"
     ]
    }
   ],
   "source": [
    "print(inv_smap)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
