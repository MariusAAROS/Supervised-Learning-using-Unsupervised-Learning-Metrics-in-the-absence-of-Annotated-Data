{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\myLibraries\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimized Generalized Scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_score.utils import *\n",
    "from custom_score.score import score \n",
    "from rouge_score import rouge_scorer\n",
    "import bert_score\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "from colorama import Fore, Style\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append(get_git_root())\n",
    "\n",
    "from BARTScore.bart_score import BARTScorer\n",
    "\n",
    "class Refiner:\n",
    "\n",
    "    def __init__(self, corpus, gold, model=None, metric=score, ratio=2, threshold=0.70, maxSpacing=10, printRange=range(0, 1)):\n",
    "        \"\"\"\n",
    "        Constructor of the Refiner class. Aims at reducing the size and noise of a given independant list of documents.\n",
    "        \n",
    "        :param1 self (Refiner): Object to initialize.\n",
    "        :param2 corpus (List): List of documents to simplify.\n",
    "        :param3 gold (List): List of gold summaries to compare to the extractive summary created with the refiner.\n",
    "        :param4 model (Any): Model used to compute scores and create sentence's ranking.\n",
    "        :param5 ratio (float, int or array-like): Number determining how much the reference text will be shortened. \n",
    "        :param6 threshold (float): Number between 0 and 1 indicating the lowest acceptable quality when tuning the length of the summary.\n",
    "        :param7 maxSpacing (int): Maximal number of adjacent space to be found and suppressed in the corpus.\n",
    "        :param8 printRange (range): Range of corpus that should be displayed when the Refiner object in printed. \n",
    "        \"\"\"\n",
    "        self.corpus = corpus\n",
    "        self.gold = gold\n",
    "        self.processedCorpus = None\n",
    "        self.model = model\n",
    "        self.metric = metric\n",
    "        self.ratio = ratio\n",
    "        self.threshold = threshold\n",
    "        self.ms = maxSpacing\n",
    "        self.refined = None\n",
    "        self.printRange = printRange\n",
    "        self.selectedIndexes = None\n",
    "\n",
    "    def refine(self, checkpoints=False, saveRate=50):\n",
    "        \"\"\"\n",
    "        Return a reduced string computed using static embedding vectors similarity. Also denoises the data by removing superfluous elements such as \"\\n\" or useless signs.\n",
    "\n",
    "        :param1 self (Refiner): Refiner Object (see __init__ function for more details).\n",
    "        :param2 checkpoints (bool): Indicates whether the refining should save partial outputs along computation to prevent from losing data in the context of a crash.\n",
    "        :param3 saveRate (int): Only applicable id safe equals True. Specify the number of consicutive iterations after which a checkpoint should be created. \n",
    "\n",
    "        :output refined (string): refined version of the initial document.\n",
    "        \"\"\"\n",
    "        self.refined = []\n",
    "        self.selectedIndexes = []\n",
    "        self.processedCorpus = []\n",
    "        if checkpoints:\n",
    "            iter = 0\n",
    "            start = datetime.now()\n",
    "            createFolder = True\n",
    "\n",
    "        for indiv in self.corpus:\n",
    "            #preprocess corpus\n",
    "            #clean = cleanString(indiv, self.ms)\n",
    "            clean = indiv\n",
    "            sentences = clean.split(\".\")\n",
    "            sentences.pop()\n",
    "            temp = []\n",
    "            for sentence in sentences: \n",
    "                if sentence != None and sentence != \"\":\n",
    "                    temp.append(sentence)\n",
    "            sentences = temp\n",
    "            respaced_sentences = []\n",
    "            for sentence in sentences:\n",
    "                if sentence[0] == \" \":\n",
    "                    sentence = sentence[1:]\n",
    "                respaced_sentences.append(sentence)\n",
    "            self.processedCorpus.append(respaced_sentences)\n",
    "\n",
    "            #compute ranking\n",
    "            scores = []\n",
    "            formated_refs = []\n",
    "            formated_cands = []\n",
    "            for sentence in respaced_sentences:\n",
    "                formated_refs.append(indiv.replace(sentence+\".\", \"\"))\n",
    "                formated_cands.append(sentence)\n",
    "                #scoreOut = self.scorer(indiv.replace(sentence+\".\", \"\"), sentence)\n",
    "                #scores.append(scoreOut)\n",
    "            scores = self.scorer(formated_refs, formated_cands)\n",
    "\n",
    "            #compute distances\n",
    "            distances = []\n",
    "            for x in range(len(respaced_sentences)):\n",
    "                try:\n",
    "                    distance = self.scorer(respaced_sentences[x]*len(respaced_sentences), respaced_sentences)\n",
    "                except:\n",
    "                    distance = [-1]*len(respaced_sentences)\n",
    "                distances.append(distance)\n",
    "            distances = parseDistances(distances)\n",
    "\n",
    "            #selection of best individuals\n",
    "            indices = None\n",
    "            if type(self.ratio) == int or type(self.ratio) == float: \n",
    "                indices = sentenceSelection(respaced_sentences, scores, distances, self.ratio)\n",
    "            else:\n",
    "                for curRatio in sorted(self.ratio):\n",
    "                    curIndices = sentenceSelection(respaced_sentences, scores, distances, curRatio)\n",
    "                    subCurRefined = [respaced_sentences[i] for i in curIndices]\n",
    "                    curSentence = \" \".join(subCurRefined)\n",
    "                    curScore = self.scorer(indiv.replace(curSentence+\".\", \"\"), curSentence) #potentiellement faux \n",
    "                    if curScore < self.threshold:\n",
    "                        try:\n",
    "                            indices = curBest\n",
    "                        except:\n",
    "                            indices = curIndices\n",
    "                        finally:\n",
    "                            break\n",
    "                    else:\n",
    "                        curBest = curIndices\n",
    "                if indices is None:\n",
    "                    indices = curIndices\n",
    "            indices.sort()\n",
    "            curRefined = []\n",
    "            for index in indices:\n",
    "                curRefined.append(respaced_sentences[index])\n",
    "            curRefined = \". \\n\".join(curRefined) + \".\"\n",
    "            self.selectedIndexes.append(indices)\n",
    "            self.refined.append(curRefined)\n",
    "\n",
    "            #checkpoint verification\n",
    "            if checkpoints:\n",
    "                if iter % saveRate == 0 and iter != 0:\n",
    "                    stop = datetime.now()\n",
    "                    partial_runtime = stop - start\n",
    "                    self.save(runtime=partial_runtime, new=createFolder)\n",
    "                    createFolder = False\n",
    "                iter += 1\n",
    "        if checkpoints:\n",
    "            stop = datetime.now()\n",
    "            runtime = stop - start\n",
    "            self.save(runtime=runtime, new=createFolder)\n",
    "\n",
    "    def scorer(self, refs, cands, param=\"F\"):\n",
    "        param = param.upper()\n",
    "        if self.metric.__module__ == \"custom_score.score\":\n",
    "            if self.model == None:\n",
    "                self.model = model_load(\"Word2Vec\", True)\n",
    "            scores = self.metric(self.model, cands, refs)[0]\n",
    "            R, P, F = []\n",
    "            for score in scores:\n",
    "                R.append(score[0])\n",
    "                P.append(score[1])\n",
    "                F.append(score[2])\n",
    "            \n",
    "        elif self.metric.__module__ == \"bert_score.score\":\n",
    "            with nostd():\n",
    "                scores = self.metric(cands, refs, lang=\"en\", verbose=0)\n",
    "            P = scores[0].tolist()\n",
    "            R = scores[1].tolist()\n",
    "            F = scores[2].tolist()\n",
    "      \n",
    "        if param == \"F\":\n",
    "            output = F\n",
    "        elif param == \"R\":\n",
    "            output = R\n",
    "        elif param == \"P\":\n",
    "            output = P\n",
    "        elif param == \"ALL\":\n",
    "            output = (R, P, F)\n",
    "        return output\n",
    "\n",
    "    def assess(self, start=0, stop=None, verbose=True):\n",
    "        \"\"\"\n",
    "        Assesses quality of the refined corpus by computing Static BERTscore and Rouge-Score on the refined version compared to it's initial version.\n",
    "\n",
    "        :param1 self (Refiner): Refiner Object (see __init__ function for more details).\n",
    "        :param2 start (int): Starting index to assess.\n",
    "        :param3 stop (int): Ending index to assess.\n",
    "        :param4 verbose (Boolean): When put to True, assess results will be printed.\n",
    "\n",
    "        :output (dict): Dictionnary containing both the scores of Static BERTScore, BERTScore, BARTScore and Rouge as well as their correlation.\n",
    "        \"\"\"\n",
    "        assert self.refined != None, \"refined corpus doesn't exists\"\n",
    "        \n",
    "        if stop == None:\n",
    "            stop = len(self.refined)\n",
    "        subset_refined = self.refined[start:stop]\n",
    "        subset_gold = self.gold[start:stop]\n",
    "\n",
    "        #Static BERTScore computation\n",
    "        scoreOut = score(self.model, subset_refined, subset_gold)\n",
    "        customScore = [parseScore(curScore) for curScore in scoreOut]\n",
    "\n",
    "        #Rouge-Score computation\n",
    "        rougeScorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        rougeScore = [rougeScorer.score(c, r) for c, r in zip(subset_gold, subset_refined)]\n",
    "\n",
    "        #BERTScore computation\n",
    "        with nostd():\n",
    "            bertscore = bert_score.score(subset_refined, subset_gold, lang=\"en\", verbose=0)\n",
    "\n",
    "        #bartscore\n",
    "        bart_scorer = BARTScorer(device='cuda:0', checkpoint='facebook/bart-large-cnn')\n",
    "        bartscore = bart_scorer.score(subset_refined, subset_gold, batch_size=4)\n",
    "\n",
    "        #Data formating\n",
    "        custom_R = [round(t, 2) for t in customScore]\n",
    "        bertscore_R = [round(t.item(), 2) for t in bertscore[1]]\n",
    "        bartscore = [round(t, 2) for t in bartscore]\n",
    "        rouge1_R = [round(t['rouge1'][0], 2) for t in rougeScore]\n",
    "        rouge2_R = [round(t['rouge2'][0], 2) for t in rougeScore]\n",
    "        rougeL_R = [round(t['rougeL'][0], 2) for t in rougeScore]\n",
    "\n",
    "        dfCustom = pd.DataFrame({'CBERT' : custom_R,\n",
    "                                 'BERTScore' : bertscore_R,\n",
    "                                 'BARTScore' : bartscore,\n",
    "                                 'R-1' : rouge1_R,\n",
    "                                 'R-2' : rouge2_R,\n",
    "                                 'R-L' : rougeL_R\n",
    "                                })\n",
    "\n",
    "        #Correlation estimation\n",
    "        pearsonCor_c_r1 = np.round(pearsonr(custom_R, rouge1_R), 2)\n",
    "        pearsonCor_c_r2 = np.round(pearsonr(custom_R, rouge2_R), 2)\n",
    "        pearsonCor_c_rl = np.round(pearsonr(custom_R, rougeL_R), 2)\n",
    "        pearsonCor_bertscore_r1 = np.round(pearsonr(bertscore_R, rouge1_R), 2)\n",
    "        pearsonCor_bertscore_r2 = np.round(pearsonr(bertscore_R, rouge2_R), 2)\n",
    "        pearsonCor_bertscore_rl = np.round(pearsonr(bertscore_R, rougeL_R), 2)\n",
    "        pearsonCor_bartscore_r1 = np.round(pearsonr(bartscore, rouge1_R), 2)\n",
    "        pearsonCor_bartscore_r2 = np.round(pearsonr(bartscore, rouge2_R), 2)\n",
    "        pearsonCor_bartscore_rl = np.round(pearsonr(bartscore, rougeL_R), 2)\n",
    "\n",
    "        dfCor = pd.DataFrame({'pearson_CBERT_R-1' : pearsonCor_c_r1,\n",
    "                              'pearson_CBERT_R-2' : pearsonCor_c_r2,\n",
    "                              'pearson_CBERT_R-L' : pearsonCor_c_rl,\n",
    "                              'pearson_BERT_R-1' : pearsonCor_bertscore_r1,\n",
    "                              'pearson_BERT_R-2' : pearsonCor_bertscore_r2,\n",
    "                              'pearson_BERT_R-l' : pearsonCor_bertscore_rl,\n",
    "                              'pearson_BART_R-1' : pearsonCor_bartscore_r1,\n",
    "                              'pearson_BART_R-2' : pearsonCor_bartscore_r2,\n",
    "                              'pearson_BART_R-l' : pearsonCor_bartscore_rl}, index=[\"Pearson score\", \"p-value\"])\n",
    "        if verbose:\n",
    "            printout = \"Scores: \\n\"\n",
    "            printout += dfCustom.to_string().decode(\"utf8\") + \"\\n\\n\"\n",
    "            printout += \"Correlations: \\n\"\n",
    "            printout += dfCor.to_string().decode(\"utf8\")\n",
    "            print(printout)\n",
    "\n",
    "        return {\"scores\": dfCustom, \"correlations\": dfCor}\n",
    "    \n",
    "    def to_dataframe(self):\n",
    "        \"\"\"\n",
    "        Transforms a Refiner object to a dataframe.\n",
    "\n",
    "        :param1 self (Refiner): Refiner Object (see __init__ function for more details).\n",
    "\n",
    "        :output output (DataFrame): DataFrame containing both the corpus and the refined texts of the Refiner class. \n",
    "        \"\"\"\n",
    "        output = pd.DataFrame({\"text\": self.corpus,\n",
    "                               \"summary\": self.refined,\n",
    "                               \"processedText\": [\". \".join(c) for c in self.processedCorpus]})\n",
    "        return output\n",
    "\n",
    "    def save(self, runtime=None, new=True):\n",
    "        \"\"\"\n",
    "        Saves Refiner output to a local folder.\n",
    "\n",
    "        :param1 self (Refiner): Refiner Object (see __init__ function for more details).\n",
    "        :param2 new (bool): Indicates if a new folder should be created. If false, output is append to the most recent ouput folder.\n",
    "        \"\"\"\n",
    "\n",
    "        #evaluation\n",
    "        start = 0\n",
    "        stop = len(self.refined)\n",
    "        assessement = self.assess(start=start, stop=stop)\n",
    "\n",
    "        #mainDf = r.to_dataframe()\n",
    "        scoreDf = assessement[\"scores\"]\n",
    "        corDf = assessement[\"correlations\"]\n",
    "\n",
    "        #write output\n",
    "        main_folder_path = os.path.join(get_git_root(), r\"myLibraries\\refining_output\")\n",
    "        countfile_name = r\"count.txt\"\n",
    "        if new:\n",
    "            count = updateFileCount(os.path.join(main_folder_path, countfile_name))\n",
    "        else:\n",
    "            count = readFileCount(os.path.join(main_folder_path, countfile_name))\n",
    "\n",
    "        current_path = os.path.join(main_folder_path, f\"experimentation_{count}\")\n",
    "        try:\n",
    "            os.mkdir(current_path)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "\n",
    "        #mainDf.to_csv(os.path.join(current_path, \"main.csv\"))\n",
    "        scoreDf.to_csv(os.path.join(current_path, \"scores.csv\"))\n",
    "        corDf.to_csv(os.path.join(current_path, \"correlations.csv\"))\n",
    "        with open(os.path.join(current_path, \"runtimes.txt\"), \"w\") as f:\n",
    "            f.write(str(runtime))\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"\n",
    "        Summarizes Refiner object to a string.\n",
    "\n",
    "        :param1 self (Refiner): Refiner Object (see __init__ function for more details).\n",
    "\n",
    "        :output printout (string): Summarized informations about the refiner object.\n",
    "        \"\"\"\n",
    "\n",
    "        printout = \"--------REFINER OBJECT--------\\n\\n\"\n",
    "        printout += \"Number of Documents : \" + str(len(self.corpus)) + \"\\n\"\n",
    "        printout += \"Corpus Avg Size     : \" + str(int(np.average([len(x) for x in self.corpus]))+1) + \"\\n\"\n",
    "        printout += \"Refined Avg Size    : \" + str(int(np.average([len(x) for x in self.refined]))+1) + \"\\n\"\n",
    "        printout += \"Ratio(s)            : \" + str(self.ratio) + \"\\n\"\n",
    "        printout += \"Threshold           : \" + str(self.threshold) + \"\\n\"\n",
    "        printout += \"Maximum Spacing     : \" + str(self.ms) + \"\\n\"\n",
    "        \n",
    "        self.printRange = self.printRange if self.printRange.start >= 0 and self.printRange.stop < len(self.processedCorpus) else range(0, len(self.processedCorpus))\n",
    "\n",
    "        for index in self.printRange:\n",
    "            printout += f\"\\nCorpus no.{index+1} : \\n\" + str(\".\\n\".join([f\"{Fore.LIGHTGREEN_EX}{self.processedCorpus[index][i]}{Style.RESET_ALL}\"\n",
    "                                                        if i in self.selectedIndexes[index]\n",
    "                                                        else f\"{Fore.RED}{self.processedCorpus[index][i]}{Style.RESET_ALL}\"\n",
    "                                                        for i in range(len(self.processedCorpus[index]))])) + \".\" + \"\\n\"\n",
    "        printout += \"\\n------------------------------\"\n",
    "        return printout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = model_load(\"Word2Vec\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Refiner([\"I am Marius. I like trains.\", \"Engineering is good. It is fun.\"], [\"My name is Marius. I think trains are cool.\", \"I enjoy datascience. It is my studies.\"], w2v, metric=bert_score.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.refine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_=r.assess()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refiner .py Test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see file <refine_tests_2.py>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\myLibraries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from refine_tests_2 import Refiner\n",
    "from custom_score.utils import model_load\n",
    "from custom_score.score import score\n",
    "import bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = model_load(\"Word2Vec\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Refiner([\"I am Marius. I like trains.\", \"Engineering is good. It is fun.\"], [\"My name is Marius. I think trains are cool.\", \"I enjoy datascience. It is my studies.\"], w2v, metric=bert_score.score, ratio=[2, 3])\n",
    "r.refine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = r.assess()\n",
    "print(r)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized Refiner - Test on Billsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\myLibraries\")\n",
    "\n",
    "from refine_tests_2 import Refiner\n",
    "from custom_score.utils import model_load\n",
    "from custom_score.score import score\n",
    "import bert_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url=\"https://drive.google.com/file/d/1Wd0M3qepNF6B4YwFYrpo7CaSERpudAG_/view?usp=share_link\"\n",
    "dataset_url='https://drive.google.com/uc?export=download&id=' + dataset_url.split('/')[-2]\n",
    "dataset = pd.read_json(dataset_url, lines=True)\n",
    "dataset = dataset.loc[:, [\"text\", \"summary\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = dataset.iloc[:2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = model_load(\"Word2Vec\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Refiner(subset[\"text\"].to_list(), subset[\"summary\"].to_list(), w2v, metric=bert_score.score)\n",
    "r.refine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.assess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\myLibraries\")\n",
    "sys.path.append(r\"C:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\myLibraries\\datasets\")\n",
    "from custom_score.refine import Refiner\n",
    "from custom_score.utils import model_load\n",
    "from custom_score.score import score\n",
    "import bert_score\n",
    "from loaders import load_billsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "billsum = load_billsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = billsum.iloc[:2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = model_load(\"Word2Vec\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Refiner(subset[\"text\"].to_list(), subset[\"summary\"].to_list(), w2v, metric=score)\n",
    "r.refine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: \n",
      "   CBERT  BERTScore  BARTScore   R-1   R-2   R-L\n",
      "0   0.91       0.89       0.43  0.07  0.04  0.06\n",
      "1   0.91       0.89       0.43  0.46  0.24  0.27\n",
      "\n",
      "Correlations: \n",
      "               pearson_CBERT_R-1  pearson_CBERT_R-2  pearson_CBERT_R-L  pearson_BERT_R-1  pearson_BERT_R-2  pearson_BERT_R-l  pearson_BART_R-1  pearson_BART_R-2  pearson_BART_R-l\n",
      "Pearson score                NaN                NaN                NaN               NaN               NaN               NaN               NaN               NaN               NaN\n",
      "p-value                      NaN                NaN                NaN               NaN               NaN               NaN               NaN               NaN               NaN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "_=r.assess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------REFINER OBJECT--------\n",
      "\n",
      "Number of Documents : 2\n",
      "Corpus Avg Size     : 5949\n",
      "Refined Avg Size    : 3241\n",
      "Ratio(s)            : 2\n",
      "Threshold           : 0.7\n",
      "Maximum Spacing     : 10\n",
      "\n",
      "Corpus no.1 : \n",
      "\u001b[92mSECTION 1\u001b[0m.\n",
      "\u001b[31mSHORT TITLE\u001b[0m.\n",
      "\u001b[92mThis Act may be cited as the National Science Education Tax Incentive for Businesses Act of 2007\u001b[0m.\n",
      "\u001b[92mSEC\u001b[0m.\n",
      "\u001b[92m2\u001b[0m.\n",
      "\u001b[92mCREDITS FOR CERTAIN CONTRIBUTIONS BENEFITING SCIENCE, TECHNOLOGY, ENGINEERING, AND MATHEMATICS EDUCATION AT THE ELEMENTARY AND SECONDARY SCHOOL LEVEL\u001b[0m.\n",
      "\u001b[92m(a) In General\u001b[0m.\n",
      "\u001b[92mSubpart D of part IV of subchapter A of chapter 1 of the Internal Revenue Code of 1986 (relating to business related credits) is amended by adding at the end the following new section: SEC\u001b[0m.\n",
      "\u001b[31m45O\u001b[0m.\n",
      "\u001b[31mCONTRIBUTIONS BENEFITING SCIENCE, TECHNOLOGY, ENGINEERING, AND MATHEMATICS EDUCATION AT THE ELEMENTARY AND SECONDARY SCHOOL LEVEL\u001b[0m.\n",
      "\u001b[92m(a) In General\u001b[0m.\n",
      "\u001b[92mFor purposes of section 38, the elementary and secondary science, technology, engineering, and mathematics (STEM) contributions credit determined under this section for the taxable year is an amount equal to 100 percent of the qualified STEM contributions of the taxpayer for such taxable year\u001b[0m.\n",
      "\u001b[92m(b) Qualified STEM Contributions\u001b[0m.\n",
      "\u001b[92mFor purposes of this section, the term `qualified STEM contributions' means (1) STEM school contributions, (2) STEM teacher externship expenses, and (3) STEM teacher training expenses\u001b[0m.\n",
      "\u001b[31m(c) STEM School Contributions\u001b[0m.\n",
      "\u001b[92mFor purposes of this section (1) In general\u001b[0m.\n",
      "\u001b[92mThe term `STEM school contributions' means (A) STEM property contributions, and (B) STEM service contributions\u001b[0m.\n",
      "\u001b[31m(2) STEM property contributions\u001b[0m.\n",
      "\u001b[31mThe term `STEM property contributions' means the amount which would (but for subsection (f)) be allowed as a deduction under section 170 for a charitable contribution of STEM inventory property if (A) the donee is an elementary or secondary school described in section 170(b)(1)(A)(ii), (B) substantially all of the use of the property by the donee is within the United States or within the defense dependents' education system for educational purposes in any of the grades K12 that are related to the purpose or function of the donee, (C) the original use of the property begins with the donee, (D) the property will fit productively into the donee's education plan, (E) the property is not transferred by the donee in exchange for money, other property, or services, except for shipping, installation and transfer costs, and (F) the donee's use and disposition of the property will be in accordance with the provisions of subparagraphs (B) and (E)\u001b[0m.\n",
      "\u001b[92mThe determination of the amount of deduction under section 170 for purposes of this paragraph shall be made as if the limitation under section 170(e)(3)(B) applied to all STEM inventory property\u001b[0m.\n",
      "\u001b[92m(3) STEM service contributions\u001b[0m.\n",
      "\u001b[92mThe term `STEM service contributions' means the amount paid or incurred during the taxable year for STEM services provided in the United States or in the defense dependents' education system for the exclusive benefit of students at an elementary or secondary school described in section 170(b)(1)(A)(ii) but only if (A) the taxpayer is engaged in the trade or business of providing such services on a commercial basis, and (B) no charge is imposed for providing such services\u001b[0m.\n",
      "\u001b[31m(4) STEM inventory property\u001b[0m.\n",
      "\u001b[92mThe term `STEM inventory property' means, with respect to any contribution to a school, any property (A) which is described in paragraph (1) or (2) of section 1221(a) with respect to the donor, and (B) which is determined by the school to be needed by the school in providing education in grades K12 in the areas of science, technology, engineering, or mathematics\u001b[0m.\n",
      "\u001b[31m(5) STEM services\u001b[0m.\n",
      "\u001b[31mThe term `STEM services' means, with respect to any contribution to a school, any service determined by the school to be needed by the school in providing education in grades K12 in the areas of science, technology, engineering, or mathematics, including teaching courses of instruction at such school in any such area\u001b[0m.\n",
      "\u001b[92m(6) Defense dependents' education system\u001b[0m.\n",
      "\u001b[92mFor purposes of this subsection, the term `defense dependents' education system' means the program established and operated under the Defense Dependents' Education Act of 1978 (20 U\u001b[0m.\n",
      "\u001b[31mS\u001b[0m.\n",
      "\u001b[31mC\u001b[0m.\n",
      "\u001b[31m921 et seq\u001b[0m.\n",
      "\u001b[92m)\u001b[0m.\n",
      "\u001b[31m(d) STEM Teacher Externship Expenses\u001b[0m.\n",
      "\u001b[31mFor purposes of this section (1) In general\u001b[0m.\n",
      "\u001b[92mThe term `STEM teacher externship expenses' means any amount paid or incurred to carry out a STEM externship program of the taxpayer but only to the extent that such amount is attributable to the participation in such program of any eligible STEM teacher, including amounts paid to such a teacher as a stipend while participating in such program\u001b[0m.\n",
      "\u001b[31m(2) STEM externship program\u001b[0m.\n",
      "\u001b[31mThe term `STEM externship program' means any program (A) established by a taxpayer engaged in a trade or business within an area of science, technology, engineering, or mathematics, and (B) under which eligible STEM teachers receive training to enhance their teaching skills in the areas of science, technology, engineering, or mathematics or otherwise improve their knowledge in such areas\u001b[0m.\n",
      "\u001b[92m(3) Eligible stem teacher\u001b[0m.\n",
      "\u001b[92mThe term `eligible STEM teacher' means any individual (A) who is a teacher in grades K12 at an educational organization described in section 170(b)(1)(A)(ii) which is located in the United States or which is located on a United States military base outside the United States, and (B) whose teaching responsibilities at such school include, or are likely to include, any course in the areas of science, technology, engineering, or mathematics\u001b[0m.\n",
      "\u001b[31m(e) STEM Teacher Training Expenses\u001b[0m.\n",
      "\u001b[92mThe term `STEM teacher training expenses' means any amount paid or incurred by a taxpayer engaged in a trade or business within an area of science, technology, engineering, or mathematics which is attributable to the participation of any eligible STEM teacher in a regular training program provided to employees of the taxpayer which is determined by such teacher's school as enhancing such teacher's teaching skills in the areas of science, technology, engineering, or mathematics\u001b[0m.\n",
      "\u001b[31m(f) Denial of Double Benefit\u001b[0m.\n",
      "\u001b[92mNo deduction shall be allowed under this chapter for any amount allowed as a credit under this section\u001b[0m.\n",
      "\u001b[31m(b) Conforming Amendments\u001b[0m.\n",
      "\u001b[31m(1) Section 38(b) of such Code is amended by striking plus at the end of paragraph (30), by striking the period at the end of paragraph (31), and inserting , plus, and by adding at the end the following new paragraph: (32) the elementary and secondary science, technology, engineering, and mathematics (STEM) contributions credit determined under section 45O\u001b[0m.\n",
      "\u001b[31m(2) The table of sections for subpart D of part IV of subchapter A of chapter 1 of such Code is amended by adding at the end the following new item: Sec\u001b[0m.\n",
      "\u001b[31m45O\u001b[0m.\n",
      "\u001b[31mContributions benefiting science, technology, engineering, and mathematics education at the elementary and secondary school level\u001b[0m.\n",
      "\u001b[31m(c) Effective Date\u001b[0m.\n",
      "\u001b[31mThe amendments made by this section shall apply to taxable years beginning after the date of the enactment of this Act\u001b[0m.\n",
      "\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
