{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\myLibraries\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimized Generalized Scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_score.utils import *\n",
    "from custom_score.score import score \n",
    "from rouge_score import rouge_scorer\n",
    "import bert_score\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "from colorama import Fore, Style\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append(get_git_root())\n",
    "\n",
    "from BARTScore.bart_score import BARTScorer\n",
    "\n",
    "class Refiner:\n",
    "\n",
    "    def __init__(self, corpus, gold, model=None, metric=score, ratio=2, threshold=0.70, maxSpacing=10, printRange=range(0, 1)):\n",
    "        \"\"\"\n",
    "        Constructor of the Refiner class. Aims at reducing the size and noise of a given independant list of documents.\n",
    "        \n",
    "        :param1 self (Refiner): Object to initialize.\n",
    "        :param2 corpus (List): List of documents to simplify.\n",
    "        :param3 gold (List): List of gold summaries to compare to the extractive summary created with the refiner.\n",
    "        :param4 model (Any): Model used to compute scores and create sentence's ranking.\n",
    "        :param5 ratio (float, int or array-like): Number determining how much the reference text will be shortened. \n",
    "        :param6 threshold (float): Number between 0 and 1 indicating the lowest acceptable quality when tuning the length of the summary.\n",
    "        :param7 maxSpacing (int): Maximal number of adjacent space to be found and suppressed in the corpus.\n",
    "        :param8 printRange (range): Range of corpus that should be displayed when the Refiner object in printed. \n",
    "        \"\"\"\n",
    "        self.corpus = corpus\n",
    "        self.gold = gold\n",
    "        self.processedCorpus = None\n",
    "        self.model = model\n",
    "        self.metric = metric\n",
    "        self.ratio = ratio\n",
    "        self.threshold = threshold\n",
    "        self.ms = maxSpacing\n",
    "        self.refined = None\n",
    "        self.printRange = printRange\n",
    "        self.selectedIndexes = None\n",
    "\n",
    "    def refine(self, checkpoints=False, saveRate=50):\n",
    "        \"\"\"\n",
    "        Return a reduced string computed using static embedding vectors similarity. Also denoises the data by removing superfluous elements such as \"\\n\" or useless signs.\n",
    "\n",
    "        :param1 self (Refiner): Refiner Object (see __init__ function for more details).\n",
    "        :param2 checkpoints (bool): Indicates whether the refining should save partial outputs along computation to prevent from losing data in the context of a crash.\n",
    "        :param3 saveRate (int): Only applicable id safe equals True. Specify the number of consicutive iterations after which a checkpoint should be created. \n",
    "\n",
    "        :output refined (string): refined version of the initial document.\n",
    "        \"\"\"\n",
    "        self.refined = []\n",
    "        self.selectedIndexes = []\n",
    "        self.processedCorpus = []\n",
    "        if checkpoints:\n",
    "            iter = 0\n",
    "            start = datetime.now()\n",
    "            createFolder = True\n",
    "\n",
    "        for indiv in self.corpus:\n",
    "            #preprocess corpus\n",
    "            #clean = cleanString(indiv, self.ms)\n",
    "            clean = indiv\n",
    "            sentences = clean.split(\".\")\n",
    "            sentences.pop()\n",
    "            temp = []\n",
    "            for sentence in sentences: \n",
    "                if sentence != None and sentence != \"\":\n",
    "                    temp.append(sentence)\n",
    "            sentences = temp\n",
    "            respaced_sentences = []\n",
    "            for sentence in sentences:\n",
    "                if sentence[0] == \" \":\n",
    "                    sentence = sentence[1:]\n",
    "                respaced_sentences.append(sentence)\n",
    "            self.processedCorpus.append(respaced_sentences)\n",
    "\n",
    "            #compute ranking\n",
    "            scores = []\n",
    "            formated_refs = []\n",
    "            formated_cands = []\n",
    "            for sentence in respaced_sentences:\n",
    "                formated_refs.append(indiv.replace(sentence+\".\", \"\"))\n",
    "                formated_cands.append(sentence)\n",
    "                #scoreOut = self.scorer(indiv.replace(sentence+\".\", \"\"), sentence)\n",
    "                #scores.append(scoreOut)\n",
    "            scores = self.scorer(formated_refs, formated_cands)\n",
    "\n",
    "            #compute distances\n",
    "            distances = []\n",
    "            for x in range(len(respaced_sentences)):\n",
    "                try:\n",
    "                    distance = self.scorer(respaced_sentences[x]*len(respaced_sentences), respaced_sentences)\n",
    "                except:\n",
    "                    distance = [-1]*len(respaced_sentences)\n",
    "                distances.append(distance)\n",
    "            distances = parseDistances(distances)\n",
    "\n",
    "            #selection of best individuals\n",
    "            indices = None\n",
    "            if type(self.ratio) == int or type(self.ratio) == float: \n",
    "                indices = sentenceSelection(respaced_sentences, scores, distances, self.ratio)\n",
    "            else:\n",
    "                for curRatio in sorted(self.ratio):\n",
    "                    curIndices = sentenceSelection(respaced_sentences, scores, distances, curRatio)\n",
    "                    subCurRefined = [respaced_sentences[i] for i in curIndices]\n",
    "                    curSentence = \" \".join(subCurRefined)\n",
    "                    curScore = self.scorer(indiv.replace(curSentence+\".\", \"\"), curSentence) #potentiellement faux \n",
    "                    if curScore < self.threshold:\n",
    "                        try:\n",
    "                            indices = curBest\n",
    "                        except:\n",
    "                            indices = curIndices\n",
    "                        finally:\n",
    "                            break\n",
    "                    else:\n",
    "                        curBest = curIndices\n",
    "                if indices is None:\n",
    "                    indices = curIndices\n",
    "            indices.sort()\n",
    "            curRefined = []\n",
    "            for index in indices:\n",
    "                curRefined.append(respaced_sentences[index])\n",
    "            curRefined = \". \\n\".join(curRefined) + \".\"\n",
    "            self.selectedIndexes.append(indices)\n",
    "            self.refined.append(curRefined)\n",
    "\n",
    "            #checkpoint verification\n",
    "            if checkpoints:\n",
    "                if iter % saveRate == 0 and iter != 0:\n",
    "                    stop = datetime.now()\n",
    "                    partial_runtime = stop - start\n",
    "                    self.save(runtime=partial_runtime, new=createFolder)\n",
    "                    createFolder = False\n",
    "                iter += 1\n",
    "        if checkpoints:\n",
    "            stop = datetime.now()\n",
    "            runtime = stop - start\n",
    "            self.save(runtime=runtime, new=createFolder)\n",
    "\n",
    "    def scorer(self, refs, cands, param=\"F\"):\n",
    "        param = param.upper()\n",
    "        if self.metric.__module__ == \"custom_score.score\":\n",
    "            if self.model == None:\n",
    "                self.model = model_load(\"Word2Vec\", True)\n",
    "            scores = self.metric(self.model, cands, refs)[0]\n",
    "            R, P, F = []\n",
    "            for score in scores:\n",
    "                R.append(score[0])\n",
    "                P.append(score[1])\n",
    "                F.append(score[2])\n",
    "            \n",
    "        elif self.metric.__module__ == \"bert_score.score\":\n",
    "            with nostd():\n",
    "                scores = self.metric(cands, refs, lang=\"en\", verbose=0)\n",
    "            P = scores[0].tolist()\n",
    "            R = scores[1].tolist()\n",
    "            F = scores[2].tolist()\n",
    "      \n",
    "        if param == \"F\":\n",
    "            output = F\n",
    "        elif param == \"R\":\n",
    "            output = R\n",
    "        elif param == \"P\":\n",
    "            output = P\n",
    "        elif param == \"ALL\":\n",
    "            output = (R, P, F)\n",
    "        return output\n",
    "\n",
    "    def assess(self, start=0, stop=None, verbose=True):\n",
    "        \"\"\"\n",
    "        Assesses quality of the refined corpus by computing Static BERTscore and Rouge-Score on the refined version compared to it's initial version.\n",
    "\n",
    "        :param1 self (Refiner): Refiner Object (see __init__ function for more details).\n",
    "        :param2 start (int): Starting index to assess.\n",
    "        :param3 stop (int): Ending index to assess.\n",
    "        :param4 verbose (Boolean): When put to True, assess results will be printed.\n",
    "\n",
    "        :output (dict): Dictionnary containing both the scores of Static BERTScore, BERTScore, BARTScore and Rouge as well as their correlation.\n",
    "        \"\"\"\n",
    "        assert self.refined != None, \"refined corpus doesn't exists\"\n",
    "        \n",
    "        if stop == None:\n",
    "            stop = len(self.refined)\n",
    "        subset_refined = self.refined[start:stop]\n",
    "        subset_gold = self.gold[start:stop]\n",
    "\n",
    "        #Static BERTScore computation\n",
    "        scoreOut = score(self.model, subset_refined, subset_gold)\n",
    "        customScore = [parseScore(curScore) for curScore in scoreOut]\n",
    "\n",
    "        #Rouge-Score computation\n",
    "        rougeScorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        rougeScore = [rougeScorer.score(c, r) for c, r in zip(subset_gold, subset_refined)]\n",
    "\n",
    "        #BERTScore computation\n",
    "        with nostd():\n",
    "            bertscore = bert_score.score(subset_refined, subset_gold, lang=\"en\", verbose=0)\n",
    "\n",
    "        #bartscore\n",
    "        bart_scorer = BARTScorer(device='cuda:0', checkpoint='facebook/bart-large-cnn')\n",
    "        bartscore = bart_scorer.score(subset_refined, subset_gold, batch_size=4)\n",
    "\n",
    "        #Data formating\n",
    "        custom_R = [round(t, 2) for t in customScore]\n",
    "        bertscore_R = [round(t.item(), 2) for t in bertscore[1]]\n",
    "        bartscore = [round(t, 2) for t in bartscore]\n",
    "        rouge1_R = [round(t['rouge1'][0], 2) for t in rougeScore]\n",
    "        rouge2_R = [round(t['rouge2'][0], 2) for t in rougeScore]\n",
    "        rougeL_R = [round(t['rougeL'][0], 2) for t in rougeScore]\n",
    "\n",
    "        dfCustom = pd.DataFrame({'CBERT' : custom_R,\n",
    "                                 'BERTScore' : bertscore_R,\n",
    "                                 'BARTScore' : bartscore,\n",
    "                                 'R-1' : rouge1_R,\n",
    "                                 'R-2' : rouge2_R,\n",
    "                                 'R-L' : rougeL_R\n",
    "                                })\n",
    "\n",
    "        #Correlation estimation\n",
    "        pearsonCor_c_r1 = np.round(pearsonr(custom_R, rouge1_R), 2)\n",
    "        pearsonCor_c_r2 = np.round(pearsonr(custom_R, rouge2_R), 2)\n",
    "        pearsonCor_c_rl = np.round(pearsonr(custom_R, rougeL_R), 2)\n",
    "        pearsonCor_bertscore_r1 = np.round(pearsonr(bertscore_R, rouge1_R), 2)\n",
    "        pearsonCor_bertscore_r2 = np.round(pearsonr(bertscore_R, rouge2_R), 2)\n",
    "        pearsonCor_bertscore_rl = np.round(pearsonr(bertscore_R, rougeL_R), 2)\n",
    "        pearsonCor_bartscore_r1 = np.round(pearsonr(bartscore, rouge1_R), 2)\n",
    "        pearsonCor_bartscore_r2 = np.round(pearsonr(bartscore, rouge2_R), 2)\n",
    "        pearsonCor_bartscore_rl = np.round(pearsonr(bartscore, rougeL_R), 2)\n",
    "\n",
    "        dfCor = pd.DataFrame({'pearson_CBERT_R-1' : pearsonCor_c_r1,\n",
    "                              'pearson_CBERT_R-2' : pearsonCor_c_r2,\n",
    "                              'pearson_CBERT_R-L' : pearsonCor_c_rl,\n",
    "                              'pearson_BERT_R-1' : pearsonCor_bertscore_r1,\n",
    "                              'pearson_BERT_R-2' : pearsonCor_bertscore_r2,\n",
    "                              'pearson_BERT_R-l' : pearsonCor_bertscore_rl,\n",
    "                              'pearson_BART_R-1' : pearsonCor_bartscore_r1,\n",
    "                              'pearson_BART_R-2' : pearsonCor_bartscore_r2,\n",
    "                              'pearson_BART_R-l' : pearsonCor_bartscore_rl}, index=[\"Pearson score\", \"p-value\"])\n",
    "        if verbose:\n",
    "            printout = \"Scores: \\n\"\n",
    "            printout += dfCustom.to_string().decode(\"utf8\") + \"\\n\\n\"\n",
    "            printout += \"Correlations: \\n\"\n",
    "            printout += dfCor.to_string().decode(\"utf8\")\n",
    "            print(printout)\n",
    "\n",
    "        return {\"scores\": dfCustom, \"correlations\": dfCor}\n",
    "    \n",
    "    def to_dataframe(self):\n",
    "        \"\"\"\n",
    "        Transforms a Refiner object to a dataframe.\n",
    "\n",
    "        :param1 self (Refiner): Refiner Object (see __init__ function for more details).\n",
    "\n",
    "        :output output (DataFrame): DataFrame containing both the corpus and the refined texts of the Refiner class. \n",
    "        \"\"\"\n",
    "        output = pd.DataFrame({\"text\": self.corpus,\n",
    "                               \"summary\": self.refined,\n",
    "                               \"processedText\": [\". \".join(c) for c in self.processedCorpus]})\n",
    "        return output\n",
    "\n",
    "    def save(self, runtime=None, new=True):\n",
    "        \"\"\"\n",
    "        Saves Refiner output to a local folder.\n",
    "\n",
    "        :param1 self (Refiner): Refiner Object (see __init__ function for more details).\n",
    "        :param2 new (bool): Indicates if a new folder should be created. If false, output is append to the most recent ouput folder.\n",
    "        \"\"\"\n",
    "\n",
    "        #evaluation\n",
    "        start = 0\n",
    "        stop = len(self.refined)\n",
    "        assessement = self.assess(start=start, stop=stop)\n",
    "\n",
    "        #mainDf = r.to_dataframe()\n",
    "        scoreDf = assessement[\"scores\"]\n",
    "        corDf = assessement[\"correlations\"]\n",
    "\n",
    "        #write output\n",
    "        main_folder_path = os.path.join(get_git_root(), r\"myLibraries\\refining_output\")\n",
    "        countfile_name = r\"count.txt\"\n",
    "        if new:\n",
    "            count = updateFileCount(os.path.join(main_folder_path, countfile_name))\n",
    "        else:\n",
    "            count = readFileCount(os.path.join(main_folder_path, countfile_name))\n",
    "\n",
    "        current_path = os.path.join(main_folder_path, f\"experimentation_{count}\")\n",
    "        try:\n",
    "            os.mkdir(current_path)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "\n",
    "        #mainDf.to_csv(os.path.join(current_path, \"main.csv\"))\n",
    "        scoreDf.to_csv(os.path.join(current_path, \"scores.csv\"))\n",
    "        corDf.to_csv(os.path.join(current_path, \"correlations.csv\"))\n",
    "        with open(os.path.join(current_path, \"runtimes.txt\"), \"w\") as f:\n",
    "            f.write(str(runtime))\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"\n",
    "        Summarizes Refiner object to a string.\n",
    "\n",
    "        :param1 self (Refiner): Refiner Object (see __init__ function for more details).\n",
    "\n",
    "        :output printout (string): Summarized informations about the refiner object.\n",
    "        \"\"\"\n",
    "\n",
    "        printout = \"--------REFINER OBJECT--------\\n\\n\"\n",
    "        printout += \"Number of Documents : \" + str(len(self.corpus)) + \"\\n\"\n",
    "        printout += \"Corpus Avg Size     : \" + str(int(np.average([len(x) for x in self.corpus]))+1) + \"\\n\"\n",
    "        printout += \"Refined Avg Size    : \" + str(int(np.average([len(x) for x in self.refined]))+1) + \"\\n\"\n",
    "        printout += \"Ratio(s)            : \" + str(self.ratio) + \"\\n\"\n",
    "        printout += \"Threshold           : \" + str(self.threshold) + \"\\n\"\n",
    "        printout += \"Maximum Spacing     : \" + str(self.ms) + \"\\n\"\n",
    "        \n",
    "        self.printRange = self.printRange if self.printRange.start >= 0 and self.printRange.stop < len(self.processedCorpus) else range(0, len(self.processedCorpus))\n",
    "\n",
    "        for index in self.printRange:\n",
    "            printout += f\"\\nCorpus no.{index+1} : \\n\" + str(\".\\n\".join([f\"{Fore.LIGHTGREEN_EX}{self.processedCorpus[index][i]}{Style.RESET_ALL}\"\n",
    "                                                        if i in self.selectedIndexes[index]\n",
    "                                                        else f\"{Fore.RED}{self.processedCorpus[index][i]}{Style.RESET_ALL}\"\n",
    "                                                        for i in range(len(self.processedCorpus[index]))])) + \".\" + \"\\n\"\n",
    "        printout += \"\\n------------------------------\"\n",
    "        return printout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = model_load(\"Word2Vec\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Refiner([\"I am Marius. I like trains.\", \"Engineering is good. It is fun.\"], [\"My name is Marius. I think trains are cool.\", \"I enjoy datascience. It is my studies.\"], w2v, metric=bert_score.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.refine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_=r.assess()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refiner .py Test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see file <refine_tests_2.py>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\myLibraries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from refine_tests_2 import Refiner\n",
    "from custom_score.utils import model_load\n",
    "from custom_score.score import score\n",
    "import bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = model_load(\"Word2Vec\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Refiner([\"I am Marius. I like trains.\", \"Engineering is good. It is fun.\"], [\"My name is Marius. I think trains are cool.\", \"I enjoy datascience. It is my studies.\"], w2v, metric=bert_score.score, ratio=[2, 3])\n",
    "r.refine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: \n",
      "   CBERT  BERTScore  BARTScore   R-1  R-2   R-L\n",
      "0   0.42       0.91        0.4  0.67  0.0  0.33\n",
      "1   0.52       0.89        0.1  0.67  0.5  0.67\n",
      "\n",
      "Correlations: \n",
      "               pearson_CBERT_R-1  pearson_CBERT_R-2  pearson_CBERT_R-L  pearson_BERT_R-1  pearson_BERT_R-2  pearson_BERT_R-l  pearson_BART_R-1  pearson_BART_R-2  pearson_BART_R-l\n",
      "Pearson score                NaN                1.0                1.0               NaN              -1.0              -1.0               NaN              -1.0              -1.0\n",
      "p-value                      NaN                1.0                1.0               NaN               1.0               1.0               NaN               1.0               1.0\n",
      "--------REFINER OBJECT--------\n",
      "\n",
      "Number of Documents : 2\n",
      "Corpus Avg Size     : 30\n",
      "Refined Avg Size    : 12\n",
      "Ratio(s)            : [2, 3]\n",
      "Threshold           : 0.7\n",
      "Maximum Spacing     : 10\n",
      "\n",
      "Corpus no.1 : \n",
      "\u001b[92mI am Marius\u001b[0m.\n",
      "\u001b[31mI like trains\u001b[0m.\n",
      "\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "out = r.assess()\n",
    "print(r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
