{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"D:\\COURS\\A4\\S8 - ESILV\\Stage\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\myLibraries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bert_score import score as bert_score\n",
    "from custom_score.score import score  as custom_score\n",
    "from custom_score.score import score\n",
    "from custom_score.utils import serialized_to_model\n",
    "\n",
    "from custom_score.refine import Refiner\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = serialized_to_model(r\"C:\\Pro\\Stages\\A4 - DVRC\\Work\\Models\\serialized_w2v.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "billsum0 = \"SECTION 1. SHORT TITLE.\\n\\n    This Act may be cited as the ``National Science Education Tax \\nIncentive for Businesses Act of 2007''.\\n\\nSEC. 2. CREDITS FOR CERTAIN CONTRIBUTIONS BENEFITING SCIENCE, \\n              TECHNOLOGY, ENGINEERING, AND MATHEMATICS EDUCATION AT THE \\n              ELEMENTARY AND SECONDARY SCHOOL LEVEL.\\n\\n    (a) In General.--Subpart D of part IV of subchapter A of chapter 1 \\nof the Internal Revenue Code of 1986 (relating to business related \\ncredits) is amended by adding at the end the following new section:\\n\\n``SEC. 45O. CONTRIBUTIONS BENEFITING SCIENCE, TECHNOLOGY, ENGINEERING, \\n              AND MATHEMATICS EDUCATION AT THE ELEMENTARY AND SECONDARY \\n              SCHOOL LEVEL.\\n\\n    ``(a) In General.--For purposes of section 38, the elementary and \\nsecondary science, technology, engineering, and mathematics (STEM) \\ncontributions credit determined under this section for the taxable year \\nis an amount equal to 100 percent of the qualified STEM contributions \\nof the taxpayer for such taxable year.\\n    ``(b) Qualified STEM Contributions.--For purposes of this section, \\nthe term `qualified STEM contributions' means--\\n            ``(1) STEM school contributions,\\n            ``(2) STEM teacher externship expenses, and\\n            ``(3) STEM teacher training expenses.\\n    ``(c) STEM School Contributions.--For purposes of this section--\\n            ``(1) In general.--The term `STEM school contributions' \\n        means--\\n                    ``(A) STEM property contributions, and\\n                    ``(B) STEM service contributions.\\n            ``(2) STEM property contributions.--The term `STEM property \\n        contributions' means the amount which would (but for subsection \\n        (f)) be allowed as a deduction under section 170 for a \\n        charitable contribution of STEM inventory property if--\\n                    ``(A) the donee is an elementary or secondary \\n                school described in section 170(b)(1)(A)(ii),\\n                    ``(B) substantially all of the use of the property \\n                by the donee is within the United States or within the \\n                defense dependents' education system for educational \\n                purposes in any of the grades K-12 that are related to \\n                the purpose or function of the donee,\\n                    ``(C) the original use of the property begins with \\n                the donee,\\n                    ``(D) the property will fit productively into the \\n                donee's education plan,\\n                    ``(E) the property is not transferred by the donee \\n                in exchange for money, other property, or services, \\n                except for shipping, installation and transfer costs, \\n                and\\n                    ``(F) the donee's use and disposition of the \\n                property will be in accordance with the provisions of \\n                subparagraphs (B) and (E).\\n        The determination of the amount of deduction under section 170 \\n        for purposes of this paragraph shall be made as if the \\n        limitation under section 170(e)(3)(B) applied to all STEM \\n        inventory property.\\n            ``(3) STEM service contributions.--The term `STEM service \\n        contributions' means the amount paid or incurred during the \\n        taxable year for STEM services provided in the United States or \\n        in the defense dependents' education system for the exclusive \\n        benefit of students at an elementary or secondary school \\n        described in section 170(b)(1)(A)(ii) but only if--\\n                    ``(A) the taxpayer is engaged in the trade or \\n                business of providing such services on a commercial \\n                basis, and\\n                    ``(B) no charge is imposed for providing such \\n                services.\\n            ``(4) STEM inventory property.--The term `STEM inventory \\n        property' means, with respect to any contribution to a school, \\n        any property--\\n                    ``(A) which is described in paragraph (1) or (2) of \\n                section 1221(a) with respect to the donor, and\\n                    ``(B) which is determined by the school to be \\n                needed by the school in providing education in grades \\n                K-12 in the areas of science, technology, engineering, \\n                or mathematics.\\n            ``(5) STEM services.--The term `STEM services' means, with \\n        respect to any contribution to a school, any service determined \\n        by the school to be needed by the school in providing education \\n        in grades K-12 in the areas of science, technology, \\n        engineering, or mathematics, including teaching courses of \\n        instruction at such school in any such area.\\n            ``(6) Defense dependents' education system.--For purposes \\n        of this subsection, the term `defense dependents' education \\n        system' means the program established and operated under the \\n        Defense Dependents' Education Act of 1978 (20 U.S.C. 921 et \\n        seq.).\\n    ``(d) STEM Teacher Externship Expenses.--For purposes of this \\nsection--\\n            ``(1) In general.--The term `STEM teacher externship \\n        expenses' means any amount paid or incurred to carry out a STEM \\n        externship program of the taxpayer but only to the extent that \\n        such amount is attributable to the participation in such \\n        program of any eligible STEM teacher, including amounts paid to \\n        such a teacher as a stipend while participating in such \\n        program.\\n            ``(2) STEM externship program.--The term `STEM externship \\n        program' means any program--\\n                    ``(A) established by a taxpayer engaged in a trade \\n                or business within an area of science, technology, \\n                engineering, or mathematics, and\\n                    ``(B) under which eligible STEM teachers receive \\n                training to enhance their teaching skills in the areas \\n                of science, technology, engineering, or mathematics or \\n                otherwise improve their knowledge in such areas.\\n            ``(3) Eligible stem teacher.--The term `eligible STEM \\n        teacher' means any individual--\\n                    ``(A) who is a teacher in grades K-12 at an \\n                educational organization described in section \\n                170(b)(1)(A)(ii) which is located in the United States \\n                or which is located on a United States military base \\n                outside the United States, and\\n                    ``(B) whose teaching responsibilities at such \\n                school include, or are likely to include, any course in \\n                the areas of science, technology, engineering, or \\n                mathematics.\\n    ``(e) STEM Teacher Training Expenses.--The term `STEM teacher \\ntraining expenses' means any amount paid or incurred by a taxpayer \\nengaged in a trade or business within an area of science, technology, \\nengineering, or mathematics which is attributable to the participation \\nof any eligible STEM teacher in a regular training program provided to \\nemployees of the taxpayer which is determined by such teacher's school \\nas enhancing such teacher's teaching skills in the areas of science, \\ntechnology, engineering, or mathematics.\\n    ``(f) Denial of Double Benefit.--No deduction shall be allowed \\nunder this chapter for any amount allowed as a credit under this \\nsection.''.\\n    (b) Conforming Amendments.--\\n            (1) Section 38(b) of such Code is amended by striking \\n        ``plus'' at the end of paragraph (30), by striking the period \\n        at the end of paragraph (31), and inserting ``, plus'', and by \\n        adding at the end the following new paragraph:\\n            ``(32) the elementary and secondary science, technology, \\n        engineering, and mathematics (STEM) contributions credit \\n        determined under section 45O.''.\\n            (2) The table of sections for subpart D of part IV of \\n        subchapter A of chapter 1 of such Code is amended by adding at \\n        the end the following new item:\\n\\n``Sec. 45O. Contributions benefiting science, technology, engineering, \\n                            and mathematics education at the elementary \\n                            and secondary school level.''.\\n    (c) Effective Date.--The amendments made by this section shall \\napply to taxable years beginning after the date of the enactment of \\nthis Act.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Refiner([billsum0, billsum0], w2v, reductionFactor=4, maxSpacing=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.refine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"len simplified: \", len(s.refined[0]), \" || \", \"len initial: \", len(s.corpus[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = s.assess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[\"scores\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[\"correlations\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess(simp, verbose=True):\n",
    "    \"\"\"\n",
    "    Assesses quality of the simplified corpus by computing Static BERTscore and Rouge-Score on the simplified version compared to it's initial version.\n",
    "\n",
    "    :param1 self (Simplifier): Simplifier Object (see __init__ function for more details).\n",
    "    :param2 verbose (Boolean): When put to true, assess results will be printed.\n",
    "\n",
    "    :output (dict): Dictionnary containing both the scores of Static BERTScore and Rouge as well as their correlation\n",
    "    \"\"\"\n",
    "    assert simp.simplified != None, \"simplified corpus doesn't exists\"\n",
    "\n",
    "    #Static BERTScore computation\n",
    "    customScore = score(simp.model, simp.simplified, simp.corpus)\n",
    "\n",
    "    #Rouge-Score computation\n",
    "    rougeScorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "    rougeScore = [rougeScorer.score(s, c) for s, c in zip(simp.simplified, simp.corpus)]\n",
    "\n",
    "    #Data formating\n",
    "    custom_R = [round(t[0], 2) for t in customScore]\n",
    "    rouge1_R = [round(t['rouge1'][0], 2) for t in rougeScore]\n",
    "    rougeL_R = [round(t['rougeL'][0], 2) for t in rougeScore]\n",
    "\n",
    "    dfCustom = pd.DataFrame({'CBERT' : custom_R,\n",
    "                             'R-1' : rouge1_R,\n",
    "                             'R-L' : rougeL_R\n",
    "                             })\n",
    "\n",
    "    #Correlation estimation\n",
    "    pearsonCor_c_r1 = pearsonr(custom_R, rouge1_R)\n",
    "    pearsonCor_c_rl = pearsonr(custom_R, rougeL_R)\n",
    "\n",
    "    dfCor = pd.DataFrame({'pearson_CBERT_R-1' : pearsonCor_c_r1,\n",
    "                          'pearson_CBERT_R-L' : pearsonCor_c_rl}, index=[\"Pearson score\", \"p-value\"])\n",
    "\n",
    "    return {\"scores\": dfCustom, \"correlations\": dfCor}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = assess(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[\"scores\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[\"correlations\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STR function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = serialized_to_model(r\"C:\\Pro\\Stages\\A4 - DVRC\\Work\\Models\\serialized_w2v.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "billsum0 = \"SECTION 1. SHORT TITLE.\\n\\n    This Act may be cited as the ``National Science Education Tax \\nIncentive for Businesses Act of 2007''.\\n\\nSEC. 2. CREDITS FO\" \\\n",
    "            \"R CERTAIN CONTRIBUTIONS BENEFITING SCIENCE, \\n              TECHNOLOGY, ENGINEERING, AND MATHEMATICS EDUCATION AT THE \\n              ELEMENTARY AND SECONDARY \"\\\n",
    "            \"SCHOOL LEVEL.\\n\\n    (a) In General.--Subpart D of part IV of subchapter A of chapter 1 \\nof the Internal Revenue Code of 1986 (relating to business related \\nc\"\\\n",
    "            \"redits) is amended by adding at the end the following new section:\\n\\n``SEC. 45O. CONTRIBUTIONS BENEFITING SCIENCE, TECHNOLOGY, ENGINEERING, \\n              AND MA\"\\\n",
    "            \"THEMATICS EDUCATION AT THE ELEMENTARY AND SECONDARY \\n              SCHOOL LEVEL.\\n\\n    ``(a) In General.--For purposes of section 38, the elementary and \\nsecond\"\\\n",
    "            \"ary science, technology, engineering, and mathematics (STEM) \\ncontributions credit determined under this section for the taxable year \\nis an amount equal to 100 \"\\\n",
    "            \"percent of the qualified STEM contributions \\nof the taxpayer for such taxable year.\\n    ``(b) Qualified STEM Contributions.--For purposes of this section, \\nthe te\"\\\n",
    "            \"rm `qualified STEM contributions' means--\\n            ``(1) STEM school contributions,\\n            ``(2) STEM teacher externship expenses, and\\n            ``(3) ST\"\\\n",
    "            \"EM teacher training expenses.\\n    ``(c) STEM School Contributions.--For purposes of this section--\\n            ``(1) In general.--The term `STEM school contributions' \"\\\n",
    "            \"\\n        means--\\n                    ``(A) STEM property contributions, and\\n                    ``(B) STEM service contributions.\\n            ``(2) STEM property\"\\\n",
    "            \" contributions.--The term `STEM property \\n        contributions' means the amount which would (but for subsection \\n        (f)) be allowed as a deduction under secti\"\\\n",
    "            \"on 170 for a \\n        charitable contribution of STEM inventory property if--\\n                    ``(A) the donee is an elementary or secondary \\n                \"\\\n",
    "            \"school described in section 170(b)(1)(A)(ii),\\n                    ``(B) substantially all of the use of the property \\n                by the donee is within the\"\\\n",
    "            \" United States or within the \\n                defense dependents' education system for educational \\n                purposes in any of the grades K-12 that \"\\\n",
    "            \"are related to \\n                the purpose or function of the donee,\\n                    ``(C) the original use of the property begins with \\n         \"\\\n",
    "            \"       the donee,\\n                    ``(D) the property will fit productively into the \\n                donee's education plan,\\n                   \"\\\n",
    "            \" ``(E) the property is not transferred by the donee \\n                in exchange for money, other property, or services, \\n                except \"\\\n",
    "            \"for shipping, installation and transfer costs, \\n                and\\n                    ``(F) the donee's use and disposition of the \\n        \"\\\n",
    "            \"        property will be in accordance with the provisions of \\n                subparagraphs (B) and (E).\\n        The determination of the amount of deduct\"\\\n",
    "                \"ion under section 170 \\n        for purposes of this paragraph shall be made as if the \\n        limitation under section 170(e)(3)(B) applied to all STEM \"\\\n",
    "                    \"\\n        inventory property.\\n            ``(3) STEM service contributions.--The term `STEM service \\n        contributions' means the amount paid or inc\"\\\n",
    "                        \"urred during the \\n        taxable year for STEM services provided in the United States or \\n        in the defense dependents' education system for the\"\\\n",
    "                            \" exclusive \\n        benefit of students at an elementary or secondary school \\n        described in section 170(b)(1)(A)(ii) but only if--\\n         \"\\\n",
    "                                \"           ``(A) the taxpayer is engaged in the trade or \\n                business of providing such services on a commercial \\n                bas\"\\\n",
    "                                    \"is, and\\n                    ``(B) no charge is imposed for providing such \\n                services.\\n            ``(4) STEM inventory property.--Th\"\\\n",
    "                                        \"e term `STEM inventory \\n        property' means, with respect to any contribution to a school, \\n        any property--\\n                    ``(A) w\"\\\n",
    "                                            \"hich is described in paragraph (1) or (2) of \\n                section 1221(a) with respect to the donor, and\\n                    ``(B) which is determined by the school to be \\n                needed by the school in providing education in grades \\n                K-12 in the areas of science, technology, engineering, \\n                or mathematics.\\n            ``(5) STEM services.--The term `STEM services' means, with \\n        respect to any contribution to a school, any service determined \\n        by the school to be needed by the school in providing education \\n        in grades K-12 in the areas of science, technology, \\n        engineering, or mathematics, including teaching courses of \\n        instruction at such school in any such area.\\n            ``(6) Defense dependents' education system.--For purposes \\n        of this subsection, the term `defense dependents' education \\n        system' means the program established and operated under the \\n        Defense Dependents' Education Act of 1978 (20 U.S.C. 921 et \\n        seq.).\\n    ``(d) STEM Teacher Externship Expenses.--For purposes of this \\nsection--\\n            ``(1) In general.--The term `STEM teacher externship \\n        expenses' means any amount paid or incurred to carry out a STEM \\n        externship program of the taxpayer but only to the extent that \\n        such amount is attributable to the participation in such \\n        program of any eligible STEM teacher, including amounts paid to \\n        such a teacher as a stipend while participating in such \\n        program.\\n            ``(2) STEM externship program.--The term `STEM externship \\n        program' means any program--\\n                    ``(A) established by a taxpayer engaged in a trade \\n                or business within an area of science, technology, \\n                engineering, or mathematics, and\\n                    ``(B) under which eligible STEM teachers receive \\n                training to enhance their teaching skills in the areas \\n                of science, technology, engineering, or mathematics or \\n                otherwise improve their knowledge in such areas.\\n            ``(3) Eligible stem teacher.--The term `eligible STEM \\n        teacher' means any individual--\\n                    ``(A) who is a teacher in grades K-12 at an \\n                educational organization described in section \\n                170(b)(1)(A)(ii) which is located in the United States \\n                or which is located on a United States military base \\n                outside the United States, and\\n                    ``(B) whose teaching responsibilities at such \\n                school include, or are likely to include, any course in \\n                the areas of science, technology, engineering, or \\n                mathematics.\\n    ``(e) STEM Teacher Training Expenses.--The term `STEM teacher \\ntraining expenses' means any amount paid or incurred by a taxpayer \\nengaged in a trade or business within an area of science, technology, \\nengineering, or mathematics which is attributable to the participation \\nof any eligible STEM teacher in a regular training program provided to \\nemployees of the taxpayer which is determined by such teacher's school \\nas enhancing such teacher's teaching skills in the areas of science, \\ntechnology, engineering, or mathematics.\\n    ``(f) Denial of Double Benefit.--No deduction shall be allowed \\nunder this chapter for any amount allowed as a credit under this \\nsection.''.\\n    (b) Conforming Amendments.--\\n            (1) Section 38(b) of such Code is amended by striking \\n        ``plus'' at the end of paragraph (30), by striking the period \\n        at the end of paragraph (31), and inserting ``, plus'', and by \\n        adding at the end the following new paragraph:\\n            ``(32) the elementary and secondary science, technology, \\n        engineering, and mathematics (STEM) contributions credit \\n        determined under section 45O.''.\\n            (2) The table of sections for subpart D of part IV of \\n        subchapter A of chapter 1 of such Code is amended by adding at \\n        the end the following new item:\\n\\n``Sec. 45O. Contributions benefiting science, technology, engineering, \\n                            and mathematics education at the elementary \\n                            and secondary school level.''.\\n    (c) Effective Date.--The amendments made by this section shall \\napply to taxable years beginning after the date of the enactment of \\nthis Act.\"\n",
    "s = Refiner([billsum0], w2v, reductionFactor=4, maxSpacing=15)\n",
    "s.simplify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colorama import Fore, Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatHighlight():\n",
    "    print(f\"{Fore.BLUE}Je suis ton pere{Style.RESET_ALL}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic and syntaxique distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_score.utils import cleanString, parseScore\n",
    "from custom_score.score import score\n",
    "import numpy as np\n",
    "from random import uniform\n",
    "from custom_score.utils import serialized_to_model\n",
    "import tensorflow_datasets as tfds\n",
    "from numpy.linalg import norm\n",
    "from rouge_score import rouge_scorer\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseDistances(distances):\n",
    "    \"\"\"\n",
    "    Converts score to a value interpretable as distance (the greatest the value, the greatest the distance).\n",
    "\n",
    "    :param1 distances (List): List of distances values for each sentences of a corpus with respect to all the others sentences.\n",
    "\n",
    "    :output parsedDisctances (List): List of converted distances. \n",
    "    \"\"\"\n",
    "    refDist = distances[0][0]\n",
    "    toPositive = False \n",
    "    toInverse = False\n",
    "\n",
    "    if refDist < 0:\n",
    "        toPositive = True\n",
    "    if refDist >=-1 and refDist <=1:\n",
    "        toInverse = True\n",
    "    \n",
    "    parsedDistances = []\n",
    "    for distance in distances:\n",
    "        parsedDistance = []\n",
    "        for value in distance:\n",
    "            cur = value\n",
    "            if toPositive:\n",
    "                cur = -cur\n",
    "            if toInverse:\n",
    "                try:\n",
    "                    cur = 1/cur\n",
    "                except ZeroDivisionError:\n",
    "                    cur = 1\n",
    "            parsedDistance.append(cur)\n",
    "        parsedDistances.append(parsedDistance)\n",
    "    return parsedDistances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentenceSelection(corpus, scores, distances, reductionFactor=2):\n",
    "    \"\"\"\n",
    "    Returns a list of selected indices of sentence that will constituate the new corpus.\n",
    "\n",
    "    :param1 corpus (list): List of sentences of the reference document.\n",
    "    :param2 scores (list): List of the similarity scores of each sentence of the reference compared to the entire reference document.\n",
    "    :param3 reductionFactor (float or int): Number determining how much the reference text will be shortened. \n",
    "\n",
    "    :output selected_indexes (list): List of indexes of the initial corpus sentences that have been selected.\n",
    "    \"\"\"\n",
    "    totalLength = len(\" \".join(corpus))\n",
    "    targetLength = int(totalLength/reductionFactor)\n",
    "    selected_indexes = []\n",
    "    \n",
    "    randomized_scores = [np.mean([curScore, uniform(0, 1)]) for curScore in scores]\n",
    "    ranking = np.argsort(randomized_scores)[::-1]\n",
    "\n",
    "    selectedLength = 0\n",
    "    selected_indexes = []\n",
    "    current_distance = lambda x: norm([distances[ranking[x]][i] for i in selected_indexes])\n",
    "    cur = 0\n",
    "    while(selectedLength < targetLength):\n",
    "        if cur == 0:\n",
    "            selected_indexes.append(ranking[cur])\n",
    "            selectedLength += len(corpus[ranking[cur]])\n",
    "        else:\n",
    "            updated_scores = [randomized_scores[i]*current_distance(i) for i in range(len(randomized_scores))]\n",
    "            updated_ranking = np.argsort(updated_scores)[::-1]\n",
    "            updated_ranking = [index for index in updated_ranking if index not in selected_indexes]\n",
    "            selected_indexes.append(updated_ranking[0])\n",
    "            selectedLength += len(corpus[updated_ranking[0]])\n",
    "        cur += 1\n",
    "\n",
    "    return selected_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Refiner:\n",
    "\n",
    "    def __init__(self, corpus, model, scorer=score, reductionFactor=2, maxSpacing=10):\n",
    "        \"\"\"\n",
    "        Constructor of the Refiner class. Aims at reducing the size and noise of a given independant list of documents.\n",
    "        \n",
    "        :param1 self (Refiner): Object to initialize.\n",
    "        :param2 corpus (List): List of documents to simplify.\n",
    "        :param3 model (Any): Model used to compute scores and create sentence's ranking.\n",
    "        :param4 reductionFactor (float or int): Number determining how much the reference text will be shortened. \n",
    "        :param5 maxSpacing (int): Maximal number of adjacent space to be found and suppressed in the corpus.\n",
    "        \"\"\"\n",
    "        self.corpus = corpus\n",
    "        self.model = model\n",
    "        self.scorer = scorer\n",
    "        self.rf = reductionFactor\n",
    "        self.ms = maxSpacing\n",
    "        self.refined = None\n",
    "\n",
    "    def refine(self):\n",
    "        \"\"\"\n",
    "        Return a reduced string computed using static embedding vectors similarity. Also denoises the data by removing superfluous elements such as \"\\n\" or useless signs.\n",
    "\n",
    "        :param1 self (Refiner): Refiner Object (see __init__ function for more details).\n",
    "\n",
    "        :output refined (string): refined version of the initial document.\n",
    "        \"\"\"\n",
    "        self.refined = []\n",
    "        for indiv in self.corpus:\n",
    "            #preprocess corpus\n",
    "            clean = cleanString(indiv, self.ms)\n",
    "            sentences = clean.split(\".\")\n",
    "            sentences.pop()\n",
    "            temp = []\n",
    "            for sentence in sentences: \n",
    "                if sentence != None and sentence != \"\":\n",
    "                    temp.append(sentence)\n",
    "            sentences = temp\n",
    "            respaced_sentences = []\n",
    "            for sentence in sentences:\n",
    "                if sentence[0] == \" \":\n",
    "                    sentence = sentence[1:]\n",
    "                respaced_sentences.append(sentence)\n",
    "            corpus = \" \".join(respaced_sentences)\n",
    "\n",
    "            #compute ranking\n",
    "            scores = []\n",
    "            for sentence in respaced_sentences:\n",
    "                scoreOut = self.scorer(self.model, [sentence], [indiv])\n",
    "                R = parseScore(scoreOut)\n",
    "                scores.append(R)\n",
    "            \n",
    "            #compute distances\n",
    "            distances = []\n",
    "            for x in range(len(respaced_sentences)):\n",
    "                distance = []\n",
    "                for y in range(len(respaced_sentences)):\n",
    "                    if x != y:\n",
    "                        try:\n",
    "                            scoreOut = self.scorer(self.model, [respaced_sentences[x]], [respaced_sentences[y]])\n",
    "                            curDistance = parseScore(scoreOut)\n",
    "                        except:\n",
    "                            curDistance = -1\n",
    "                    else:\n",
    "                        curDistance = 1\n",
    "                    distance.append(curDistance)\n",
    "                distances.append(distance)\n",
    "            distances = parseDistances(distances)\n",
    "\n",
    "            #selection of best individuals\n",
    "            indices = sentenceSelection(respaced_sentences, scores, distances, self.rf)\n",
    "            \n",
    "            curRefined = []\n",
    "            for index in indices:\n",
    "                curRefined.append(respaced_sentences[index])\n",
    "            \n",
    "            curRefined = \" \".join(curRefined)\n",
    "            self.refined.append(curRefined)\n",
    "\n",
    "    def assess(self, verbose=True):\n",
    "        \"\"\"\n",
    "        Assesses quality of the refined corpus by computing Static BERTscore and Rouge-Score on the refined version compared to it's initial version.\n",
    "\n",
    "        :param1 self (Refiner): Refiner Object (see __init__ function for more details).\n",
    "        :param2 verbose (Boolean): When put to true, assess results will be printed.\n",
    "\n",
    "        :output (dict): Dictionnary containing both the scores of Static BERTScore and Rouge as well as their correlation\n",
    "        \"\"\"\n",
    "        assert self.refined != None, \"refined corpus doesn't exists\"\n",
    "\n",
    "        #Static BERTScore computation\n",
    "        scoreOut = self.scorer(self.model, self.refined, self.corpus)\n",
    "        customScore = [parseScore(curScore) for curScore in scoreOut]\n",
    "\n",
    "        #Rouge-Score computation\n",
    "        rougeScorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "        rougeScore = [rougeScorer.score(s, c) for s, c in zip(self.refined, self.corpus)]\n",
    "\n",
    "        #Data formating\n",
    "        custom_R = [round(t, 2) for t in customScore]\n",
    "        rouge1_R = [round(t['rouge1'][0], 2) for t in rougeScore]\n",
    "        rougeL_R = [round(t['rougeL'][0], 2) for t in rougeScore]\n",
    "\n",
    "        dfCustom = pd.DataFrame({'CBERT' : custom_R,\n",
    "                                'R-1' : rouge1_R,\n",
    "                                'R-L' : rougeL_R\n",
    "                                })\n",
    "\n",
    "        #Correlation estimation\n",
    "        pearsonCor_c_r1 = np.round(pearsonr(custom_R, rouge1_R), 2)\n",
    "        pearsonCor_c_rl = np.round(pearsonr(custom_R, rougeL_R), 2)\n",
    "\n",
    "        dfCor = pd.DataFrame({'pearson_CBERT_R-1' : pearsonCor_c_r1,\n",
    "                            'pearson_CBERT_R-L' : pearsonCor_c_rl}, index=[\"Pearson score\", \"p-value\"])\n",
    "\n",
    "        return {\"scores\": dfCustom, \"correlations\": dfCor}\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        printout = \"--------REFINER OBJECT--------\\n\"\n",
    "        printout += \"Number of Documents : \" + str(len(self.corpus)) + \"\\n\"\n",
    "        printout += \"Corpus Avg Size     : \" + str(int(np.average([len(x) for x in self.corpus]))+1) + \"\\n\"\n",
    "        printout += \"Refined Avg Size    : \" + str(int(np.average([len(x) for x in self.refined]))+1) + \"\\n\"\n",
    "        printout += \"Reduction Factor    : \" + str(self.rf) + \"\\n\"\n",
    "        printout += \"Maximum Spacing     : \" + str(self.ms) + \"\\n\"\n",
    "        printout += \"------------------------------\"\n",
    "        return printout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = serialized_to_model(r\"C:\\Pro\\Stages\\A4 - DVRC\\Work\\Models\\serialized_w2v.pkl\")\n",
    "billsum0 = \"SECTION 1. SHORT TITLE.\\n\\n    This Act may be cited as the ``National Science Education Tax \\nIncentive for Businesses Act of 2007''.\\n\\nSEC. 2. CREDITS FOR CERTAIN CONTRIBUTIONS BENEFITING SCIENCE, \\n              TECHNOLOGY, ENGINEERING, AND MATHEMATICS EDUCATION AT THE \\n              ELEMENTARY AND SECONDARY SCHOOL LEVEL.\\n\\n    (a) In General.--Subpart D of part IV of subchapter A of chapter 1 \\nof the Internal Revenue Code of 1986 (relating to business related \\ncredits) is amended by adding at the end the following new section:\\n\\n``SEC. 45O. CONTRIBUTIONS BENEFITING SCIENCE, TECHNOLOGY, ENGINEERING, \\n              AND MATHEMATICS EDUCATION AT THE ELEMENTARY AND SECONDARY \\n              SCHOOL LEVEL.\\n\\n    ``(a) In General.--For purposes of section 38, the elementary and \\nsecondary science, technology, engineering, and mathematics (STEM) \\ncontributions credit determined under this section for the taxable year \\nis an amount equal to 100 percent of the qualified STEM contributions \\nof the taxpayer for such taxable year.\\n    ``(b) Qualified STEM Contributions.--For purposes of this section, \\nthe term `qualified STEM contributions' means--\\n            ``(1) STEM school contributions,\\n            ``(2) STEM teacher externship expenses, and\\n            ``(3) STEM teacher training expenses.\\n    ``(c) STEM School Contributions.--For purposes of this section--\\n            ``(1) In general.--The term `STEM school contributions' \\n        means--\\n                    ``(A) STEM property contributions, and\\n                    ``(B) STEM service contributions.\\n            ``(2) STEM property contributions.--The term `STEM property \\n        contributions' means the amount which would (but for subsection \\n        (f)) be allowed as a deduction under section 170 for a \\n        charitable contribution of STEM inventory property if--\\n                    ``(A) the donee is an elementary or secondary \\n                school described in section 170(b)(1)(A)(ii),\\n                    ``(B) substantially all of the use of the property \\n                by the donee is within the United States or within the \\n                defense dependents' education system for educational \\n                purposes in any of the grades K-12 that are related to \\n                the purpose or function of the donee,\\n                    ``(C) the original use of the property begins with \\n                the donee,\\n                    ``(D) the property will fit productively into the \\n                donee's education plan,\\n                    ``(E) the property is not transferred by the donee \\n                in exchange for money, other property, or services, \\n                except for shipping, installation and transfer costs, \\n                and\\n                    ``(F) the donee's use and disposition of the \\n                property will be in accordance with the provisions of \\n                subparagraphs (B) and (E).\\n        The determination of the amount of deduction under section 170 \\n        for purposes of this paragraph shall be made as if the \\n        limitation under section 170(e)(3)(B) applied to all STEM \\n        inventory property.\\n            ``(3) STEM service contributions.--The term `STEM service \\n        contributions' means the amount paid or incurred during the \\n        taxable year for STEM services provided in the United States or \\n        in the defense dependents' education system for the exclusive \\n        benefit of students at an elementary or secondary school \\n        described in section 170(b)(1)(A)(ii) but only if--\\n                    ``(A) the taxpayer is engaged in the trade or \\n                business of providing such services on a commercial \\n                basis, and\\n                    ``(B) no charge is imposed for providing such \\n                services.\\n            ``(4) STEM inventory property.--The term `STEM inventory \\n        property' means, with respect to any contribution to a school, \\n        any property--\\n                    ``(A) which is described in paragraph (1) or (2) of \\n                section 1221(a) with respect to the donor, and\\n                    ``(B) which is determined by the school to be \\n                needed by the school in providing education in grades \\n                K-12 in the areas of science, technology, engineering, \\n                or mathematics.\\n            ``(5) STEM services.--The term `STEM services' means, with \\n        respect to any contribution to a school, any service determined \\n        by the school to be needed by the school in providing education \\n        in grades K-12 in the areas of science, technology, \\n        engineering, or mathematics, including teaching courses of \\n        instruction at such school in any such area.\\n            ``(6) Defense dependents' education system.--For purposes \\n        of this subsection, the term `defense dependents' education \\n        system' means the program established and operated under the \\n        Defense Dependents' Education Act of 1978 (20 U.S.C. 921 et \\n        seq.).\\n    ``(d) STEM Teacher Externship Expenses.--For purposes of this \\nsection--\\n            ``(1) In general.--The term `STEM teacher externship \\n        expenses' means any amount paid or incurred to carry out a STEM \\n        externship program of the taxpayer but only to the extent that \\n        such amount is attributable to the participation in such \\n        program of any eligible STEM teacher, including amounts paid to \\n        such a teacher as a stipend while participating in such \\n        program.\\n            ``(2) STEM externship program.--The term `STEM externship \\n        program' means any program--\\n                    ``(A) established by a taxpayer engaged in a trade \\n                or business within an area of science, technology, \\n                engineering, or mathematics, and\\n                    ``(B) under which eligible STEM teachers receive \\n                training to enhance their teaching skills in the areas \\n                of science, technology, engineering, or mathematics or \\n                otherwise improve their knowledge in such areas.\\n            ``(3) Eligible stem teacher.--The term `eligible STEM \\n        teacher' means any individual--\\n                    ``(A) who is a teacher in grades K-12 at an \\n                educational organization described in section \\n                170(b)(1)(A)(ii) which is located in the United States \\n                or which is located on a United States military base \\n                outside the United States, and\\n                    ``(B) whose teaching responsibilities at such \\n                school include, or are likely to include, any course in \\n                the areas of science, technology, engineering, or \\n                mathematics.\\n    ``(e) STEM Teacher Training Expenses.--The term `STEM teacher \\ntraining expenses' means any amount paid or incurred by a taxpayer \\nengaged in a trade or business within an area of science, technology, \\nengineering, or mathematics which is attributable to the participation \\nof any eligible STEM teacher in a regular training program provided to \\nemployees of the taxpayer which is determined by such teacher's school \\nas enhancing such teacher's teaching skills in the areas of science, \\ntechnology, engineering, or mathematics.\\n    ``(f) Denial of Double Benefit.--No deduction shall be allowed \\nunder this chapter for any amount allowed as a credit under this \\nsection.''.\\n    (b) Conforming Amendments.--\\n            (1) Section 38(b) of such Code is amended by striking \\n        ``plus'' at the end of paragraph (30), by striking the period \\n        at the end of paragraph (31), and inserting ``, plus'', and by \\n        adding at the end the following new paragraph:\\n            ``(32) the elementary and secondary science, technology, \\n        engineering, and mathematics (STEM) contributions credit \\n        determined under section 45O.''.\\n            (2) The table of sections for subpart D of part IV of \\n        subchapter A of chapter 1 of such Code is amended by adding at \\n        the end the following new item:\\n\\n``Sec. 45O. Contributions benefiting science, technology, engineering, \\n                            and mathematics education at the elementary \\n                            and secondary school level.''.\\n    (c) Effective Date.--The amendments made by this section shall \\napply to taxable years beginning after the date of the enactment of \\nthis Act.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Refiner([billsum0], w2v, score, 4)\n",
    "r.refine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.refined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res =  r.assess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[\"scores\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[\"correlations\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_score.utils import *\n",
    "from custom_score.score import score \n",
    "from rouge_score import rouge_scorer\n",
    "import bert_score\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "from colorama import Fore, Style\n",
    "\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\")\n",
    "from BARTScore.bart_score import BARTScorer\n",
    "\n",
    "class Refiner:\n",
    "\n",
    "    def __init__(self, corpus, model, scorer=score, ratio=2, threshold=0.70, maxSpacing=10, printRange=range(0, 1)):\n",
    "        \"\"\"\n",
    "        Constructor of the Refiner class. Aims at reducing the size and noise of a given independant list of documents.\n",
    "        \n",
    "        :param1 self (Refiner): Object to initialize.\n",
    "        :param2 corpus (List): List of documents to simplify.\n",
    "        :param3 model (Any): Model used to compute scores and create sentence's ranking.\n",
    "        :param4 ratio (float, int or array-like): Number determining how much the reference text will be shortened. \n",
    "        :param5 threshold (float): Number between 0 and 1 indicating the lowest acceptable quality when tuning the length of the summary.\n",
    "        :param6 maxSpacing (int): Maximal number of adjacent space to be found and suppressed in the corpus.\n",
    "        :param7 printRange (range): Range of corpus that should be displayed when the Refiner object in printed. \n",
    "        \"\"\"\n",
    "        self.corpus = corpus\n",
    "        self.processedCorpus = None\n",
    "        self.model = model\n",
    "        self.scorer = scorer\n",
    "        self.ratio = ratio\n",
    "        self.threshold = threshold\n",
    "        self.ms = maxSpacing\n",
    "        self.refined = None\n",
    "        self.printRange = printRange\n",
    "        self.selectedIndexes = None\n",
    "\n",
    "    def refine(self):\n",
    "        \"\"\"\n",
    "        Return a reduced string computed using static embedding vectors similarity. Also denoises the data by removing superfluous elements such as \"\\n\" or useless signs.\n",
    "\n",
    "        :param1 self (Refiner): Refiner Object (see __init__ function for more details).\n",
    "\n",
    "        :output refined (string): refined version of the initial document.\n",
    "        \"\"\"\n",
    "        self.refined = []\n",
    "        self.selectedIndexes = []\n",
    "        self.processedCorpus = []\n",
    "        for indiv in self.corpus:\n",
    "            #preprocess corpus\n",
    "            clean = cleanString(indiv, self.ms)\n",
    "            sentences = clean.split(\".\")\n",
    "            sentences.pop()\n",
    "            temp = []\n",
    "            for sentence in sentences: \n",
    "                if sentence != None and sentence != \"\":\n",
    "                    temp.append(sentence)\n",
    "            sentences = temp\n",
    "            respaced_sentences = []\n",
    "            for sentence in sentences:\n",
    "                if sentence[0] == \" \":\n",
    "                    sentence = sentence[1:]\n",
    "                respaced_sentences.append(sentence)\n",
    "            self.processedCorpus.append(respaced_sentences)\n",
    "\n",
    "            #compute ranking\n",
    "            scores = []\n",
    "            for sentence in respaced_sentences:\n",
    "                scoreOut = self.scorer(self.model, [sentence], [indiv])\n",
    "                R = parseScore(scoreOut)\n",
    "                scores.append(R)\n",
    "            \n",
    "            #compute distances\n",
    "            distances = []\n",
    "            for x in range(len(respaced_sentences)):\n",
    "                distance = []\n",
    "                for y in range(len(respaced_sentences)):\n",
    "                    if x != y:\n",
    "                        try:\n",
    "                            scoreOut = self.scorer(self.model, [respaced_sentences[x]], [respaced_sentences[y]])\n",
    "                            curDistance = parseScore(scoreOut)\n",
    "                        except:\n",
    "                            curDistance = -1\n",
    "                    else:\n",
    "                        curDistance = 1\n",
    "                    distance.append(curDistance)\n",
    "                distances.append(distance)\n",
    "            distances = parseDistances(distances)\n",
    "\n",
    "            #selection of best individuals\n",
    "            if type(self.ratio) == int or type(self.ratio) == float: \n",
    "                indices = sentenceSelection(respaced_sentences, scores, distances, self.ratio)\n",
    "            else:\n",
    "                for curRatio in sorted(self.ratio):\n",
    "                    curIndices = sentenceSelection(respaced_sentences, scores, distances, curRatio)\n",
    "                    subCurRefined = [respaced_sentences[i] for i in curIndices]\n",
    "                    scoreOut = score(self.model, [\" \".join(subCurRefined)], [indiv])\n",
    "                    curScore = parseScore(scoreOut)\n",
    "                    if curScore < self.threshold:\n",
    "                        try:\n",
    "                            indices = curBest\n",
    "                        except:\n",
    "                            indices = curIndices\n",
    "                        finally:\n",
    "                            break\n",
    "                    else:\n",
    "                        curBest = curIndices\n",
    "                if \"indices\" not in locals():\n",
    "                    indices = curIndices\n",
    "            indices.sort()\n",
    "            curRefined = []\n",
    "            for index in indices:\n",
    "                curRefined.append(respaced_sentences[index])\n",
    "            curRefined = \". \\n\".join(curRefined) + \".\"\n",
    "            self.selectedIndexes.append(indices)\n",
    "            self.refined.append(curRefined)\n",
    "\n",
    "    def assess(self, verbose=True):\n",
    "        \"\"\"\n",
    "        Assesses quality of the refined corpus by computing Static BERTscore and Rouge-Score on the refined version compared to it's initial version.\n",
    "\n",
    "        :param1 self (Refiner): Refiner Object (see __init__ function for more details).\n",
    "        :param2 verbose (Boolean): When put to true, assess results will be printed.\n",
    "\n",
    "        :output (dict): Dictionnary containing both the scores of Static BERTScore, BERTScore, BARTScore and Rouge as well as their correlation.\n",
    "        \"\"\"\n",
    "        assert self.refined != None, \"refined corpus doesn't exists\"\n",
    "\n",
    "        #Static BERTScore computation\n",
    "        scoreOut = self.scorer(self.model, self.refined, self.corpus)\n",
    "        customScore = [parseScore(curScore) for curScore in scoreOut]\n",
    "\n",
    "        #Rouge-Score computation\n",
    "        rougeScorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "        rougeScore = [rougeScorer.score(c, r) for c, r in zip(self.corpus, self.refined)]\n",
    "\n",
    "        #BERTScore computation\n",
    "        with nostd():\n",
    "            bertscore = bert_score.score(self.refined, self.corpus.to_list(), lang=\"en\", verbose=0)\n",
    "\n",
    "        #bartscore\n",
    "        bart_scorer = BARTScorer(device='cuda:0', checkpoint='facebook/bart-large-cnn')\n",
    "        bartscore = bart_scorer.score(self.corpus.to_list(), self.refined, batch_size=4)\n",
    "\n",
    "        #Data formating\n",
    "        custom_R = [round(t, 2) for t in customScore]\n",
    "        bertscore_R = [round(t.item(), 2) for t in bertscore[1]]\n",
    "        bartscore = [round(t, 2) for t in bartscore]\n",
    "        rouge1_R = [round(t['rouge1'][0], 2) for t in rougeScore]\n",
    "        rougeL_R = [round(t['rougeL'][0], 2) for t in rougeScore]\n",
    "\n",
    "        dfCustom = pd.DataFrame({'CBERT' : custom_R,\n",
    "                                 'BERTScore' : bertscore_R,\n",
    "                                 'BARTScore' : bartscore,\n",
    "                                 'R-1' : rouge1_R,\n",
    "                                 'R-L' : rougeL_R\n",
    "                                })\n",
    "\n",
    "        #Correlation estimation\n",
    "        pearsonCor_c_r1 = np.round(pearsonr(custom_R, rouge1_R), 2)\n",
    "        pearsonCor_c_rl = np.round(pearsonr(custom_R, rougeL_R), 2)\n",
    "        pearsonCor_bertscore_r1 = np.round(pearsonr(bertscore_R, rouge1_R), 2)\n",
    "        pearsonCor_bertscore_rl = np.round(pearsonr(bertscore_R, rougeL_R), 2)\n",
    "        pearsonCor_bartscore_r1 = np.round(pearsonr(bartscore, rouge1_R), 2)\n",
    "        pearsonCor_bartscore_rl = np.round(pearsonr(bartscore, rougeL_R), 2)\n",
    "\n",
    "        dfCor = pd.DataFrame({'pearson_CBERT_R-1' : pearsonCor_c_r1,\n",
    "                              'pearson_CBERT_R-L' : pearsonCor_c_rl,\n",
    "                              'pearson_BERT_R-1' : pearsonCor_bertscore_r1,\n",
    "                              'pearson_BERT_R-l' : pearsonCor_bertscore_rl,\n",
    "                              'pearson_BART_R-1' : pearsonCor_bartscore_r1,\n",
    "                              'pearson_BART_R-l' : pearsonCor_bartscore_rl}, index=[\"Pearson score\", \"p-value\"])\n",
    "        if verbose:\n",
    "            printout = \"Scores: \\n\"\n",
    "            printout += dfCustom.to_string() + \"\\n\\n\"\n",
    "            printout += \"Correlations: \\n\"\n",
    "            printout += dfCor.to_string()\n",
    "            print(printout)\n",
    "\n",
    "        return {\"scores\": dfCustom, \"correlations\": dfCor}\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        printout = \"--------REFINER OBJECT--------\\n\\n\"\n",
    "        printout += \"Number of Documents : \" + str(len(self.corpus)) + \"\\n\"\n",
    "        printout += \"Corpus Avg Size     : \" + str(int(np.average([len(x) for x in self.corpus]))+1) + \"\\n\"\n",
    "        printout += \"Refined Avg Size    : \" + str(int(np.average([len(x) for x in self.refined]))+1) + \"\\n\"\n",
    "        printout += \"Ratio(s)            : \" + str(self.ratio) + \"\\n\"\n",
    "        printout += \"Threshold           : \" + str(self.threshold) + \"\\n\"\n",
    "        printout += \"Maximum Spacing     : \" + str(self.ms) + \"\\n\"\n",
    "        \n",
    "        self.printRange = self.printRange if self.printRange.start >= 0 and self.printRange.stop < len(self.processedCorpus) else range(0, len(self.processedCorpus))\n",
    "\n",
    "        for index in self.printRange:\n",
    "            printout += f\"\\nCorpus no.{index+1} : \\n\" + str(\".\\n\".join([f\"{Fore.LIGHTGREEN_EX}{self.processedCorpus[index][i]}{Style.RESET_ALL}\"\n",
    "                                                        if i in self.selectedIndexes[index]\n",
    "                                                        else f\"{Fore.RED}{self.processedCorpus[index][i]}{Style.RESET_ALL}\"\n",
    "                                                        for i in range(len(self.processedCorpus[index]))])) + \".\" + \"\\n\"\n",
    "        printout += \"\\n------------------------------\"\n",
    "        return printout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized scorer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_score.utils import *\n",
    "from custom_score.score import score \n",
    "from rouge_score import rouge_scorer\n",
    "import bert_score\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "from colorama import Fore, Style\n",
    "from datetime import datetime\n",
    "\n",
    "import sys\n",
    "sys.path.append(get_git_root())\n",
    "\n",
    "from BARTScore.bart_score import BARTScorer\n",
    "\n",
    "class Refiner:\n",
    "\n",
    "    def __init__(self, corpus, gold, model=None, metric=score, ratio=2, threshold=0.70, maxSpacing=10, printRange=range(0, 1)):\n",
    "        \"\"\"\n",
    "        Constructor of the Refiner class. Aims at reducing the size and noise of a given independant list of documents.\n",
    "        \n",
    "        :param1 self (Refiner): Object to initialize.\n",
    "        :param2 corpus (List): List of documents to simplify.\n",
    "        :param3 gold (List): List of gold summaries to compare to the extractive summary created with the refiner.\n",
    "        :param4 model (Any): Model used to compute scores and create sentence's ranking.\n",
    "        :param5 ratio (float, int or array-like): Number determining how much the reference text will be shortened. \n",
    "        :param6 threshold (float): Number between 0 and 1 indicating the lowest acceptable quality when tuning the length of the summary.\n",
    "        :param7 maxSpacing (int): Maximal number of adjacent space to be found and suppressed in the corpus.\n",
    "        :param8 printRange (range): Range of corpus that should be displayed when the Refiner object in printed. \n",
    "        \"\"\"\n",
    "        self.corpus = corpus\n",
    "        self.gold = gold\n",
    "        self.processedCorpus = None\n",
    "        self.model = model\n",
    "        self.metric = metric\n",
    "        self.ratio = ratio\n",
    "        self.threshold = threshold\n",
    "        self.ms = maxSpacing\n",
    "        self.refined = None\n",
    "        self.printRange = printRange\n",
    "        self.selectedIndexes = None\n",
    "\n",
    "    def refine(self, checkpoints=False, saveRate=50):\n",
    "        \"\"\"\n",
    "        Return a reduced string computed using static embedding vectors similarity. Also denoises the data by removing superfluous elements such as \"\\n\" or useless signs.\n",
    "\n",
    "        :param1 self (Refiner): Refiner Object (see __init__ function for more details).\n",
    "        :param2 checkpoints (bool): Indicates whether the refining should save partial outputs along computation to prevent from losing data in the context of a crash.\n",
    "        :param3 saveRate (int): Only applicable id safe equals True. Specify the number of consicutive iterations after which a checkpoint should be created. \n",
    "\n",
    "        :output refined (string): refined version of the initial document.\n",
    "        \"\"\"\n",
    "        self.refined = []\n",
    "        self.selectedIndexes = []\n",
    "        self.processedCorpus = []\n",
    "        if checkpoints:\n",
    "            iter = 0\n",
    "            start = datetime.now()\n",
    "            createFolder = True\n",
    "\n",
    "        for indiv in self.corpus:\n",
    "            #preprocess corpus\n",
    "            clean = cleanString(indiv, self.ms)\n",
    "            sentences = clean.split(\".\")\n",
    "            sentences.pop()\n",
    "            temp = []\n",
    "            for sentence in sentences: \n",
    "                if sentence != None and sentence != \"\":\n",
    "                    temp.append(sentence)\n",
    "            sentences = temp\n",
    "            respaced_sentences = []\n",
    "            for sentence in sentences:\n",
    "                if sentence[0] == \" \":\n",
    "                    sentence = sentence[1:]\n",
    "                respaced_sentences.append(sentence)\n",
    "            self.processedCorpus.append(respaced_sentences)\n",
    "\n",
    "            #compute ranking\n",
    "            scores = []\n",
    "            for sentence in respaced_sentences:\n",
    "                scoreOut = self.scorer(indiv.replace(sentence+\".\", \"\"), sentence)\n",
    "                scores.append(scoreOut)\n",
    "            \n",
    "            #compute distances\n",
    "            distances = []\n",
    "            for x in range(len(respaced_sentences)):\n",
    "                distance = []\n",
    "                for y in range(len(respaced_sentences)):\n",
    "                    if x != y:\n",
    "                        try:\n",
    "                            curDistance = self.scorer(respaced_sentences[x], respaced_sentences[y])\n",
    "                        except:\n",
    "                            curDistance = -1\n",
    "                    else:\n",
    "                        curDistance = 1\n",
    "                    distance.append(curDistance)\n",
    "                distances.append(distance)\n",
    "            distances = parseDistances(distances)\n",
    "\n",
    "            #selection of best individuals\n",
    "            indices = None\n",
    "            if type(self.ratio) == int or type(self.ratio) == float: \n",
    "                indices = sentenceSelection(respaced_sentences, scores, distances, self.ratio)\n",
    "            else:\n",
    "                for curRatio in sorted(self.ratio):\n",
    "                    curIndices = sentenceSelection(respaced_sentences, scores, distances, curRatio)\n",
    "                    subCurRefined = [respaced_sentences[i] for i in curIndices]\n",
    "                    curSentence = \" \".join(subCurRefined)\n",
    "                    curScore = self.scorer(indiv.replace(curSentence+\".\", \"\"), curSentence)\n",
    "                    if curScore < self.threshold:\n",
    "                        try:\n",
    "                            indices = curBest\n",
    "                        except:\n",
    "                            indices = curIndices\n",
    "                        finally:\n",
    "                            break\n",
    "                    else:\n",
    "                        curBest = curIndices\n",
    "                if indices is None:\n",
    "                    indices = curIndices\n",
    "            indices.sort()\n",
    "            curRefined = []\n",
    "            for index in indices:\n",
    "                curRefined.append(respaced_sentences[index])\n",
    "            curRefined = \". \\n\".join(curRefined) + \".\"\n",
    "            self.selectedIndexes.append(indices)\n",
    "            self.refined.append(curRefined)\n",
    "\n",
    "            #checkpoint verification\n",
    "            if checkpoints:\n",
    "                if iter % saveRate == 0 and iter != 0:\n",
    "                    stop = datetime.now()\n",
    "                    partial_runtime = stop - start\n",
    "                    self.save(runtime=partial_runtime, new=createFolder)\n",
    "                    createFolder = False\n",
    "                iter += 1\n",
    "        if checkpoints:\n",
    "            stop = datetime.now()\n",
    "            runtime = stop - start\n",
    "            self.save(runtime=runtime, new=createFolder)\n",
    "\n",
    "    def scorer(self, ref, cand, param=\"F\"):\n",
    "        param = param.upper()\n",
    "        if self.metric.__module__ == \"custom_score.score\":\n",
    "            if self.model == None:\n",
    "                self.model = model_load(\"Word2Vec\", True)\n",
    "            score = self.metric(self.model, [cand], [ref])[0]\n",
    "            \n",
    "        elif self.metric.__module__ == \"bert_score.score\":\n",
    "            with nostd():\n",
    "                score = self.metric([cand], [ref], lang=\"en\", verbose=0)\n",
    "            score = [value.item() for value in score]\n",
    "            score[0], score[1] = score[1], score[0]\n",
    "      \n",
    "        if param == \"F\":\n",
    "            output = score[2]\n",
    "        elif param == \"R\":\n",
    "            output = score[0]\n",
    "        elif param == \"P\":\n",
    "            output = score[1]\n",
    "        elif param == \"ALL\":\n",
    "            output = score\n",
    "        return output\n",
    "\n",
    "    def assess(self, start=0, stop=None, verbose=True):\n",
    "        \"\"\"\n",
    "        Assesses quality of the refined corpus by computing Static BERTscore and Rouge-Score on the refined version compared to it's initial version.\n",
    "\n",
    "        :param1 self (Refiner): Refiner Object (see __init__ function for more details).\n",
    "        :param2 start (int): Starting index to assess.\n",
    "        :param3 stop (int): Ending index to assess.\n",
    "        :param4 verbose (Boolean): When put to True, assess results will be printed.\n",
    "\n",
    "        :output (dict): Dictionnary containing both the scores of Static BERTScore, BERTScore, BARTScore and Rouge as well as their correlation.\n",
    "        \"\"\"\n",
    "        assert self.refined != None, \"refined corpus doesn't exists\"\n",
    "        \n",
    "        if stop == None:\n",
    "            stop = len(self.refined)\n",
    "        subset_refined = self.refined[start:stop]\n",
    "        subset_gold = self.gold[start:stop]\n",
    "\n",
    "        #Static BERTScore computation\n",
    "        scoreOut = score(self.model, subset_refined, subset_gold)\n",
    "        customScore = [parseScore(curScore) for curScore in scoreOut]\n",
    "\n",
    "        #Rouge-Score computation\n",
    "        rougeScorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        rougeScore = [rougeScorer.score(c, r) for c, r in zip(subset_gold, subset_refined)]\n",
    "\n",
    "        #BERTScore computation\n",
    "        with nostd():\n",
    "            bertscore = bert_score.score(subset_refined, subset_gold, lang=\"en\", verbose=0)\n",
    "\n",
    "        #bartscore\n",
    "        bart_scorer = BARTScorer(device='cuda:0', checkpoint='facebook/bart-large-cnn')\n",
    "        bartscore = bart_scorer.score(subset_refined, subset_gold, batch_size=4)\n",
    "\n",
    "        #Data formating\n",
    "        custom_R = [round(t, 2) for t in customScore]\n",
    "        bertscore_R = [round(t.item(), 2) for t in bertscore[1]]\n",
    "        bartscore = [round(t, 2) for t in bartscore]\n",
    "        rouge1_R = [round(t['rouge1'][0], 2) for t in rougeScore]\n",
    "        rouge2_R = [round(t['rouge2'][0], 2) for t in rougeScore]\n",
    "        rougeL_R = [round(t['rougeL'][0], 2) for t in rougeScore]\n",
    "\n",
    "        dfCustom = pd.DataFrame({'CBERT' : custom_R,\n",
    "                                 'BERTScore' : bertscore_R,\n",
    "                                 'BARTScore' : bartscore,\n",
    "                                 'R-1' : rouge1_R,\n",
    "                                 'R-2' : rouge2_R,\n",
    "                                 'R-L' : rougeL_R\n",
    "                                })\n",
    "\n",
    "        #Correlation estimation\n",
    "        pearsonCor_c_r1 = np.round(pearsonr(custom_R, rouge1_R), 2)\n",
    "        pearsonCor_c_r2 = np.round(pearsonr(custom_R, rouge2_R), 2)\n",
    "        pearsonCor_c_rl = np.round(pearsonr(custom_R, rougeL_R), 2)\n",
    "        pearsonCor_bertscore_r1 = np.round(pearsonr(bertscore_R, rouge1_R), 2)\n",
    "        pearsonCor_bertscore_r2 = np.round(pearsonr(bertscore_R, rouge2_R), 2)\n",
    "        pearsonCor_bertscore_rl = np.round(pearsonr(bertscore_R, rougeL_R), 2)\n",
    "        pearsonCor_bartscore_r1 = np.round(pearsonr(bartscore, rouge1_R), 2)\n",
    "        pearsonCor_bartscore_r2 = np.round(pearsonr(bartscore, rouge2_R), 2)\n",
    "        pearsonCor_bartscore_rl = np.round(pearsonr(bartscore, rougeL_R), 2)\n",
    "\n",
    "        dfCor = pd.DataFrame({'pearson_CBERT_R-1' : pearsonCor_c_r1,\n",
    "                              'pearson_CBERT_R-2' : pearsonCor_c_r2,\n",
    "                              'pearson_CBERT_R-L' : pearsonCor_c_rl,\n",
    "                              'pearson_BERT_R-1' : pearsonCor_bertscore_r1,\n",
    "                              'pearson_BERT_R-2' : pearsonCor_bertscore_r2,\n",
    "                              'pearson_BERT_R-l' : pearsonCor_bertscore_rl,\n",
    "                              'pearson_BART_R-1' : pearsonCor_bartscore_r1,\n",
    "                              'pearson_BART_R-2' : pearsonCor_bartscore_r2,\n",
    "                              'pearson_BART_R-l' : pearsonCor_bartscore_rl}, index=[\"Pearson score\", \"p-value\"])\n",
    "        if verbose:\n",
    "            printout = \"Scores: \\n\"\n",
    "            printout += dfCustom.to_string() + \"\\n\\n\"\n",
    "            printout += \"Correlations: \\n\"\n",
    "            printout += dfCor.to_string()\n",
    "            print(printout)\n",
    "\n",
    "        return {\"scores\": dfCustom, \"correlations\": dfCor}\n",
    "    \n",
    "    def to_dataframe(self):\n",
    "        \"\"\"\n",
    "        Transforms a Refiner object to a dataframe.\n",
    "\n",
    "        :param1 self (Refiner): Refiner Object (see __init__ function for more details).\n",
    "\n",
    "        :output output (DataFrame): DataFrame containing both the corpus and the refined texts of the Refiner class. \n",
    "        \"\"\"\n",
    "        output = pd.DataFrame({\"text\": self.corpus,\n",
    "                               \"summary\": self.refined,\n",
    "                               \"processedText\": [\". \".join(c) for c in self.processedCorpus]})\n",
    "        return output\n",
    "\n",
    "    def save(self, runtime=None, new=True):\n",
    "        \"\"\"\n",
    "        Saves Refiner output to a local folder.\n",
    "\n",
    "        :param1 self (Refiner): Refiner Object (see __init__ function for more details).\n",
    "        :param2 new (bool): Indicates if a new folder should be created. If false, output is append to the most recent ouput folder.\n",
    "        \"\"\"\n",
    "\n",
    "        #evaluation\n",
    "        start = 0\n",
    "        stop = len(self.refined)\n",
    "        assessement = self.assess(start=start, stop=stop)\n",
    "\n",
    "        #mainDf = r.to_dataframe()\n",
    "        scoreDf = assessement[\"scores\"]\n",
    "        corDf = assessement[\"correlations\"]\n",
    "\n",
    "        #write output\n",
    "        main_folder_path = os.path.join(get_git_root(), r\"myLibraries\\refining_output\")\n",
    "        countfile_name = r\"count.txt\"\n",
    "        if new:\n",
    "            count = updateFileCount(os.path.join(main_folder_path, countfile_name))\n",
    "        else:\n",
    "            count = readFileCount(os.path.join(main_folder_path, countfile_name))\n",
    "\n",
    "        current_path = os.path.join(main_folder_path, f\"experimentation_{count}\")\n",
    "        try:\n",
    "            os.mkdir(current_path)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "\n",
    "        #mainDf.to_csv(os.path.join(current_path, \"main.csv\"))\n",
    "        scoreDf.to_csv(os.path.join(current_path, \"scores.csv\"))\n",
    "        corDf.to_csv(os.path.join(current_path, \"correlations.csv\"))\n",
    "        with open(os.path.join(current_path, \"runtimes.txt\"), \"w\") as f:\n",
    "            f.write(str(runtime))\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"\n",
    "        Summarizes Refiner object to a string.\n",
    "\n",
    "        :param1 self (Refiner): Refiner Object (see __init__ function for more details).\n",
    "\n",
    "        :output printout (string): Summarized informations about the refiner object.\n",
    "        \"\"\"\n",
    "\n",
    "        printout = \"--------REFINER OBJECT--------\\n\\n\"\n",
    "        printout += \"Number of Documents : \" + str(len(self.corpus)) + \"\\n\"\n",
    "        printout += \"Corpus Avg Size     : \" + str(int(np.average([len(x) for x in self.corpus]))+1) + \"\\n\"\n",
    "        printout += \"Refined Avg Size    : \" + str(int(np.average([len(x) for x in self.refined]))+1) + \"\\n\"\n",
    "        printout += \"Ratio(s)            : \" + str(self.ratio) + \"\\n\"\n",
    "        printout += \"Threshold           : \" + str(self.threshold) + \"\\n\"\n",
    "        printout += \"Maximum Spacing     : \" + str(self.ms) + \"\\n\"\n",
    "        \n",
    "        self.printRange = self.printRange if self.printRange.start >= 0 and self.printRange.stop < len(self.processedCorpus) else range(0, len(self.processedCorpus))\n",
    "\n",
    "        for index in self.printRange:\n",
    "            printout += f\"\\nCorpus no.{index+1} : \\n\" + str(\".\\n\".join([f\"{Fore.LIGHTGREEN_EX}{self.processedCorpus[index][i]}{Style.RESET_ALL}\"\n",
    "                                                        if i in self.selectedIndexes[index]\n",
    "                                                        else f\"{Fore.RED}{self.processedCorpus[index][i]}{Style.RESET_ALL}\"\n",
    "                                                        for i in range(len(self.processedCorpus[index]))])) + \".\" + \"\\n\"\n",
    "        printout += \"\\n------------------------------\"\n",
    "        return printout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = model_load(\"Word2Vec\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Refiner([\"I am Marius. I like trains.\", \"Engineering is good. It is fun\"], [\"I am Marius. I like trains.\", \"I enjoy datascience. It is my studies.\"], w2v, metric=bert_score.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.refine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am Marius. \\nI like trains.', 'Engineering is good.']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.refined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: \n",
      "   CBERT  BERTScore  BARTScore   R-1  R-2   R-L\n",
      "0   1.00       0.98       0.70  1.00  1.0  1.00\n",
      "1   0.36       0.88       0.07  0.33  0.0  0.33\n",
      "\n",
      "Correlations: \n",
      "               pearson_CBERT_R-1  pearson_CBERT_R-2  pearson_CBERT_R-L  pearson_BERT_R-1  pearson_BERT_R-2  pearson_BERT_R-l  pearson_BART_R-1  pearson_BART_R-2  pearson_BART_R-l\n",
      "Pearson score                1.0                1.0                1.0               1.0               1.0               1.0               1.0               1.0               1.0\n",
      "p-value                      1.0                1.0                1.0               1.0               1.0               1.0               1.0               1.0               1.0\n"
     ]
    }
   ],
   "source": [
    "_=r.assess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------REFINER OBJECT--------\n",
      "\n",
      "Number of Documents : 2\n",
      "Corpus Avg Size     : 29\n",
      "Refined Avg Size    : 25\n",
      "Ratio(s)            : 2\n",
      "Threshold           : 0.7\n",
      "Maximum Spacing     : 10\n",
      "\n",
      "Corpus no.1 : \n",
      "\u001b[92mI am Marius\u001b[0m.\n",
      "\u001b[92mI like trains\u001b[0m.\n",
      "\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am Marius. I like trains.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.gold"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Billsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\myLibraries\")\n",
    "\n",
    "import pandas as pd\n",
    "from custom_score.utils import model_load\n",
    "from custom_score.refine import Refiner\n",
    "from custom_score.score import score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SECTION 1. SHORT TITLE.\\n\\n    This Act may be...</td>\n",
       "      <td>National Science Education Tax Incentive for B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SECTION 1. SHORT TITLE.\\n\\n    This Act may be...</td>\n",
       "      <td>Small Business Expansion and Hiring Act of 201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SECTION 1. RELEASE OF DOCUMENTS CAPTURED IN IR...</td>\n",
       "      <td>Requires the Director of National Intelligence...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SECTION 1. SHORT TITLE.\\n\\n    This Act may be...</td>\n",
       "      <td>National Cancer Act of 2003 - Amends the Publi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SECTION 1. SHORT TITLE.\\n\\n    This Act may be...</td>\n",
       "      <td>Military Call-up Relief Act - Amends the Inter...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   \n",
       "0  SECTION 1. SHORT TITLE.\\n\\n    This Act may be...  \\\n",
       "1  SECTION 1. SHORT TITLE.\\n\\n    This Act may be...   \n",
       "2  SECTION 1. RELEASE OF DOCUMENTS CAPTURED IN IR...   \n",
       "3  SECTION 1. SHORT TITLE.\\n\\n    This Act may be...   \n",
       "4  SECTION 1. SHORT TITLE.\\n\\n    This Act may be...   \n",
       "\n",
       "                                             summary  \n",
       "0  National Science Education Tax Incentive for B...  \n",
       "1  Small Business Expansion and Hiring Act of 201...  \n",
       "2  Requires the Director of National Intelligence...  \n",
       "3  National Cancer Act of 2003 - Amends the Publi...  \n",
       "4  Military Call-up Relief Act - Amends the Inter...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_url='https://drive.google.com/file/d/1Wd0M3qepNF6B4YwFYrpo7CaSERpudAG_/view?usp=share_link'\n",
    "dataset_url='https://drive.google.com/uc?export=download&id=' + dataset_url.split('/')[-2]\n",
    "dataset = pd.read_json(dataset_url, lines=True)\n",
    "dataset = dataset.loc[:, [\"text\", \"summary\"]]\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = dataset.iloc[:2, :]\n",
    "subset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = model_load(\"Word2Vec\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Refiner(subset[\"text\"].to_list(), subset[\"summary\"].to_list(), w2v, bert_score.score, ratio=2, maxSpacing=15, printRange=range(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.refine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------REFINER OBJECT--------\n",
      "\n",
      "Number of Documents : 2\n",
      "Corpus Avg Size     : 7509\n",
      "Refined Avg Size    : 3151\n",
      "Ratio(s)            : 2\n",
      "Threshold           : 0.7\n",
      "Maximum Spacing     : 15\n",
      "\n",
      "Corpus no.1 : \n",
      "\u001b[31mSECTION 1\u001b[0m.\n",
      "\u001b[31mSHORT TITLE\u001b[0m.\n",
      "\u001b[92mThis Act may be cited as the National Science Education Tax Incentive for Businesses Act of 2007\u001b[0m.\n",
      "\u001b[31mSEC\u001b[0m.\n",
      "\u001b[31m2\u001b[0m.\n",
      "\u001b[31mCREDITS FOR CERTAIN CONTRIBUTIONS BENEFITING SCIENCE, TECHNOLOGY, ENGINEERING, AND MATHEMATICS EDUCATION AT THE ELEMENTARY AND SECONDARY SCHOOL LEVEL\u001b[0m.\n",
      "\u001b[31m(a) In General\u001b[0m.\n",
      "\u001b[31mSubpart D of part IV of subchapter A of chapter 1 of the Internal Revenue Code of 1986 (relating to business related credits) is amended by adding at the end the following new section: SEC\u001b[0m.\n",
      "\u001b[31m45O\u001b[0m.\n",
      "\u001b[31mCONTRIBUTIONS BENEFITING SCIENCE, TECHNOLOGY, ENGINEERING, AND MATHEMATICS EDUCATION AT THE ELEMENTARY AND SECONDARY SCHOOL LEVEL\u001b[0m.\n",
      "\u001b[31m(a) In General\u001b[0m.\n",
      "\u001b[92mFor purposes of section 38, the elementary and secondary science, technology, engineering, and mathematics (STEM) contributions credit determined under this section for the taxable year is an amount equal to 100 percent of the qualified STEM contributions of the taxpayer for such taxable year\u001b[0m.\n",
      "\u001b[31m(b) Qualified STEM Contributions\u001b[0m.\n",
      "\u001b[92mFor purposes of this section, the term `qualified STEM contributions' means (1) STEM school contributions, (2) STEM teacher externship expenses, and (3) STEM teacher training expenses\u001b[0m.\n",
      "\u001b[31m(c) STEM School Contributions\u001b[0m.\n",
      "\u001b[92mFor purposes of this section (1) In general\u001b[0m.\n",
      "\u001b[92mThe term `STEM school contributions' means (A) STEM property contributions, and (B) STEM service contributions\u001b[0m.\n",
      "\u001b[92m(2) STEM property contributions\u001b[0m.\n",
      "\u001b[31mThe term `STEM property contributions' means the amount which would (but for subsection (f)) be allowed as a deduction under section 170 for a charitable contribution of STEM inventory property if (A) the donee is an elementary or secondary school described in section 170(b)(1)(A)(ii), (B) substantially all of the use of the property by the donee is within the United States or within the defense dependents' education system for educational purposes in any of the grades K12 that are related to the purpose or function of the donee, (C) the original use of the property begins with the donee, (D) the property will fit productively into the donee's education plan, (E) the property is not transferred by the donee in exchange for money, other property, or services, except for shipping, installation and transfer costs, and (F) the donee's use and disposition of the property will be in accordance with the provisions of subparagraphs (B) and (E)\u001b[0m.\n",
      "\u001b[92mThe determination of the amount of deduction under section 170 for purposes of this paragraph shall be made as if the limitation under section 170(e)(3)(B) applied to all STEM inventory property\u001b[0m.\n",
      "\u001b[31m(3) STEM service contributions\u001b[0m.\n",
      "\u001b[31mThe term `STEM service contributions' means the amount paid or incurred during the taxable year for STEM services provided in the United States or in the defense dependents' education system for the exclusive benefit of students at an elementary or secondary school described in section 170(b)(1)(A)(ii) but only if (A) the taxpayer is engaged in the trade or business of providing such services on a commercial basis, and (B) no charge is imposed for providing such services\u001b[0m.\n",
      "\u001b[31m(4) STEM inventory property\u001b[0m.\n",
      "\u001b[31mThe term `STEM inventory property' means, with respect to any contribution to a school, any property (A) which is described in paragraph (1) or (2) of section 1221(a) with respect to the donor, and (B) which is determined by the school to be needed by the school in providing education in grades K12 in the areas of science, technology, engineering, or mathematics\u001b[0m.\n",
      "\u001b[31m(5) STEM services\u001b[0m.\n",
      "\u001b[92mThe term `STEM services' means, with respect to any contribution to a school, any service determined by the school to be needed by the school in providing education in grades K12 in the areas of science, technology, engineering, or mathematics, including teaching courses of instruction at such school in any such area\u001b[0m.\n",
      "\u001b[92m(6) Defense dependents' education system\u001b[0m.\n",
      "\u001b[92mFor purposes of this subsection, the term `defense dependents' education system' means the program established and operated under the Defense Dependents' Education Act of 1978 (20 U\u001b[0m.\n",
      "\u001b[31mS\u001b[0m.\n",
      "\u001b[31mC\u001b[0m.\n",
      "\u001b[31m921 et seq\u001b[0m.\n",
      "\u001b[31m)\u001b[0m.\n",
      "\u001b[92m(d) STEM Teacher Externship Expenses\u001b[0m.\n",
      "\u001b[31mFor purposes of this section (1) In general\u001b[0m.\n",
      "\u001b[92mThe term `STEM teacher externship expenses' means any amount paid or incurred to carry out a STEM externship program of the taxpayer but only to the extent that such amount is attributable to the participation in such program of any eligible STEM teacher, including amounts paid to such a teacher as a stipend while participating in such program\u001b[0m.\n",
      "\u001b[31m(2) STEM externship program\u001b[0m.\n",
      "\u001b[31mThe term `STEM externship program' means any program (A) established by a taxpayer engaged in a trade or business within an area of science, technology, engineering, or mathematics, and (B) under which eligible STEM teachers receive training to enhance their teaching skills in the areas of science, technology, engineering, or mathematics or otherwise improve their knowledge in such areas\u001b[0m.\n",
      "\u001b[92m(3) Eligible stem teacher\u001b[0m.\n",
      "\u001b[92mThe term `eligible STEM teacher' means any individual (A) who is a teacher in grades K12 at an educational organization described in section 170(b)(1)(A)(ii) which is located in the United States or which is located on a United States military base outside the United States, and (B) whose teaching responsibilities at such school include, or are likely to include, any course in the areas of science, technology, engineering, or mathematics\u001b[0m.\n",
      "\u001b[92m(e) STEM Teacher Training Expenses\u001b[0m.\n",
      "\u001b[92mThe term `STEM teacher training expenses' means any amount paid or incurred by a taxpayer engaged in a trade or business within an area of science, technology, engineering, or mathematics which is attributable to the participation of any eligible STEM teacher in a regular training program provided to employees of the taxpayer which is determined by such teacher's school as enhancing such teacher's teaching skills in the areas of science, technology, engineering, or mathematics\u001b[0m.\n",
      "\u001b[31m(f) Denial of Double Benefit\u001b[0m.\n",
      "\u001b[92mNo deduction shall be allowed under this chapter for any amount allowed as a credit under this section\u001b[0m.\n",
      "\u001b[92m(b) Conforming Amendments\u001b[0m.\n",
      "\u001b[92m(1) Section 38(b) of such Code is amended by striking plus at the end of paragraph (30), by striking the period at the end of paragraph (31), and inserting , plus, and by adding at the end the following new paragraph: (32) the elementary and secondary science, technology, engineering, and mathematics (STEM) contributions credit determined under section 45O\u001b[0m.\n",
      "\u001b[31m(2) The table of sections for subpart D of part IV of subchapter A of chapter 1 of such Code is amended by adding at the end the following new item: Sec\u001b[0m.\n",
      "\u001b[92m45O\u001b[0m.\n",
      "\u001b[92mContributions benefiting science, technology, engineering, and mathematics education at the elementary and secondary school level\u001b[0m.\n",
      "\u001b[31m(c) Effective Date\u001b[0m.\n",
      "\u001b[31mThe amendments made by this section shall apply to taxable years beginning after the date of the enactment of this Act\u001b[0m.\n",
      "\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: \n",
      "   CBERT  BERTScore  BARTScore   R-1   R-2   R-L\n",
      "0   0.87       0.88       0.42  0.08  0.03  0.05\n",
      "1   0.91       0.89       0.43  0.44  0.25  0.23\n",
      "\n",
      "Correlations: \n",
      "               pearson_CBERT_R-1  pearson_CBERT_R-2  pearson_CBERT_R-L  pearson_BERT_R-1  pearson_BERT_R-2  pearson_BERT_R-l  pearson_BART_R-1  pearson_BART_R-2  pearson_BART_R-l\n",
      "Pearson score                1.0                1.0                1.0               1.0               1.0               1.0               1.0               1.0               1.0\n",
      "p-value                      1.0                1.0                1.0               1.0               1.0               1.0               1.0               1.0               1.0\n"
     ]
    }
   ],
   "source": [
    "res = s.assess()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on PubMed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from custom_score.refine import Refiner\n",
    "from custom_score.score import score\n",
    "from custom_score.utils import model_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_path = r\"C:\\Pro\\Stages\\A4 - DVRC\\Work\\Datasets\\pubmed\\test.json\"\n",
    "pubmed_test = pd.read_json(pm_path, lines=True)\n",
    "pm = pubmed_test[[\"article_text\", \"abstract_text\"]]\n",
    "cleaner = lambda x: \". \".join(x).replace(\"<S>\", \"\").strip()\n",
    "\n",
    "pm.loc[:,\"abstract_text\"] = pm[\"abstract_text\"].replace(regex=r\"\\[[^\\]]*\\]\", value=\"\")\n",
    "pm.loc[:,\"article_text\"] = pm[\"article_text\"].replace(regex=r\"\\[[^\\]]*\\]\", value=\"\")\n",
    "pm.loc[:,\"abstract_text\"] = pm[\"abstract_text\"].map(cleaner)\n",
    "pm.loc[:,\"article_text\"] = pm[\"article_text\"].map(cleaner)\n",
    "pm = pm.rename(columns={\"article_text\":\"text\", \"abstract_text\":\"summary\"})\n",
    "pubmed = pm.copy()\n",
    "\n",
    "del pm \n",
    "del pubmed_test\n",
    "\n",
    "pubmed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lim = 10\n",
    "subset = pubmed.iloc[:lim, :]\n",
    "w2v = model_load(\"Word2Vec\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Refiner(subset[\"text\"].to_list(), subset[\"summary\"].to_list(), w2v, score, 4, printRange=range(0, 3))\n",
    "r.refine()\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = r.assess()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
