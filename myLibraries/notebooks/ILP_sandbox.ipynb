{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select N-words with higher TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "sys.path.append(r\"C:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\myLibraries\")\n",
    "from MARScore.score import MARSCore\n",
    "from MARScore.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_json(r'C:\\Pro\\Stages\\A4 - DVRC\\Work\\Datasets\\pubmed\\test.json', lines=True)\n",
    "dataset = dataset[[\"article_text\", \"abstract_text\"]]\n",
    "cleaner = lambda x: \". \".join(x).replace(\"<S>\", \"\").strip()\n",
    "format_dot = lambda x: x.replace(\" .\", \".\")\n",
    "dataset.loc[:,\"abstract_text\"] = dataset[\"abstract_text\"].replace(regex=r\"\\[[^\\]]*\\]\", value=\"\")\n",
    "dataset.loc[:,\"article_text\"] = dataset[\"article_text\"].replace(regex=r\"\\[[^\\]]*\\]\", value=\"\")\n",
    "dataset.loc[:,\"abstract_text\"] = dataset[\"abstract_text\"].map(cleaner)\n",
    "dataset.loc[:,\"article_text\"] = dataset[\"article_text\"].map(cleaner)\n",
    "dataset.loc[:,\"abstract_text\"] = dataset[\"abstract_text\"].map(cleanString)\n",
    "dataset.loc[:,\"article_text\"] = dataset[\"article_text\"].map(cleanString)\n",
    "dataset.loc[:,\"abstract_text\"] = dataset[\"abstract_text\"].map(format_dot)\n",
    "dataset.loc[:,\"article_text\"] = dataset[\"article_text\"].map(format_dot)\n",
    "dataset = dataset.rename(columns={\"abstract_text\": \"summary\",\n",
    "                        \"article_text\": \"text\"})\n",
    "\n",
    "subset = dataset.iloc[3:5, :]\n",
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tf = []\n",
    "for indiv in subset[\"text\"].to_list():\n",
    "    o, l = tokenizeCorpus(indiv)\n",
    "    v = vectorizeCorpus(o)\n",
    "    v, l = cleanAll(v, l)\n",
    "    all_tf.append(tf(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_words_format(tfs, n):\n",
    "    output = \"Maximize\\nscore: \"\n",
    "    for i, v in enumerate(tfs.values()):\n",
    "        output += f\"+ {v} c{i}\"\n",
    "    \n",
    "    output += \"\\n\\nSubject To\\n\"\n",
    "    output += \"length:\"\n",
    "    for i, v in enumerate(tfs.values()):\n",
    "        output += f\" c{i} +\"\n",
    "    output = output[:-1]\n",
    "    output += f\"< {n}\"\n",
    "\n",
    "    output += \"\\n\\nBinary\\n\"\n",
    "    for i, v in enumerate(tfs.values()):\n",
    "        output += f\"c{i}\\n\"\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format = best_words_format(all_tf[0], 4)\n",
    "\n",
    "with open(os.path.join(get_git_root(), r\"myLibraries\\ilp_outputs\\ilp_in.ilp\"), \"w\") as f:\n",
    "    f.write(format)\n",
    "    f.close()\n",
    "\n",
    "os.system(f'glpsol --tmlim 100 --lp \"{os.path.join(get_git_root(), r\"myLibraries/ilp_outputs/ilp_in.ilp\")}\" -o \"{os.path.join(get_git_root(), r\"myLibraries/ilp_outputs/ilp_out.sol\")}\"')\n",
    "\n",
    "with open(os.path.join(get_git_root(), r\"myLibraries/ilp_outputs/ilp_out.sol\"), \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "\n",
    "sentences_lines = [line for line in lines if re.search(r\"c\\d\", line)]\n",
    "\n",
    "sorted_lines = sorted(sentences_lines, key=lambda line: int(line.split()[1][1:]))\n",
    "result = [int(sorted_line.split()[3]) for sorted_line in sorted_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = [i for i, value in enumerate(result) if value == 1]\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = []\n",
    "for i, (k, v) in enumerate(all_tf[0].items()):\n",
    "    ref.append([i, k, v])\n",
    "ref.sort(key=lambda x: x[2], reverse=True)\n",
    "res = [v[0] for v in ref[:4]]\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select N-sentences maximizing words tfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\umap\\distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\umap\\distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\umap\\distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\umap\\umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "sys.path.append(r\"C:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\myLibraries\")\n",
    "from MARScore.score import MARSCore\n",
    "from MARScore.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>congenital adrenal hyperplasia ( cah ) refers ...</td>\n",
       "      <td>congenital adrenal hyperplasia is a group of a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>type 1 diabetes ( t1d ) results from the destr...</td>\n",
       "      <td>objective(s):pentoxifylline is an immunomodula...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   \n",
       "3  congenital adrenal hyperplasia ( cah ) refers ...  \\\n",
       "4  type 1 diabetes ( t1d ) results from the destr...   \n",
       "\n",
       "                                             summary  \n",
       "3  congenital adrenal hyperplasia is a group of a...  \n",
       "4  objective(s):pentoxifylline is an immunomodula...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_json(r'C:\\Pro\\Stages\\A4 - DVRC\\Work\\Datasets\\pubmed\\test.json', lines=True)\n",
    "dataset = dataset[[\"article_text\", \"abstract_text\"]]\n",
    "cleaner = lambda x: \". \".join(x).replace(\"<S>\", \"\").strip()\n",
    "format_dot = lambda x: x.replace(\" .\", \".\")\n",
    "dataset.loc[:,\"abstract_text\"] = dataset[\"abstract_text\"].replace(regex=r\"\\[[^\\]]*\\]\", value=\"\")\n",
    "dataset.loc[:,\"article_text\"] = dataset[\"article_text\"].replace(regex=r\"\\[[^\\]]*\\]\", value=\"\")\n",
    "dataset.loc[:,\"abstract_text\"] = dataset[\"abstract_text\"].map(cleaner)\n",
    "dataset.loc[:,\"article_text\"] = dataset[\"article_text\"].map(cleaner)\n",
    "dataset.loc[:,\"abstract_text\"] = dataset[\"abstract_text\"].map(cleanString)\n",
    "dataset.loc[:,\"article_text\"] = dataset[\"article_text\"].map(cleanString)\n",
    "dataset.loc[:,\"abstract_text\"] = dataset[\"abstract_text\"].map(format_dot)\n",
    "dataset.loc[:,\"article_text\"] = dataset[\"article_text\"].map(format_dot)\n",
    "dataset = dataset.rename(columns={\"abstract_text\": \"summary\",\n",
    "                        \"article_text\": \"text\"})\n",
    "\n",
    "subset = dataset.iloc[3:5, :]\n",
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = []\n",
    "all_tf = []\n",
    "for indiv in subset[\"text\"].to_list():\n",
    "    o, l = tokenizeCorpus(indiv)\n",
    "    v = vectorizeCorpus(o)\n",
    "    v, l = cleanAll(v, l)\n",
    "    tfs = tf(l)\n",
    "\n",
    "    all_tokens.append(l)\n",
    "    all_tf.append(tfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_sentences_format(tokens, tfs, n):\n",
    "    ### sentence tfs\n",
    "    i = 0\n",
    "    sentences_tfs = {}\n",
    "    for token in tokens:\n",
    "        if i in sentences_tfs.keys():\n",
    "            sentences_tfs[i] += tfs[token]\n",
    "        else:\n",
    "            sentences_tfs[i] = tfs[token]\n",
    "        if token == \".\":\n",
    "            i += 1\n",
    "    \n",
    "    output = \"Maximize\\nscore:\"\n",
    "    for i, cur_tf in enumerate(sentences_tfs.values()):\n",
    "        output += f\"+ {cur_tf} s{i}\"\n",
    "    \n",
    "    output += \"\\n\\nSubject To\\n\"\n",
    "    output += f\"length:\"\n",
    "    for i in range(len(sentences_tfs.keys())):\n",
    "        output += f\" s{i} +\"\n",
    "    output = output[:-1]\n",
    "    output += f\"< {n}\"\n",
    "\n",
    "    output += \"\\n\\nBinary\"\n",
    "    for i in range(len(sentences_tfs.keys())):\n",
    "        output += f\"\\ns{i}\"\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "format = best_sentences_format(all_tokens[1], all_tf[1], 4)\n",
    "\n",
    "with open(os.path.join(get_git_root(), r\"myLibraries\\ilp_outputs\\ilp_in.ilp\"), \"w\") as f:\n",
    "    f.write(format)\n",
    "    f.close()\n",
    "\n",
    "os.system(f'glpsol --tmlim 100 --lp \"{os.path.join(get_git_root(), r\"myLibraries/ilp_outputs/ilp_in.ilp\")}\" -o \"{os.path.join(get_git_root(), r\"myLibraries/ilp_outputs/ilp_out.sol\")}\"')\n",
    "\n",
    "with open(os.path.join(get_git_root(), r\"myLibraries/ilp_outputs/ilp_out.sol\"), \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "\n",
    "sentences_lines = [line for line in lines if re.search(r\"s\\d\", line)]\n",
    "\n",
    "sorted_lines = sorted(sentences_lines, key=lambda line: int(line.split()[1][1:]))\n",
    "result = [int(sorted_line.split()[3]) for sorted_line in sorted_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[38, 68, 125, 132]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = [i for i, value in enumerate(result) if value == 1]\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[38, 68, 125, 132]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref = []\n",
    "### sentence tfs\n",
    "i = 0\n",
    "sentences_tfs = {}\n",
    "for token in all_tokens[1]:\n",
    "    if i in sentences_tfs.keys():\n",
    "        sentences_tfs[i] += all_tf[1][token]\n",
    "    else:\n",
    "        sentences_tfs[i] = all_tf[1][token]\n",
    "    if token == \".\":\n",
    "        i += 1\n",
    "\n",
    "\n",
    "for i, (k, v) in enumerate(sentences_tfs.items()):\n",
    "    ref.append([i, k, v])\n",
    "ref.sort(key=lambda x: x[2], reverse=True)\n",
    "res = [v[0] for v in ref[:4]]\n",
    "res.sort()\n",
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
