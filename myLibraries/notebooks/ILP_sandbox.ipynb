{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select N-words with higher TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "sys.path.append(r\"C:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\myLibraries\")\n",
    "from MARScore.score import MARSCore\n",
    "from MARScore.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_json(r'C:\\Pro\\Stages\\A4 - DVRC\\Work\\Datasets\\pubmed\\test.json', lines=True)\n",
    "dataset = dataset[[\"article_text\", \"abstract_text\"]]\n",
    "cleaner = lambda x: \". \".join(x).replace(\"<S>\", \"\").strip()\n",
    "format_dot = lambda x: x.replace(\" .\", \".\")\n",
    "dataset.loc[:,\"abstract_text\"] = dataset[\"abstract_text\"].replace(regex=r\"\\[[^\\]]*\\]\", value=\"\")\n",
    "dataset.loc[:,\"article_text\"] = dataset[\"article_text\"].replace(regex=r\"\\[[^\\]]*\\]\", value=\"\")\n",
    "dataset.loc[:,\"abstract_text\"] = dataset[\"abstract_text\"].map(cleaner)\n",
    "dataset.loc[:,\"article_text\"] = dataset[\"article_text\"].map(cleaner)\n",
    "dataset.loc[:,\"abstract_text\"] = dataset[\"abstract_text\"].map(cleanString)\n",
    "dataset.loc[:,\"article_text\"] = dataset[\"article_text\"].map(cleanString)\n",
    "dataset.loc[:,\"abstract_text\"] = dataset[\"abstract_text\"].map(format_dot)\n",
    "dataset.loc[:,\"article_text\"] = dataset[\"article_text\"].map(format_dot)\n",
    "dataset = dataset.rename(columns={\"abstract_text\": \"summary\",\n",
    "                        \"article_text\": \"text\"})\n",
    "\n",
    "subset = dataset.iloc[3:5, :]\n",
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tf = []\n",
    "for indiv in subset[\"text\"].to_list():\n",
    "    o, l = tokenizeCorpus(indiv)\n",
    "    v = vectorizeCorpus(o)\n",
    "    v, l = cleanAll(v, l)\n",
    "    all_tf.append(tf(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_words_format(tfs, n):\n",
    "    output = \"Maximize\\nscore: \"\n",
    "    for i, v in enumerate(tfs.values()):\n",
    "        output += f\"+ {v} c{i}\"\n",
    "    \n",
    "    output += \"\\n\\nSubject To\\n\"\n",
    "    output += \"length:\"\n",
    "    for i, v in enumerate(tfs.values()):\n",
    "        output += f\" c{i} +\"\n",
    "    output = output[:-1]\n",
    "    output += f\"< {n}\"\n",
    "\n",
    "    output += \"\\n\\nBinary\\n\"\n",
    "    for i, v in enumerate(tfs.values()):\n",
    "        output += f\"c{i}\\n\"\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format = best_words_format(all_tf[0], 4)\n",
    "\n",
    "with open(os.path.join(get_git_root(), r\"myLibraries\\ilp_outputs\\ilp_in.ilp\"), \"w\") as f:\n",
    "    f.write(format)\n",
    "    f.close()\n",
    "\n",
    "os.system(f'glpsol --tmlim 100 --lp \"{os.path.join(get_git_root(), r\"myLibraries/ilp_outputs/ilp_in.ilp\")}\" -o \"{os.path.join(get_git_root(), r\"myLibraries/ilp_outputs/ilp_out.sol\")}\"')\n",
    "\n",
    "with open(os.path.join(get_git_root(), r\"myLibraries/ilp_outputs/ilp_out.sol\"), \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "\n",
    "sentences_lines = [line for line in lines if re.search(r\"c\\d\", line)]\n",
    "\n",
    "sorted_lines = sorted(sentences_lines, key=lambda line: int(line.split()[1][1:]))\n",
    "result = [int(sorted_line.split()[3]) for sorted_line in sorted_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = [i for i, value in enumerate(result) if value == 1]\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = []\n",
    "for i, (k, v) in enumerate(all_tf[0].items()):\n",
    "    ref.append([i, k, v])\n",
    "ref.sort(key=lambda x: x[2], reverse=True)\n",
    "res = [v[0] for v in ref[:4]]\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select N-sentences maximizing words tfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "sys.path.append(r\"C:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\myLibraries\")\n",
    "from MARScore.score import MARSCore\n",
    "from MARScore.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_json(r'C:\\Pro\\Stages\\A4 - DVRC\\Work\\Datasets\\pubmed\\test.json', lines=True)\n",
    "dataset = dataset[[\"article_text\", \"abstract_text\"]]\n",
    "cleaner = lambda x: \". \".join(x).replace(\"<S>\", \"\").strip()\n",
    "format_dot = lambda x: x.replace(\" .\", \".\")\n",
    "dataset.loc[:,\"abstract_text\"] = dataset[\"abstract_text\"].replace(regex=r\"\\[[^\\]]*\\]\", value=\"\")\n",
    "dataset.loc[:,\"article_text\"] = dataset[\"article_text\"].replace(regex=r\"\\[[^\\]]*\\]\", value=\"\")\n",
    "dataset.loc[:,\"abstract_text\"] = dataset[\"abstract_text\"].map(cleaner)\n",
    "dataset.loc[:,\"article_text\"] = dataset[\"article_text\"].map(cleaner)\n",
    "dataset.loc[:,\"abstract_text\"] = dataset[\"abstract_text\"].map(cleanString)\n",
    "dataset.loc[:,\"article_text\"] = dataset[\"article_text\"].map(cleanString)\n",
    "dataset.loc[:,\"abstract_text\"] = dataset[\"abstract_text\"].map(format_dot)\n",
    "dataset.loc[:,\"article_text\"] = dataset[\"article_text\"].map(format_dot)\n",
    "dataset = dataset.rename(columns={\"abstract_text\": \"summary\",\n",
    "                        \"article_text\": \"text\"})\n",
    "\n",
    "subset = dataset.iloc[3:5, :]\n",
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = []\n",
    "all_tf = []\n",
    "for indiv in subset[\"text\"].to_list():\n",
    "    o, l = tokenizeCorpus(indiv)\n",
    "    v = vectorizeCorpus(o)\n",
    "    v, l = cleanAll(v, l)\n",
    "    tfs = tf(l)\n",
    "\n",
    "    all_tokens.append(l)\n",
    "    all_tf.append(tfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_sentences_format(tokens, tfs, n):\n",
    "    ### sentence tfs\n",
    "    i = 0\n",
    "    sentences_tfs = {}\n",
    "    for token in tokens:\n",
    "        if i in sentences_tfs.keys():\n",
    "            sentences_tfs[i] += tfs[token]\n",
    "        else:\n",
    "            sentences_tfs[i] = tfs[token]\n",
    "        if token == \".\":\n",
    "            i += 1\n",
    "    \n",
    "    output = \"Maximize\\nscore:\"\n",
    "    for i, cur_tf in enumerate(sentences_tfs.values()):\n",
    "        output += f\"+ {cur_tf} s{i}\"\n",
    "    \n",
    "    output += \"\\n\\nSubject To\\n\"\n",
    "    output += f\"length:\"\n",
    "    for i in range(len(sentences_tfs.keys())):\n",
    "        output += f\" s{i} +\"\n",
    "    output = output[:-1]\n",
    "    output += f\"< {n}\"\n",
    "\n",
    "    output += \"\\n\\nBinary\"\n",
    "    for i in range(len(sentences_tfs.keys())):\n",
    "        output += f\"\\ns{i}\"\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format = best_sentences_format(all_tokens[1], all_tf[1], 4)\n",
    "\n",
    "with open(os.path.join(get_git_root(), r\"myLibraries\\ilp_outputs\\ilp_in.ilp\"), \"w\") as f:\n",
    "    f.write(format)\n",
    "    f.close()\n",
    "\n",
    "os.system(f'glpsol --tmlim 100 --lp \"{os.path.join(get_git_root(), r\"myLibraries/ilp_outputs/ilp_in.ilp\")}\" -o \"{os.path.join(get_git_root(), r\"myLibraries/ilp_outputs/ilp_out.sol\")}\"')\n",
    "\n",
    "with open(os.path.join(get_git_root(), r\"myLibraries/ilp_outputs/ilp_out.sol\"), \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "\n",
    "sentences_lines = [line for line in lines if re.search(r\"s\\d\", line)]\n",
    "\n",
    "sorted_lines = sorted(sentences_lines, key=lambda line: int(line.split()[1][1:]))\n",
    "result = [int(sorted_line.split()[3]) for sorted_line in sorted_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = [i for i, value in enumerate(result) if value == 1]\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = []\n",
    "### sentence tfs\n",
    "i = 0\n",
    "sentences_tfs = {}\n",
    "for token in all_tokens[1]:\n",
    "    if i in sentences_tfs.keys():\n",
    "        sentences_tfs[i] += all_tf[1][token]\n",
    "    else:\n",
    "        sentences_tfs[i] = all_tf[1][token]\n",
    "    if token == \".\":\n",
    "        i += 1\n",
    "\n",
    "\n",
    "for i, (k, v) in enumerate(sentences_tfs.items()):\n",
    "    ref.append([i, k, v])\n",
    "ref.sort(key=lambda x: x[2], reverse=True)\n",
    "res = [v[0] for v in ref[:4]]\n",
    "res.sort()\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convex Hull and Inner Cluster Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "sys.path.append(r\"C:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\myLibraries\")\n",
    "from MARScore.score import MARSCore\n",
    "from MARScore.utils import *\n",
    "from hdbscan import HDBSCAN\n",
    "from umap import UMAP\n",
    "from scipy.spatial import ConvexHull\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_json(r'C:\\Pro\\Stages\\A4 - DVRC\\Work\\Datasets\\pubmed\\test.json', lines=True)\n",
    "dataset = dataset[[\"article_text\", \"abstract_text\"]]\n",
    "cleaner = lambda x: \". \".join(x).replace(\"<S>\", \"\").strip()\n",
    "format_dot = lambda x: x.replace(\" .\", \".\")\n",
    "dataset.loc[:,\"abstract_text\"] = dataset[\"abstract_text\"].replace(regex=r\"\\[[^\\]]*\\]\", value=\"\")\n",
    "dataset.loc[:,\"article_text\"] = dataset[\"article_text\"].replace(regex=r\"\\[[^\\]]*\\]\", value=\"\")\n",
    "dataset.loc[:,\"abstract_text\"] = dataset[\"abstract_text\"].map(cleaner)\n",
    "dataset.loc[:,\"article_text\"] = dataset[\"article_text\"].map(cleaner)\n",
    "dataset.loc[:,\"abstract_text\"] = dataset[\"abstract_text\"].map(cleanString)\n",
    "dataset.loc[:,\"article_text\"] = dataset[\"article_text\"].map(cleanString)\n",
    "dataset.loc[:,\"abstract_text\"] = dataset[\"abstract_text\"].map(format_dot)\n",
    "dataset.loc[:,\"article_text\"] = dataset[\"article_text\"].map(format_dot)\n",
    "dataset = dataset.rename(columns={\"abstract_text\": \"summary\",\n",
    "                        \"article_text\": \"text\"})\n",
    "\n",
    "subset = dataset.iloc[3:5, :]\n",
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embs = []\n",
    "all_reduced_embs = []\n",
    "all_tokens = []\n",
    "all_clabels = []\n",
    "for indiv in subset[\"text\"].to_list():\n",
    "    o, l = tokenizeCorpus(indiv)\n",
    "    v = vectorizeCorpus(o, method=\"concat_l4\")\n",
    "    v, l = cleanAll(v, l)\n",
    "    reduced_v, clabels = clusterizeCorpus(UMAP(n_components=2, init=\"random\", random_state=0), v, \n",
    "                                        HDBSCAN())\n",
    "    tf_values = tf(l)\n",
    "    clusters_tf_values = clusters_tf(tf_values, l, clabels)\n",
    "    all_embs.append(v)\n",
    "    all_reduced_embs.append(reduced_v)\n",
    "    all_tokens.append(l)\n",
    "    all_clabels.append(clabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tidy_tokens(tokens, clabels, embs):\n",
    "    d = {}\n",
    "    for token, clabel, emb in zip(tokens, clabels, embs):\n",
    "        if clabel in d.keys():\n",
    "            d[clabel][\"tokens\"].append(token)\n",
    "            d[clabel][\"embs\"].append(emb)\n",
    "        else:\n",
    "            d[clabel] = {\"tokens\": [token], \"embs\": [emb]}\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_tokens = tidy_tokens(all_tokens[0], all_clabels[0], all_reduced_embs[0])\n",
    "for k, v in d_tokens.items():\n",
    "    d_tokens[k][\"embs\"] = np.array(v[\"embs\"])\n",
    "convex_hulls = {k: ConvexHull(v[\"embs\"]) for k, v in d_tokens.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = d_tokens[0][\"embs\"]\n",
    "hull = ConvexHull(points)\n",
    "\n",
    "# Get the indices of the points forming the convex hull\n",
    "hull_indices = hull.vertices\n",
    "hull_indices = np.append(hull_indices, hull.vertices[0])\n",
    "\n",
    "# Plotting\n",
    "plt.figure()\n",
    "plt.plot(points[:, 0], points[:, 1], 'ko', label='Points')\n",
    "plt.plot(points[hull_indices, 0], points[hull_indices, 1], 'r-', lw=2, label='Convex Hull')\n",
    "for x, y, index in zip(points[hull_indices, 0], points[hull_indices, 1], hull_indices):\n",
    "    plt.text(x, y, str(index))\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Convex Hull')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclideanDistance(p, q):\n",
    "    p = np.array(p)\n",
    "    q = np.array(q)\n",
    "    return np.linalg.norm(p - q)\n",
    "   \n",
    "\n",
    "def crossProduct(p, q, r):\n",
    "    pq = np.array(q) - np.array(p)\n",
    "    pr = np.array(r) - np.array(p)\n",
    "    return np.cross(pq, pr)\n",
    "    \n",
    "\n",
    "def rotatingCaliper(points, convex_hull):\n",
    "   \n",
    "    # Takes O(n)\n",
    "    hull = [points[i] for i in range(len(points)) if i in convex_hull.vertices]\n",
    "    n = len(hull)\n",
    " \n",
    "    # Base Cases\n",
    "    if n == 1:\n",
    "        return 0\n",
    "    if n == 2:\n",
    "        return euclideanDistance(hull[0], hull[1])\n",
    "    k = 1\n",
    " \n",
    "    # Find the farthest vertex\n",
    "    # from hull[0] and hull[n-1]\n",
    "    while crossProduct(hull[n - 1], hull[0], hull[(k + 1) % n]) > crossProduct(hull[n - 1], hull[0], hull[k]):\n",
    "        k += 1\n",
    " \n",
    "    res = 0\n",
    " \n",
    "    # Check points from 0 to k\n",
    "    for i in range(k + 1):\n",
    "        j = (i + 1) % n\n",
    "        while crossProduct(hull[i], hull[(i + 1) % n], hull[(j + 1) % n]) > crossProduct(hull[i], hull[(i + 1) % n], hull[j]):\n",
    "            # Update res\n",
    "            res = max(res, euclideanDistance(hull[i], hull[(j + 1) % n]))\n",
    "            j = (j + 1) % n\n",
    " \n",
    "    # Return the result distance\n",
    "    return res\n",
    "\n",
    "def biggerDistance(points):\n",
    "    points = np.array(points)\n",
    "    convex_hull = ConvexHull(points)\n",
    "    return rotatingCaliper(points, convex_hull)\n",
    "\n",
    "# Code inspired by amit_mangal_ on geeksforgeeks forum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotatingCaliper(d_tokens[0][\"embs\"], convex_hulls[0]) #complexity = O(N*log(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biggerDistance(d_tokens[0][\"embs\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevancy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\umap\\distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\umap\\distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\umap\\distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\umap\\umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "sys.path.append(r\"C:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\myLibraries\")\n",
    "from MARScore.score import MARSCore\n",
    "from MARScore.utils import *\n",
    "from hdbscan import HDBSCAN\n",
    "from umap import UMAP\n",
    "from scipy.spatial import ConvexHull\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>congenital adrenal hyperplasia ( cah ) refers ...</td>\n",
       "      <td>congenital adrenal hyperplasia is a group of a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>type 1 diabetes ( t1d ) results from the destr...</td>\n",
       "      <td>objective(s):pentoxifylline is an immunomodula...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   \n",
       "3  congenital adrenal hyperplasia ( cah ) refers ...  \\\n",
       "4  type 1 diabetes ( t1d ) results from the destr...   \n",
       "\n",
       "                                             summary  \n",
       "3  congenital adrenal hyperplasia is a group of a...  \n",
       "4  objective(s):pentoxifylline is an immunomodula...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_json(r'C:\\Pro\\Stages\\A4 - DVRC\\Work\\Datasets\\pubmed\\test.json', lines=True)\n",
    "dataset = dataset[[\"article_text\", \"abstract_text\"]]\n",
    "cleaner = lambda x: \". \".join(x).replace(\"<S>\", \"\").strip()\n",
    "format_dot = lambda x: x.replace(\" .\", \".\")\n",
    "dataset.loc[:,\"abstract_text\"] = dataset[\"abstract_text\"].replace(regex=r\"\\[[^\\]]*\\]\", value=\"\")\n",
    "dataset.loc[:,\"article_text\"] = dataset[\"article_text\"].replace(regex=r\"\\[[^\\]]*\\]\", value=\"\")\n",
    "dataset.loc[:,\"abstract_text\"] = dataset[\"abstract_text\"].map(cleaner)\n",
    "dataset.loc[:,\"article_text\"] = dataset[\"article_text\"].map(cleaner)\n",
    "dataset.loc[:,\"abstract_text\"] = dataset[\"abstract_text\"].map(cleanString)\n",
    "dataset.loc[:,\"article_text\"] = dataset[\"article_text\"].map(cleanString)\n",
    "dataset.loc[:,\"abstract_text\"] = dataset[\"abstract_text\"].map(format_dot)\n",
    "dataset.loc[:,\"article_text\"] = dataset[\"article_text\"].map(format_dot)\n",
    "dataset = dataset.rename(columns={\"abstract_text\": \"summary\",\n",
    "                        \"article_text\": \"text\"})\n",
    "\n",
    "subset = dataset.iloc[3:5, :]\n",
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embs = []\n",
    "all_reduced_embs = []\n",
    "all_tokens = []\n",
    "all_clabels = []\n",
    "for indiv in subset[\"text\"].to_list():\n",
    "    o, l = tokenizeCorpus(indiv)\n",
    "    v = vectorizeCorpus(o, method=\"concat_l4\")\n",
    "    v, l = cleanAll(v, l)\n",
    "    reduced_v, clabels = clusterizeCorpus(UMAP(n_components=2, init=\"random\", random_state=0), v, \n",
    "                                        HDBSCAN())\n",
    "    tf_values = tf(l)\n",
    "    clusters_tf_values = clusters_tf(tf_values, l, clabels)\n",
    "    all_embs.append(v)\n",
    "    all_reduced_embs.append(reduced_v)\n",
    "    all_tokens.append(l)\n",
    "    all_clabels.append(clabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tidy_tokens(tokens, clabels, embs):\n",
    "    d = {}\n",
    "    for token, clabel, emb in zip(tokens, clabels, embs):\n",
    "        if clabel in d.keys():\n",
    "            d[clabel][\"tokens\"].append(token)\n",
    "            d[clabel][\"embs\"].append(emb)\n",
    "        else:\n",
    "            d[clabel] = {\"tokens\": [token], \"embs\": [emb]}\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_tokens = tidy_tokens(all_tokens[1], all_clabels[1], all_reduced_embs[1])\n",
    "for k, v in d_tokens.items():\n",
    "    d_tokens[k][\"embs\"] = np.array(v[\"embs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclideanDistance(p, q):\n",
    "    p = np.array(p)\n",
    "    q = np.array(q)\n",
    "    return np.linalg.norm(p - q)\n",
    "\n",
    "\n",
    "def crossProduct(p, q, r):\n",
    "    pq = np.array(q) - np.array(p)\n",
    "    pr = np.array(r) - np.array(p)\n",
    "    return np.cross(pq, pr)\n",
    "\n",
    "\n",
    "def rotatingCaliper(points, convex_hull):\n",
    "   \n",
    "    # Takes O(n)\n",
    "    hull = [points[i] for i in range(len(points)) if i in convex_hull.vertices]\n",
    "    n = len(hull)\n",
    " \n",
    "    # Base Cases\n",
    "    if n == 1:\n",
    "        return 0\n",
    "    if n == 2:\n",
    "        return euclideanDistance(hull[0], hull[1])\n",
    "    k = 1\n",
    " \n",
    "    # Find the farthest vertex\n",
    "    # from hull[0] and hull[n-1]\n",
    "    while crossProduct(hull[n - 1], hull[0], hull[(k + 1) % n]) > crossProduct(hull[n - 1], hull[0], hull[k]):\n",
    "        k += 1\n",
    " \n",
    "    res = 0\n",
    " \n",
    "    # Check points from 0 to k\n",
    "    for i in range(k + 1):\n",
    "        j = (i + 1) % n\n",
    "        while crossProduct(hull[i], hull[(i + 1) % n], hull[(j + 1) % n]) > crossProduct(hull[i], hull[(i + 1) % n], hull[j]):\n",
    "            # Update res\n",
    "            res = max(res, euclideanDistance(hull[i], hull[(j + 1) % n]))\n",
    "            j = (j + 1) % n\n",
    " \n",
    "    # Return the result distance\n",
    "    return res\n",
    "\n",
    "def biggerDistance(points):\n",
    "    points = np.array(points)\n",
    "    convex_hull = ConvexHull(points)\n",
    "    return rotatingCaliper(points, convex_hull)\n",
    "\n",
    "# Code inspired by amit_mangal_ on geeksforgeeks forum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_score(tokens_dict, clusters_tfs):\n",
    "    scores = {}\n",
    "    for k, v in tokens_dict.items():\n",
    "        try:\n",
    "            max_dist = biggerDistance(v[\"embs\"])\n",
    "            scores[k] = clusters_tfs[k]/(max_dist*len(v[\"tokens\"])) if max_dist > 0 else 0\n",
    "        except:\n",
    "            scores[k]=0\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_tfs = clusters_tf(tf(all_tokens[1]), all_tokens[1], all_clabels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{82: 0,\n",
       " 121: 252.99409446202395,\n",
       " 54: 520.9817463218787,\n",
       " 78: 73.67253936238083,\n",
       " 153: 107.56429776259358,\n",
       " -1: 4.9214190301694,\n",
       " 46: 23.371296840135972,\n",
       " 154: 25.63985403494408,\n",
       " 79: 6.235203052542828,\n",
       " 147: 28.263387679234608,\n",
       " 134: 60.29300651702029,\n",
       " 88: 246.75050328308333,\n",
       " 115: 10.302458752929974,\n",
       " 21: 264.71790481034617,\n",
       " 25: 259.79532733425475,\n",
       " 26: 41.63589719090981,\n",
       " 19: 269.7715746600524,\n",
       " 103: 25.486783497062596,\n",
       " 44: 21.29757521718107,\n",
       " 149: 26.792687342000928,\n",
       " 126: 0,\n",
       " 47: 77.87142776464563,\n",
       " 72: 0,\n",
       " 158: 100.63731608978482,\n",
       " 151: 414.5765366450074,\n",
       " 162: 0,\n",
       " 136: 83.0082353588312,\n",
       " 148: 24.07443686464583,\n",
       " 150: 18.501816577753885,\n",
       " 159: 57.03227824153284,\n",
       " 138: 0,\n",
       " 118: 0,\n",
       " 39: 575.9894945153402,\n",
       " 135: 25.08894480474084,\n",
       " 12: 411.24804673551637,\n",
       " 102: 6.8783713020184996,\n",
       " 111: 166.89598815755517,\n",
       " 146: 32.47671298354557,\n",
       " 140: 0,\n",
       " 164: 17.572105333109892,\n",
       " 163: 18.738344948641938,\n",
       " 67: 4.246814870526829,\n",
       " 131: 0,\n",
       " 45: 328.1416781283377,\n",
       " 119: 172.50612704657686,\n",
       " 27: 4.67882455587144,\n",
       " 87: 18.361156463267488,\n",
       " 59: 11.465054404742252,\n",
       " 98: 122.09895963143798,\n",
       " 73: 32.95155043327284,\n",
       " 55: 1500.079699202968,\n",
       " 63: 76.67676028381887,\n",
       " 7: 0,\n",
       " 8: 199.90328291809203,\n",
       " 1: 4094.1183313382326,\n",
       " 16: 157.17823596250804,\n",
       " 10: 487.15022415624605,\n",
       " 56: 0,\n",
       " 145: 13.983022893480596,\n",
       " 160: 0,\n",
       " 165: 11.92543626572798,\n",
       " 110: 0,\n",
       " 51: 6.779732674659229,\n",
       " 107: 24.40646544978532,\n",
       " 117: 65.752952933813,\n",
       " 41: 79.86236700732559,\n",
       " 18: 104.18727171812857,\n",
       " 30: 79.82141103380545,\n",
       " 42: 20.930681219258375,\n",
       " 58: 20.119626689456762,\n",
       " 124: 12.361924782482982,\n",
       " 40: 192.38013038490294,\n",
       " 71: 0,\n",
       " 81: 0,\n",
       " 65: 38.07467124864029,\n",
       " 38: 347.1328905139376,\n",
       " 24: 308.73247402132426,\n",
       " 61: 47.344922517715894,\n",
       " 62: 53.70733984361813,\n",
       " 53: 43.346933233277724,\n",
       " 85: 38.351987552738656,\n",
       " 92: 0,\n",
       " 143: 5.107518575505161,\n",
       " 9: 259.0917961683078,\n",
       " 22: 49.57772743138728,\n",
       " 128: 84.49605067234191,\n",
       " 28: 61.165926624699146,\n",
       " 66: 13.35793358157426,\n",
       " 2: 2550.1482505742565,\n",
       " 122: 0,\n",
       " 127: 7.413851760594047,\n",
       " 70: 0,\n",
       " 29: 0,\n",
       " 116: 17.177658350659637,\n",
       " 129: 657.9253178999827,\n",
       " 133: 0,\n",
       " 99: 14.081144072733679,\n",
       " 6: 771.3637844288507,\n",
       " 108: 19.8327214049633,\n",
       " 14: 54.46750990340694,\n",
       " 13: 218.7963668750241,\n",
       " 74: 0,\n",
       " 86: 104.9245616038217,\n",
       " 120: 12.111496452946371,\n",
       " 20: 12.01013633770723,\n",
       " 97: 16.842959814072444,\n",
       " 3: 164.85430873420125,\n",
       " 11: 310.55168938320367,\n",
       " 161: 575.1475119862314,\n",
       " 48: 9100.362803914079,\n",
       " 139: 30.171853441888505,\n",
       " 5: 465.79177091902136,\n",
       " 37: 31.38411261444149,\n",
       " 36: 24.071981746903056,\n",
       " 113: 26.053650250107726,\n",
       " 23: 0,\n",
       " 69: 0,\n",
       " 142: 16.31006708034513,\n",
       " 123: 0,\n",
       " 89: 0,\n",
       " 156: 30.81972463618527,\n",
       " 34: 0,\n",
       " 32: 521.6905960090183,\n",
       " 4: 320.850615148896,\n",
       " 137: 109.93462901144332,\n",
       " 114: 48.86936896786961,\n",
       " 17: 0,\n",
       " 33: 0,\n",
       " 90: 0,\n",
       " 49: 0,\n",
       " 152: 12.949051188319975,\n",
       " 35: 71.49723867471387,\n",
       " 43: 154.44992116461185,\n",
       " 125: 0,\n",
       " 15: 205.95090598687386,\n",
       " 94: 258.20143815546743,\n",
       " 157: 16.026321863437985,\n",
       " 141: 0,\n",
       " 95: 33.76337187889839,\n",
       " 50: 0,\n",
       " 93: 29.28648631664329,\n",
       " 144: 44.32424616665952,\n",
       " 91: 0,\n",
       " 106: 0,\n",
       " 57: 29.937058929175986,\n",
       " 80: 0,\n",
       " 52: 962.1992107957002,\n",
       " 104: 46.93804177055933,\n",
       " 132: 35.28290091558635,\n",
       " 100: 0,\n",
       " 105: 160.39076402078024,\n",
       " 130: 422.6569720391234,\n",
       " 75: 0,\n",
       " 109: 5.120569219954516,\n",
       " 64: 0,\n",
       " 77: 16.762041637993406,\n",
       " 68: 78.31169222925631,\n",
       " 0: 0,\n",
       " 60: 20.2586453402193,\n",
       " 31: 107.4084767331934,\n",
       " 96: 11.432356469277352,\n",
       " 76: 7.677737857643589,\n",
       " 84: 27.46939879277724,\n",
       " 83: 32.06997053976536,\n",
       " 101: 19.783791815131963,\n",
       " 155: 0,\n",
       " 112: 6.226802510799359}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_score(d_tokens, c_tfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52, 2)\n"
     ]
    }
   ],
   "source": [
    "for v in list(d_tokens.values())[:1]:\n",
    "    print(v[\"embs\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redundancy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
