{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with BERTScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"D:\\COURS\\A4\\S8 - ESILV\\Stage\\Work\\Repositories\\bert_score\")\n",
    "\n",
    "from bert_score.score import score as bscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = bscore([\"I am Marius\"], [\"My name is marius\"], lang=\"en\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with Roberta tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "test = tokenizer(\"I am Marius\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = tokenizer(\"I am Marius\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.input_ids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with AutoModel class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "from transformers import RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(\"roberta-large\")\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I am Marius\"\n",
    "tokens = tokenizer(sentence)\n",
    "token_ids = tokens[\"input_ids\"]\n",
    "masks = tokens[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(token_ids, attention_mask=masks, output_hidden_states=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Embedding - Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "text = \"My name is Marius.\"\n",
    "marked_text = \"[CLS]\"+text+\"[SEP]\"\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "    print('{:<12}{:>6,}'.format(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mark tokenks as belonging to sentence 1.\n",
    "segment_ids = [1]*len(tokenized_text)\n",
    "print(segment_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensor = torch.tensor([segment_ids])\n",
    "print(tokens_tensor)\n",
    "print(segments_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, segments_tensor)\n",
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_hidden_states = outputs[-1]\n",
    "len(bert_hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings = torch.stack(bert_hidden_states, dim=0)\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we have 1 sentence so we remove the batch size\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#swap dim 0 and 1\n",
    "token_embeddings = token_embeddings.permute(1, 0, 2)\n",
    "token_embeddings.size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate layers\n",
    "token_vec_cat = []\n",
    "for token in token_embeddings:\n",
    "    cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "    token_vec_cat.append(cat_vec)\n",
    "print(\"Shape:\", len(token_vec_cat),\"x\", len(token_vec_cat[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(token_vec_cat)):\n",
    "    if len(token_vec_cat[i] != 3072):\n",
    "        print(i)\n",
    "        print(len(token_vec_cat[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [token.tolist() for token in token_vec_cat]\n",
    "test = np.array(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum layers\n",
    "token_vec_sum = []\n",
    "for token in token_embeddings:\n",
    "    cat_vec = torch.sum(token[-4:], dim=0)\n",
    "    token_vec_sum.append(cat_vec)\n",
    "print(\"Shape:\", len(token_vec_sum),\"x\", len(token_vec_sum[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_vecs = bert_hidden_states[-2][0]\n",
    "sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "sentence_embedding.size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap3D = UMAP(n_components=3, init='random', random_state=0)\n",
    "proj3D = umap3D.fit_transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = px.scatter_3d(proj3D, x=0, y=1, z=2)\n",
    "f.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Sentence-level Embedding - Paragraphs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states=True)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"Wikipedia is a multilingual, free, online encyclopedia written and maintained by a community of volunteers, known as Wikipedians, through open collaboration and using a wiki-based editing system called MediaWiki. Wikipedia is the largest and most-read reference work in history. It is consistently one of the 10 most popular websites ranked by Similarweb and formerly Alexa; as of 2023, Wikipedia was ranked the 5th most popular site in the world according to Semrush. It is hosted by the Wikimedia Foundation, an American non-profit organization funded mainly through donations.\"\n",
    "delimiter=\".\"\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splited = [sentence+delimiter for sentence in corpus.split(\".\")]\n",
    "max_len = max(len(x) for x in splited)\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "for sentence in splited:\n",
    "    encoded = tokenizer.encode_plus(sentence, \n",
    "                                    add_special_tokens=True,\n",
    "                                    max_length=max_len+1,\n",
    "                                    pad_to_max_length=True,\n",
    "                                    return_attention_mask=True,\n",
    "                                    return_tensors='pt',\n",
    "                                    truncation=True)\n",
    "    input_ids.append(encoded[\"input_ids\"])\n",
    "    attention_masks.append(encoded[\"attention_mask\"])\n",
    "\n",
    "inputs_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(inputs_ids, attention_mask=attention_masks)\n",
    "hidden_state = output.last_hidden_state\n",
    "cls_emb = hidden_state[:,0,:]\n",
    "cls_emb = cls_emb.detach().numpy()\n",
    "np.shape(cls_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(splited)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Word-level Embedding - Paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from umap import UMAP\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states=True)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "corpus = \"Wikipedia is a multilingual, free, online encyclopedia written and maintained by a community of volunteers, known as Wikipedians, through open collaboration and using a wiki-based editing system called MediaWiki. Wikipedia is the largest and most-read reference work in history. It is consistently one of the 10 most popular websites ranked by Similarweb and formerly Alexa; as of 2023, Wikipedia was ranked the 5th most popular site in the world according to Semrush. It is hosted by the Wikimedia Foundation, an American non-profit organization funded mainly through donations.\"\n",
    "delimiter=\".\"\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(splited[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 512 - 1\n",
    "corpusWords = corpus.split(\" \")\n",
    "splited = [\" \".join(corpusWords[i:i+input_size]) for i in range(0, len(corpusWords), input_size)]\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "for sentence in splited:\n",
    "    encoded = tokenizer.encode_plus(sentence, \n",
    "                                    add_special_tokens=True,\n",
    "                                    max_length=input_size+1,\n",
    "                                    pad_to_max_length=True,\n",
    "                                    return_attention_mask=True,\n",
    "                                    return_tensors='pt',\n",
    "                                    truncation=True)\n",
    "    input_ids.append(encoded[\"input_ids\"])\n",
    "    attention_masks.append(encoded[\"attention_mask\"])\n",
    "\n",
    "#inputs_ids = torch.Tensor(len(input_ids),1, max_len+1)\n",
    "#torch.cat(input_ids, out=inputs_ids)\n",
    "inputs_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(inputs_ids, attention_mask=attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "splited = [sentence+delimiter for sentence in corpus.split(\".\")]\n",
    "max_len = max(len(x) for x in splited)\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "for sentence in splited:\n",
    "    encoded = tokenizer.encode_plus(sentence, \n",
    "                                    add_special_tokens=True,\n",
    "                                    max_length=max_len+1,\n",
    "                                    pad_to_max_length=True,\n",
    "                                    return_attention_mask=True,\n",
    "                                    return_tensors='pt',\n",
    "                                    truncation=True)\n",
    "    input_ids.append(encoded[\"input_ids\"])\n",
    "    attention_masks.append(encoded[\"attention_mask\"])\n",
    "\n",
    "#inputs_ids = torch.Tensor(len(input_ids),1, max_len+1)\n",
    "#torch.cat(input_ids, out=inputs_ids)\n",
    "inputs_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(inputs_ids, attention_mask=attention_masks)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = output.hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "print(token_embeddings.size())\n",
    "#token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "#print(token_embeddings.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings = token_embeddings.permute(1,2,0,3)\n",
    "print(token_embeddings.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = []\n",
    "for batch in token_embeddings:\n",
    "    for token in batch:\n",
    "        emb = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "        embs.append(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [token.tolist() for token in embs]\n",
    "test = np.array(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap3D = UMAP(n_components=3, init='random', random_state=0)\n",
    "proj3D = umap3D.fit_transform(test).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "temp = flatten([batch.tolist() for batch in input_ids])\n",
    "labels = np.array(temp)\n",
    "labels = labels.reshape((labels.shape[0]*labels.shape[1]))\n",
    "labels = tokenizer.convert_ids_to_tokens(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"x\": proj3D[0],\n",
    "        \"y\": proj3D[1],\n",
    "        \"z\": proj3D[2], \n",
    "        \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_indexes = [i for i in range(len(labels)) if labels[i] != \"[PAD]\" and labels[i] != \"[CLS]\" and labels[i] != \"[SEP]\"]\n",
    "for k in data.keys():\n",
    "    data[k] = [data[k][i] for i in range(len(data[k])) if i in token_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = []\n",
    "for i in range(len(data['x'])):\n",
    "    trace = go.Scatter3d(\n",
    "        x=[data['x'][i]],\n",
    "        y=[data['y'][i]],\n",
    "        z=[data['z'][i]],\n",
    "        mode='markers',\n",
    "        marker=dict(size=6),\n",
    "        text=[data['labels'][i]],\n",
    "        name=data['labels'][i]\n",
    "    )\n",
    "    traces.append(trace)\n",
    "\n",
    "# Create layout\n",
    "layout = go.Layout(\n",
    "    title='3D Scatter Plot',\n",
    "    scene=dict(\n",
    "        xaxis=dict(title='X'),\n",
    "        yaxis=dict(title='Y'),\n",
    "        zaxis=dict(title='Z')\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create figure\n",
    "fig = go.Figure(data=traces, layout=layout)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Sentence Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "#Our sentences we like to encode\n",
    "sentences = ['This framework generates embeddings for each input sentence',\n",
    "    'Sentences are passed as a list of string.',\n",
    "    'The quick brown fox jumps over the lazy dog.']\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "#Print the embeddings\n",
    "for sentence, embedding in zip(sentences, embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from umap import UMAP\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states=True)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "corpus = \"Wikipedia is a multilingual, free, online encyclopedia written and maintained by a community of volunteers, known as Wikipedians, through open collaboration and using a wiki-based editing system called MediaWiki. Wikipedia is the largest and most-read reference work in history. It is consistently one of the 10 most popular websites ranked by Similarweb and formerly Alexa; as of 2023, Wikipedia was ranked the 5th most popular site in the world according to Semrush. It is hosted by the Wikimedia Foundation, an American non-profit organization funded mainly through donations.\"\n",
    "delimiter=\".\"\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeCorpus(corpus, model=BertModel.from_pretrained('bert-base-uncased', \n",
    "                                                           output_hidden_states=True), \n",
    "                           tokenizer = BertTokenizer.from_pretrained('bert-base-uncased'), \n",
    "                           model_input_size=512):\n",
    "    def flatten(l):\n",
    "        return [item for sublist in l for item in sublist]\n",
    "    input_size = model_input_size - 1\n",
    "    corpusWords = corpus.split(\" \")\n",
    "    splited = [\" \".join(corpusWords[i:i+input_size]) for i in range(0, len(corpusWords), input_size)]\n",
    "\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for sentence in splited:\n",
    "        encoded = tokenizer.encode_plus(sentence, \n",
    "                                        add_special_tokens=True,\n",
    "                                        max_length=input_size+1,\n",
    "                                        pad_to_max_length=True,\n",
    "                                        return_attention_mask=True,\n",
    "                                        return_tensors='pt',\n",
    "                                        truncation=True)\n",
    "        input_ids.append(encoded[\"input_ids\"])\n",
    "        attention_masks.append(encoded[\"attention_mask\"])\n",
    "\n",
    "    #inputs_ids = torch.Tensor(len(input_ids),1, max_len+1)\n",
    "    #torch.cat(input_ids, out=inputs_ids)\n",
    "    inputs_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    \n",
    "    temp = flatten([batch.tolist() for batch in input_ids])\n",
    "    labels = np.array(temp)\n",
    "    labels = labels.reshape((labels.shape[0]*labels.shape[1]))\n",
    "    labels = tokenizer.convert_ids_to_tokens(labels)\n",
    "    with torch.no_grad():\n",
    "        output = model(inputs_ids, attention_mask=attention_masks)\n",
    "    return output, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizeCorpus(model_output, allStates=True):\n",
    "    if allStates==True:\n",
    "        hidden_states = model_output.hidden_states\n",
    "    else:\n",
    "        hidden_states = [model_output.last_hidden_state]\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "    token_embeddings = token_embeddings.permute(1,2,0,3)\n",
    "    embs = []\n",
    "    for batch in token_embeddings:\n",
    "        for token in batch:\n",
    "            emb = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "            embs.append(emb)\n",
    "    return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeCorpus(embs, labels):\n",
    "    \n",
    "    formated_embs = [token.tolist() for token in embs]\n",
    "    formated_embs = np.array(formated_embs)\n",
    "    umap3D = UMAP(n_components=3, init='random', random_state=0)\n",
    "    proj3D = umap3D.fit_transform(formated_embs).T\n",
    "\n",
    "    data = {\"x\": proj3D[0],\n",
    "            \"y\": proj3D[1],\n",
    "            \"z\": proj3D[2], \n",
    "            \"labels\": labels}\n",
    "    \n",
    "    token_indexes = [i for i in range(len(labels)) if labels[i] != \"[PAD]\" and labels[i] != \"[CLS]\" and labels[i] != \"[SEP]\"]\n",
    "    for k in data.keys():\n",
    "        data[k] = [data[k][i] for i in range(len(data[k])) if i in token_indexes]\n",
    "\n",
    "    traces = []\n",
    "    for i in range(len(data['x'])):\n",
    "        trace = go.Scatter3d(\n",
    "            x=[data['x'][i]],\n",
    "            y=[data['y'][i]],\n",
    "            z=[data['z'][i]],\n",
    "            mode='markers',\n",
    "            marker=dict(size=6),\n",
    "            text=[data['labels'][i]],\n",
    "            name=data['labels'][i]\n",
    "        )\n",
    "        traces.append(trace)\n",
    "\n",
    "    # Create layout\n",
    "    layout = go.Layout(\n",
    "        title='3D Scatter Plot',\n",
    "        scene=dict(\n",
    "            xaxis=dict(title='X'),\n",
    "            yaxis=dict(title='Y'),\n",
    "            zaxis=dict(title='Z')\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Create figure\n",
    "    fig = go.Figure(data=traces, layout=layout)\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o, l = tokenizeCorpus(corpus)\n",
    "v = vectorizeCorpus(o)\n",
    "visualizeCorpus(v, l)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on billsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url=\"https://drive.google.com/file/d/1Wd0M3qepNF6B4YwFYrpo7CaSERpudAG_/view?usp=share_link\"\n",
    "dataset_url='https://drive.google.com/uc?export=download&id=' + dataset_url.split('/')[-2]\n",
    "dataset = pd.read_json(dataset_url, lines=True)\n",
    "dataset = dataset.loc[:, [\"text\", \"summary\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elem0 = dataset.iloc[0, 0]\n",
    "print(elem0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o, l = tokenizeCorpus(elem0)\n",
    "v = vectorizeCorpus(o)\n",
    "visualizeCorpus(v, l)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDBScan (DBScan temporary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
