{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with BERTScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"D:\\COURS\\A4\\S8 - ESILV\\Stage\\Work\\Repositories\\bert_score\")\n",
    "\n",
    "from bert_score.score import score as bscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "o = bscore([\"I am Marius\"], [\"My name is marius\"], lang=\"en\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with Roberta tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\COURS\\A4\\S8 - ESILV\\Stage\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "test = tokenizer(\"I am Marius\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = tokenizer(\"I am Marius\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 100, 524, 1127, 6125, 2], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 100, 524, 1127, 6125, 2]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.input_ids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with AutoModel class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "from transformers import RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"roberta-large\")\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I am Marius\"\n",
    "tokens = tokenizer(sentence)\n",
    "token_ids = tokens[\"input_ids\"]\n",
    "masks = tokens[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(token_ids, attention_mask=masks, output_hidden_states=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Embedding - Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'my', 'name', 'is', 'marius', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "text = \"My name is Marius.\"\n",
    "marked_text = \"[CLS]\"+text+\"[SEP]\"\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]          101\n",
      "my           2,026\n",
      "name         2,171\n",
      "is           2,003\n",
      "marius      20,032\n",
      ".            1,012\n",
      "[SEP]          102\n"
     ]
    }
   ],
   "source": [
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "    print('{:<12}{:>6,}'.format(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "#mark tokenks as belonging to sentence 1.\n",
    "segment_ids = [1]*len(tokenized_text)\n",
    "print(segment_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  2026,  2171,  2003, 20032,  1012,   102]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensor = torch.tensor([segment_ids])\n",
    "print(tokens_tensor)\n",
    "print(segments_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output', 'hidden_states'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, segments_tensor)\n",
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_hidden_states = outputs[-1]\n",
    "len(bert_hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 1, 7, 768])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings = torch.stack(bert_hidden_states, dim=0)\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 7, 768])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#here we have 1 sentence so we remove the batch size\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 13, 768])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#swap dim 0 and 1\n",
    "token_embeddings = token_embeddings.permute(1, 0, 2)\n",
    "token_embeddings.size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: 7 x 3072\n"
     ]
    }
   ],
   "source": [
    "#concatenate layers\n",
    "token_vec_cat = []\n",
    "for token in token_embeddings:\n",
    "    cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "    token_vec_cat.append(cat_vec)\n",
    "print(\"Shape:\", len(token_vec_cat),\"x\", len(token_vec_cat[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "3072\n",
      "1\n",
      "3072\n",
      "2\n",
      "3072\n",
      "3\n",
      "3072\n",
      "4\n",
      "3072\n",
      "5\n",
      "3072\n",
      "6\n",
      "3072\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(token_vec_cat)):\n",
    "    if len(token_vec_cat[i] != 3072):\n",
    "        print(i)\n",
    "        print(len(token_vec_cat[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [token.tolist() for token in token_vec_cat]\n",
    "test = np.array(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 3072)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: 7 x 768\n"
     ]
    }
   ],
   "source": [
    "#sum layers\n",
    "token_vec_sum = []\n",
    "for token in token_embeddings:\n",
    "    cat_vec = torch.sum(token[-4:], dim=0)\n",
    "    token_vec_sum.append(cat_vec)\n",
    "print(\"Shape:\", len(token_vec_sum),\"x\", len(token_vec_sum[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_vecs = bert_hidden_states[-2][0]\n",
    "sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "sentence_embedding.size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\COURS\\A4\\S8 - ESILV\\Stage\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\umap\\umap_.py:2344: UserWarning: n_neighbors is larger than the dataset size; truncating to X.shape[0] - 1\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "umap3D = UMAP(n_components=3, init='random', random_state=0)\n",
    "proj3D = umap3D.fit_transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "0=%{x}<br>1=%{y}<br>2=%{z}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#636efa",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "",
         "scene": "scene",
         "showlegend": false,
         "type": "scatter3d",
         "x": [
          -14.382111549377441,
          -13.482431411743164,
          -13.616006851196289,
          -13.816569328308105,
          -14.84157657623291,
          -14.942224502563477,
          -14.374594688415527
         ],
         "y": [
          24.415237426757812,
          24.406505584716797,
          23.54535675048828,
          24.07982635498047,
          24.99071502685547,
          24.25547981262207,
          25.099777221679688
         ],
         "z": [
          15.910723686218262,
          16.352338790893555,
          16.82695198059082,
          17.403045654296875,
          17.326881408691406,
          16.687265396118164,
          16.677001953125
         ]
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "scene": {
         "domain": {
          "x": [
           0,
           1
          ],
          "y": [
           0,
           1
          ]
         },
         "xaxis": {
          "title": {
           "text": "0"
          }
         },
         "yaxis": {
          "title": {
           "text": "1"
          }
         },
         "zaxis": {
          "title": {
           "text": "2"
          }
         }
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = px.scatter_3d(proj3D, x=0, y=1, z=2)\n",
    "f.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Sentence-level Embedding - Paragraphs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 570/570 [00:00<?, ?B/s] \n",
      "Downloading pytorch_model.bin: 100%|██████████| 440M/440M [00:11<00:00, 40.0MB/s] \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 3.36MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 23.2kB/s]\n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states=True)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia is a multilingual, free, online encyclopedia written and maintained by a community of volunteers, known as Wikipedians, through open collaboration and using a wiki-based editing system called MediaWiki. Wikipedia is the largest and most-read reference work in history. It is consistently one of the 10 most popular websites ranked by Similarweb and formerly Alexa; as of 2023, Wikipedia was ranked the 5th most popular site in the world according to Semrush. It is hosted by the Wikimedia Foundation, an American non-profit organization funded mainly through donations.\n"
     ]
    }
   ],
   "source": [
    "corpus = \"Wikipedia is a multilingual, free, online encyclopedia written and maintained by a community of volunteers, known as Wikipedians, through open collaboration and using a wiki-based editing system called MediaWiki. Wikipedia is the largest and most-read reference work in history. It is consistently one of the 10 most popular websites ranked by Similarweb and formerly Alexa; as of 2023, Wikipedia was ranked the 5th most popular site in the world according to Semrush. It is hosted by the Wikimedia Foundation, an American non-profit organization funded mainly through donations.\"\n",
    "delimiter=\".\"\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5, 768)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splited = [sentence+delimiter for sentence in corpus.split(\".\")]\n",
    "max_len = max(len(x) for x in splited)\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "for sentence in splited:\n",
    "    encoded = tokenizer.encode_plus(sentence, \n",
    "                                    add_special_tokens=True,\n",
    "                                    max_length=max_len+1,\n",
    "                                    pad_to_max_length=True,\n",
    "                                    return_attention_mask=True,\n",
    "                                    return_tensors='pt',\n",
    "                                    truncation=True)\n",
    "    input_ids.append(encoded[\"input_ids\"])\n",
    "    attention_masks.append(encoded[\"attention_mask\"])\n",
    "\n",
    "inputs_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(inputs_ids, attention_mask=attention_masks)\n",
    "hidden_state = output.last_hidden_state\n",
    "cls_emb = hidden_state[:,0,:]\n",
    "cls_emb = cls_emb.detach().numpy()\n",
    "np.shape(cls_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.10181614, -0.04256969, -0.37916115, ..., -0.01706432,\n",
       "         0.38435265,  0.77578974],\n",
       "       [-0.15074998,  0.16900486, -0.4508034 , ..., -0.41086987,\n",
       "         0.6468059 ,  0.44568998],\n",
       "       [-0.00590458,  0.07766522, -0.09788167, ..., -0.05835519,\n",
       "         0.6742643 ,  0.6657301 ],\n",
       "       [-0.46835747,  0.30540103, -0.3483228 , ..., -0.31408456,\n",
       "         0.18837832,  0.8075387 ],\n",
       "       [-0.1548306 ,  0.25915366,  0.14536104, ..., -0.2707459 ,\n",
       "         0.49331143,  0.22958532]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splited)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Word-level Embedding - Paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia is a multilingual, free, online encyclopedia written and maintained by a community of volunteers, known as Wikipedians, through open collaboration and using a wiki-based editing system called MediaWiki. Wikipedia is the largest and most-read reference work in history. It is consistently one of the 10 most popular websites ranked by Similarweb and formerly Alexa; as of 2023, Wikipedia was ranked the 5th most popular site in the world according to Semrush. It is hosted by the Wikimedia Foundation, an American non-profit organization funded mainly through donations.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states=True)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "corpus = \"Wikipedia is a multilingual, free, online encyclopedia written and maintained by a community of volunteers, known as Wikipedians, through open collaboration and using a wiki-based editing system called MediaWiki. Wikipedia is the largest and most-read reference work in history. It is consistently one of the 10 most popular websites ranked by Similarweb and formerly Alexa; as of 2023, Wikipedia was ranked the 5th most popular site in the world according to Semrush. It is hosted by the Wikimedia Foundation, an American non-profit organization funded mainly through donations.\"\n",
    "delimiter=\".\"\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splited = [sentence+delimiter for sentence in corpus.split(\".\")]\n",
    "max_len = max(len(x) for x in splited)\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "for sentence in splited:\n",
    "    encoded = tokenizer.encode_plus(sentence, \n",
    "                                    add_special_tokens=True,\n",
    "                                    max_length=max_len+1,\n",
    "                                    pad_to_max_length=True,\n",
    "                                    return_attention_mask=True,\n",
    "                                    return_tensors='pt',\n",
    "                                    truncation=True)\n",
    "    input_ids.append(encoded[\"input_ids\"])\n",
    "    attention_masks.append(encoded[\"attention_mask\"])\n",
    "\n",
    "\n",
    "inputs_ids = torch.Tensor(len(input_ids),1, max_len+1)\n",
    "torch.cat(input_ids, out=inputs_ids)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(inputs_ids, attention_mask=attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 213])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = output.hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 5, 213, 768])\n",
      "torch.Size([13, 5, 213, 768])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "print(token_embeddings.size())\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "print(token_embeddings.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 213, 13, 768])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embeddings.permute(1,2,0,3)\n",
    "print(token_embeddings.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in token_embeddings:\n",
    "    for token in batch:\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Sentence Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)e9125/.gitattributes: 100%|██████████| 1.18k/1.18k [00:00<?, ?B/s]\n",
      "Downloading (…)_Pooling/config.json: 100%|██████████| 190/190 [00:00<?, ?B/s] \n",
      "Downloading (…)7e55de9125/README.md: 100%|██████████| 10.6k/10.6k [00:00<?, ?B/s]\n",
      "Downloading (…)55de9125/config.json: 100%|██████████| 612/612 [00:00<?, ?B/s] \n",
      "Downloading (…)ce_transformers.json: 100%|██████████| 116/116 [00:00<?, ?B/s] \n",
      "Downloading (…)125/data_config.json: 100%|██████████| 39.3k/39.3k [00:00<00:00, 485kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 90.9M/90.9M [00:02<00:00, 44.5MB/s]\n",
      "Downloading (…)nce_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<?, ?B/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<?, ?B/s] \n",
      "Downloading (…)e9125/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 2.92MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 350/350 [00:00<?, ?B/s] \n",
      "Downloading (…)9125/train_script.py: 100%|██████████| 13.2k/13.2k [00:00<?, ?B/s]\n",
      "Downloading (…)7e55de9125/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 2.72MB/s]\n",
      "Downloading (…)5de9125/modules.json: 100%|██████████| 349/349 [00:00<?, ?B/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: This framework generates embeddings for each input sentence\n",
      "Embedding: [-1.37173552e-02 -4.28515412e-02 -1.56286098e-02  1.40537675e-02\n",
      "  3.95537615e-02  1.21796317e-01  2.94333659e-02 -3.17524336e-02\n",
      "  3.54959927e-02 -7.93139786e-02  1.75878350e-02 -4.04370092e-02\n",
      "  4.97259349e-02  2.54912134e-02 -7.18700662e-02  8.14968720e-02\n",
      "  1.47072482e-03  4.79627065e-02 -4.50336114e-02 -9.92174670e-02\n",
      " -2.81769820e-02  6.45046234e-02  4.44670580e-02 -4.76217046e-02\n",
      " -3.52952480e-02  4.38671671e-02 -5.28566055e-02  4.33045556e-04\n",
      "  1.01921499e-01  1.64072420e-02  3.26996632e-02 -3.45986634e-02\n",
      "  1.21339280e-02  7.94871151e-02  4.58342070e-03  1.57778412e-02\n",
      " -9.68210399e-03  2.87625976e-02 -5.05806208e-02 -1.55793503e-02\n",
      " -2.87907012e-02 -9.62282438e-03  3.15556787e-02  2.27348972e-02\n",
      "  8.71449485e-02 -3.85027118e-02 -8.84718373e-02 -8.75501521e-03\n",
      " -2.12343540e-02  2.08923593e-02 -9.02077407e-02 -5.25732078e-02\n",
      " -1.05638644e-02  2.88310759e-02 -1.61455162e-02  6.17835438e-03\n",
      " -1.23234289e-02 -1.07337097e-02  2.83353999e-02 -5.28567694e-02\n",
      " -3.58618274e-02 -5.97989149e-02 -1.09055219e-02  2.91566122e-02\n",
      "  7.97979608e-02 -3.27874062e-04  6.83498336e-03  1.32718282e-02\n",
      " -4.24620323e-02  1.87656973e-02 -9.89234224e-02  2.09049918e-02\n",
      " -8.69605839e-02 -1.50151886e-02 -4.86201644e-02  8.04414749e-02\n",
      " -3.67701286e-03 -6.65044412e-02  1.14556789e-01 -3.04228850e-02\n",
      "  2.96632033e-02 -2.80695353e-02  4.64990139e-02 -2.25513875e-02\n",
      "  8.54223296e-02  3.15446779e-02  7.34541863e-02 -2.21861824e-02\n",
      " -5.29678613e-02  1.27130309e-02 -5.27339913e-02 -1.06188744e-01\n",
      "  7.04731569e-02  2.76736598e-02 -8.05531368e-02  2.39649490e-02\n",
      " -2.65125111e-02 -2.17330754e-02  4.35275510e-02  4.84711640e-02\n",
      " -2.37067267e-02  2.85768267e-02  1.11846142e-01 -6.34935945e-02\n",
      " -1.58318207e-02 -2.26169750e-02 -1.31027866e-02 -1.62069069e-03\n",
      " -3.60928886e-02 -9.78297666e-02 -4.67729047e-02  1.76271796e-02\n",
      " -3.97492200e-02 -1.76388785e-04  3.39627489e-02 -2.09633652e-02\n",
      "  6.33662753e-03 -2.59411260e-02  8.10410231e-02  6.14393316e-02\n",
      " -5.44595858e-03  6.48275912e-02 -1.16844073e-01  2.36860942e-02\n",
      " -1.32058710e-02 -1.12476520e-01  1.90049503e-02 -1.74661689e-34\n",
      "  5.58949746e-02  1.94244701e-02  4.65438776e-02  5.18645346e-02\n",
      "  3.89390551e-02  3.40541191e-02 -4.32114340e-02  7.90637210e-02\n",
      " -9.79529992e-02 -1.27441091e-02 -2.91870870e-02  1.02052018e-02\n",
      "  1.88115928e-02  1.08942568e-01  6.63464963e-02 -5.35294972e-02\n",
      " -3.29228379e-02  4.69826907e-02  2.28882916e-02  2.74114534e-02\n",
      " -2.91982964e-02  3.12706679e-02 -2.22850628e-02 -1.02282189e-01\n",
      " -2.79116724e-02  1.13793267e-02  9.06308591e-02 -4.75414395e-02\n",
      " -1.00718938e-01 -1.23232193e-02 -7.96929076e-02 -1.44636277e-02\n",
      " -7.76400790e-02 -7.66916666e-03  9.73954238e-03  2.24204753e-02\n",
      "  7.77268335e-02 -3.17154033e-03  2.11537927e-02 -3.30394097e-02\n",
      "  9.55248531e-03 -3.73011716e-02  2.61360835e-02 -9.79087781e-03\n",
      " -6.31505325e-02  5.77434432e-03 -3.80031206e-02  1.29684824e-02\n",
      " -1.82499811e-02 -1.56282876e-02 -1.23363815e-03  5.55579364e-02\n",
      "  1.13087393e-04 -5.61256632e-02  7.40165487e-02  1.84452217e-02\n",
      " -2.66368277e-02  1.31951785e-02  7.50086308e-02 -2.46796962e-02\n",
      " -3.24006155e-02 -1.57674886e-02 -8.03518295e-03 -5.61318127e-03\n",
      "  1.05687715e-02  3.26167489e-03 -3.91989946e-02 -9.38677117e-02\n",
      "  1.14227191e-01  6.57304302e-02 -4.72633801e-02  1.45087512e-02\n",
      " -3.54490355e-02 -3.37761678e-02 -5.15505895e-02 -3.81005858e-03\n",
      " -5.15036434e-02 -5.93429320e-02 -1.69411942e-03  7.42107630e-02\n",
      " -4.20091674e-02 -7.19975084e-02  3.17250118e-02 -1.66303422e-02\n",
      "  3.96981975e-03 -6.52750805e-02  2.77391076e-02 -7.51648918e-02\n",
      "  2.27455646e-02 -3.91368270e-02  1.54315718e-02 -5.54908514e-02\n",
      "  1.23318862e-02 -2.59520635e-02  6.66423738e-02 -6.91258591e-34\n",
      "  3.31629068e-02  8.47929269e-02 -6.65583909e-02  3.33541408e-02\n",
      "  4.71612765e-03  1.35361934e-02 -5.38694113e-02  9.20693949e-02\n",
      " -2.96876673e-02  3.16219479e-02 -2.37497017e-02  1.98770948e-02\n",
      "  1.03446215e-01 -9.06947702e-02  6.30626967e-03  1.42886192e-02\n",
      "  1.19293891e-02  6.43724250e-03  4.20104600e-02  1.25344750e-02\n",
      "  3.93019356e-02  5.35691679e-02 -4.30750176e-02  6.10432290e-02\n",
      " -5.39811481e-05  6.91682622e-02  1.05520012e-02  1.22112045e-02\n",
      " -7.23184943e-02  2.50469781e-02 -5.18371351e-02 -4.36562113e-02\n",
      " -6.71818256e-02  1.34828426e-02 -7.25888833e-02  7.04166479e-03\n",
      "  6.58939555e-02  1.08994395e-02 -2.60012201e-03  5.49969003e-02\n",
      "  5.06966710e-02  3.27948742e-02 -6.68832958e-02  6.45557940e-02\n",
      " -2.52075847e-02 -2.92571764e-02 -1.16696715e-01  3.24064456e-02\n",
      "  5.85858040e-02 -3.51756327e-02 -7.15240240e-02  2.24936120e-02\n",
      " -1.00786746e-01 -4.74544764e-02 -7.61963055e-02 -5.87166548e-02\n",
      "  4.21138555e-02 -7.47213811e-02  1.98467858e-02 -3.36502981e-03\n",
      " -5.29736876e-02  2.74729859e-02  3.45736854e-02 -6.11846596e-02\n",
      "  1.06364816e-01 -9.64119509e-02 -4.55945134e-02  1.51489768e-02\n",
      " -5.13522048e-03 -6.64447770e-02  4.31721583e-02 -1.10405777e-02\n",
      " -9.80249792e-03  7.53783137e-02 -1.49570899e-02 -4.80208285e-02\n",
      "  5.80726378e-02 -2.43896805e-02 -2.23137513e-02 -4.36992496e-02\n",
      "  5.12053780e-02 -3.28625888e-02  1.08763315e-01  6.08926788e-02\n",
      "  3.30791320e-03  5.53819910e-02  8.43200982e-02  1.27087180e-02\n",
      "  3.84465493e-02  6.52325749e-02 -2.94683762e-02  5.08005247e-02\n",
      " -2.09348034e-02  1.46135688e-01  2.25561056e-02 -1.77227744e-08\n",
      " -5.02672680e-02 -2.79217988e-04 -1.00328587e-01  2.42811274e-02\n",
      " -7.54043385e-02 -3.79139856e-02  3.96049805e-02  3.10079996e-02\n",
      " -9.05703939e-03 -6.50411919e-02  4.05453108e-02  4.83390130e-02\n",
      " -4.56962362e-02  4.76006093e-03  2.64361198e-03  9.35614109e-02\n",
      " -4.02599461e-02  3.27401720e-02  1.18298177e-02  5.54344505e-02\n",
      "  1.48052245e-01  7.21189156e-02  2.76968669e-04  1.68651380e-02\n",
      "  8.34882259e-03 -8.76155589e-03 -1.33649791e-02  6.14237227e-02\n",
      "  1.57167856e-02  6.94960803e-02  1.08621800e-02  6.08018301e-02\n",
      " -5.33421896e-02 -3.47923823e-02 -3.36272158e-02  6.93906844e-02\n",
      "  1.22987675e-02 -1.45237386e-01 -2.06969213e-03 -4.61132526e-02\n",
      "  3.72749381e-03 -5.59355551e-03 -1.00659855e-01 -4.45953310e-02\n",
      "  5.40921204e-02  4.98893578e-03  1.49534279e-02 -8.26059952e-02\n",
      "  6.26630858e-02 -5.01906965e-03 -4.81857695e-02 -3.53991278e-02\n",
      "  9.03388113e-03 -2.42338069e-02  5.66267483e-02  2.51529217e-02\n",
      " -1.70708969e-02 -1.24780377e-02  3.19518335e-02  1.38421012e-02\n",
      " -1.55815175e-02  1.00178257e-01  1.23657197e-01 -4.22967114e-02]\n",
      "\n",
      "Sentence: Sentences are passed as a list of string.\n",
      "Embedding: [ 5.64524680e-02  5.50024174e-02  3.13795842e-02  3.39485072e-02\n",
      " -3.54247317e-02  8.34668055e-02  9.88801047e-02  7.27545656e-03\n",
      " -6.68659527e-03 -7.65814353e-03  7.93738067e-02  7.39676878e-04\n",
      "  1.49292015e-02 -1.51046831e-02  3.67674194e-02  4.78743315e-02\n",
      " -4.81969342e-02 -3.76052037e-02 -4.60278317e-02 -8.89815986e-02\n",
      "  1.20228142e-01  1.30663216e-01 -3.73935886e-02  2.47856718e-03\n",
      "  2.55824998e-03  7.25814551e-02 -6.80436566e-02 -5.24696223e-02\n",
      "  4.90234047e-02  2.99563184e-02 -5.84429577e-02 -2.02263352e-02\n",
      "  2.08821986e-02  9.76691917e-02  3.52390222e-02  3.91140766e-02\n",
      "  1.05668101e-02  1.56232214e-03 -1.30823022e-02  8.52904655e-03\n",
      " -4.84091695e-03 -2.03766637e-02 -2.71800943e-02  2.83307564e-02\n",
      "  3.66017781e-02  2.51276512e-02 -9.90861878e-02  1.15626566e-02\n",
      " -3.60380486e-02 -7.23784119e-02 -1.12670071e-01  1.12941880e-02\n",
      " -3.86397354e-02  4.67386283e-02 -2.88461000e-02  2.26703621e-02\n",
      " -8.52407888e-03  3.32815051e-02 -1.06580416e-03 -7.09745362e-02\n",
      " -6.31170198e-02 -5.72187193e-02 -6.16026372e-02  5.47146648e-02\n",
      "  1.18317613e-02 -4.66261208e-02  2.56959815e-02 -7.07414700e-03\n",
      " -5.73842935e-02  4.12839167e-02 -5.91503195e-02  5.89021929e-02\n",
      " -4.41697501e-02  4.65081595e-02 -3.15814093e-02  5.58312312e-02\n",
      "  5.54578640e-02 -5.96533529e-02  4.06406969e-02  4.83761635e-03\n",
      " -4.96768020e-02 -1.00944370e-01  3.40078361e-02  4.13270993e-03\n",
      " -2.93522305e-03  2.11837403e-02 -3.73962112e-02 -2.79066768e-02\n",
      " -4.61767986e-02  5.26138432e-02 -2.79734787e-02 -1.62379250e-01\n",
      "  6.61042035e-02  1.72274057e-02 -5.45110693e-03  4.74474132e-02\n",
      " -3.82237434e-02 -3.96896712e-02  1.34544717e-02  4.49653938e-02\n",
      "  4.53674421e-03  2.82978918e-02  8.36633295e-02 -1.00858193e-02\n",
      " -1.19353965e-01 -3.84624526e-02  4.82858680e-02 -9.46083814e-02\n",
      "  1.91854071e-02 -9.96518508e-02 -6.30596578e-02  3.02696042e-02\n",
      "  1.17402663e-02 -4.78372499e-02 -6.20265398e-03 -3.32850479e-02\n",
      " -4.04385710e-03  1.28307156e-02  4.05253991e-02  7.56477043e-02\n",
      "  2.92434730e-02  2.84270346e-02 -2.78938990e-02  1.66858118e-02\n",
      " -2.47961786e-02 -6.83650672e-02  2.89968699e-02 -5.39867784e-33\n",
      " -2.69014016e-03 -2.65069250e-02 -6.47948007e-04 -8.46194755e-03\n",
      " -7.35154748e-02  4.94082971e-03 -5.97842298e-02  1.03438376e-02\n",
      "  2.12907116e-03 -2.88217608e-03 -3.17076370e-02 -9.42364335e-02\n",
      "  3.03020086e-02  7.00226426e-02  4.50685918e-02  3.69439498e-02\n",
      "  1.13593210e-02  3.53026949e-02  5.50449779e-03  1.34416856e-03\n",
      "  3.46122310e-03  7.75047541e-02  5.45112342e-02 -7.92055726e-02\n",
      " -9.31696743e-02 -4.03398760e-02  3.10668796e-02 -3.83081436e-02\n",
      " -5.89443222e-02  1.93331931e-02 -2.67160218e-02 -7.91938156e-02\n",
      "  1.04167084e-04  7.70621300e-02  4.16603684e-02  8.90932605e-02\n",
      "  3.56843062e-02 -1.09153129e-02  3.71498577e-02 -2.07070485e-02\n",
      " -2.46100798e-02 -2.05025375e-02  2.62201298e-02  3.43590379e-02\n",
      "  4.39251103e-02 -8.20518006e-03 -8.40710029e-02  4.24170531e-02\n",
      "  4.87499125e-02  5.95385022e-02  2.87747774e-02  3.37638371e-02\n",
      " -4.07443047e-02 -1.66371954e-03  7.91927576e-02  3.41088399e-02\n",
      " -5.72830730e-04  1.87749546e-02 -1.36963921e-02  7.38333166e-02\n",
      "  5.74524689e-04  8.33505467e-02  5.60811087e-02 -1.13710631e-02\n",
      "  4.42611724e-02  2.69582067e-02 -4.80536111e-02 -3.15087363e-02\n",
      "  7.75226504e-02  1.81773156e-02 -8.83005112e-02 -7.85520673e-03\n",
      " -6.22243024e-02  7.19372854e-02 -2.33475063e-02  6.52483990e-03\n",
      " -9.49527696e-03 -9.88313034e-02  4.01306450e-02  3.07397265e-02\n",
      " -2.21607015e-02 -9.45911631e-02  1.02368211e-02  1.02187768e-01\n",
      " -4.12960313e-02 -3.15778181e-02  4.74752374e-02 -1.10209785e-01\n",
      "  1.69614665e-02 -3.71709354e-02 -1.03261694e-02 -4.72538061e-02\n",
      " -1.20214000e-02 -1.93255264e-02  5.79292364e-02  4.23866870e-34\n",
      "  3.92013118e-02  8.41362029e-02 -1.02946699e-01  6.92259818e-02\n",
      "  1.68821476e-02 -3.26760784e-02  9.65956785e-03  1.80899836e-02\n",
      "  2.17940304e-02  1.63189452e-02 -9.69292447e-02  3.74847278e-03\n",
      " -2.38456912e-02 -3.44056040e-02  7.11962655e-02  9.21890896e-04\n",
      " -6.23857509e-03  3.23754027e-02 -8.90389609e-04  5.01904683e-03\n",
      " -4.24538329e-02  9.89083871e-02 -4.60321084e-02  4.69704978e-02\n",
      " -1.75284687e-02 -7.02514872e-03  1.32743679e-02 -5.30152470e-02\n",
      "  2.66409595e-03  1.45818982e-02  7.43344938e-03 -3.07131745e-02\n",
      " -2.09416468e-02  8.24110433e-02 -5.15893996e-02 -2.71178447e-02\n",
      "  1.17583029e-01  7.72509351e-03 -1.89522896e-02  3.94559279e-02\n",
      "  7.17360228e-02  2.59116702e-02  2.75191721e-02  9.50542651e-03\n",
      " -3.02355643e-02 -4.07944806e-02 -1.04028471e-01 -7.97419436e-03\n",
      " -3.64458491e-03  3.29716504e-02 -2.35954728e-02 -7.50513747e-03\n",
      " -5.82234189e-02 -3.17906216e-02 -4.18049283e-02  2.17453465e-02\n",
      " -6.67292178e-02 -4.89104167e-02  4.58515296e-03 -2.66046412e-02\n",
      " -1.12597026e-01  5.11167720e-02  5.48534133e-02 -6.69857115e-02\n",
      "  1.26766309e-01 -8.59487951e-02 -5.94231524e-02 -2.92191119e-03\n",
      " -1.14875855e-02 -1.26025781e-01 -3.48276924e-03 -9.12001729e-02\n",
      " -1.22933060e-01  1.33777205e-02 -4.75774817e-02 -6.57933056e-02\n",
      " -3.39409709e-02 -3.07107456e-02 -5.22034131e-02 -2.35463865e-02\n",
      "  5.90035059e-02 -3.85757759e-02  3.19701135e-02  4.05118763e-02\n",
      "  1.67077929e-02 -3.58281098e-02  1.45687908e-02  3.20137553e-02\n",
      " -1.34843802e-02  6.07819781e-02 -8.31397530e-03 -1.08106034e-02\n",
      "  4.69410606e-02  7.66133741e-02 -4.23400402e-02 -2.11963318e-08\n",
      " -7.25292638e-02 -4.20227982e-02 -6.12374656e-02  5.24666533e-02\n",
      " -1.42363729e-02  1.18487515e-02 -1.40789254e-02 -3.67530212e-02\n",
      " -4.44977731e-02 -1.15139959e-02  5.23317270e-02  2.96651851e-02\n",
      " -4.62780371e-02 -3.70892622e-02  1.89129841e-02  2.04307064e-02\n",
      " -2.24006027e-02 -1.48562631e-02 -1.79504305e-02  4.20007706e-02\n",
      "  1.40942698e-02 -2.83492431e-02 -1.16862997e-01  1.48956478e-02\n",
      " -7.30583211e-04  5.66028282e-02 -2.68739909e-02  1.09106690e-01\n",
      "  2.94558844e-03  1.19267896e-01  1.14212424e-01  8.92973691e-02\n",
      " -1.70255750e-02 -4.99053672e-02 -2.11930890e-02  3.18421274e-02\n",
      "  7.03435838e-02 -1.02929436e-01  8.23816732e-02  2.81968303e-02\n",
      "  3.21146473e-02  3.79107893e-02 -1.09553084e-01  8.19620490e-02\n",
      "  8.73216689e-02 -5.73563762e-02 -2.01709308e-02 -5.69444448e-02\n",
      " -1.30338343e-02 -5.55684157e-02 -1.32966395e-02  8.64009745e-03\n",
      "  5.30012473e-02 -4.06847149e-02  2.71709040e-02 -2.55948678e-03\n",
      "  3.05775534e-02 -4.61865515e-02  4.68034158e-03 -3.64947058e-02\n",
      "  6.80802986e-02  6.65087774e-02  8.49152133e-02 -3.32848988e-02]\n",
      "\n",
      "Sentence: The quick brown fox jumps over the lazy dog.\n",
      "Embedding: [ 4.39335704e-02  5.89344501e-02  4.81783673e-02  7.75481015e-02\n",
      "  2.67443936e-02 -3.76295634e-02 -2.60511087e-03 -5.99430278e-02\n",
      " -2.49600294e-03  2.20728200e-02  4.80259694e-02  5.57553098e-02\n",
      " -3.89454216e-02 -2.66167801e-02  7.69339455e-03 -2.62376741e-02\n",
      " -3.64160873e-02 -3.78161147e-02  7.40781426e-02 -4.95050363e-02\n",
      " -5.85217439e-02 -6.36196807e-02  3.24349664e-02  2.20085494e-02\n",
      " -7.10637197e-02 -3.31577770e-02 -6.94103986e-02 -5.00373952e-02\n",
      "  7.46267810e-02 -1.11133851e-01 -1.23063186e-02  3.77456769e-02\n",
      " -2.80313343e-02  1.45353572e-02 -3.15585695e-02 -8.05836096e-02\n",
      "  5.83525896e-02  2.59005441e-03  3.92801873e-02  2.57695932e-02\n",
      "  4.98505756e-02 -1.75626890e-03 -4.55297753e-02  2.92607471e-02\n",
      " -1.02017261e-01  5.22287488e-02 -7.90899470e-02 -1.02857612e-02\n",
      "  9.20246262e-03  1.30732525e-02 -4.04777937e-02 -2.77925283e-02\n",
      "  1.24667073e-02  6.72832653e-02  6.81248009e-02 -7.57119851e-03\n",
      " -6.09940477e-03 -4.23776656e-02  5.17815948e-02 -1.56707261e-02\n",
      "  9.56360903e-03  4.12390195e-02  2.14959066e-02  1.04293600e-02\n",
      "  2.73349509e-02  1.87062360e-02 -2.69607585e-02 -7.00542256e-02\n",
      " -1.04700468e-01 -1.89871888e-03  1.77016705e-02 -5.74725568e-02\n",
      " -1.44223543e-02  4.70450439e-04  2.33228644e-03 -2.51920652e-02\n",
      "  4.93004322e-02 -5.09609841e-02  6.31983057e-02  1.49165075e-02\n",
      " -2.70766746e-02 -4.52875644e-02 -4.90594469e-02  3.74940485e-02\n",
      "  3.84579897e-02  1.56902603e-03  3.09922379e-02  2.01630481e-02\n",
      " -1.24363741e-02 -3.06720156e-02 -2.78819017e-02 -6.89182580e-02\n",
      " -5.13676666e-02  2.14795880e-02  1.15746930e-02  1.25410571e-03\n",
      "  1.88765563e-02 -4.42319065e-02 -4.49817292e-02 -3.41869332e-03\n",
      "  1.31131476e-02  2.00099498e-02  1.21099755e-01  2.31074784e-02\n",
      " -2.20158976e-02 -3.28847170e-02 -3.15515604e-03  1.17878408e-04\n",
      "  9.91498753e-02  1.65239088e-02 -4.69672075e-03 -1.45366723e-02\n",
      " -3.71077610e-03  9.65136215e-02  2.85908207e-02  2.13481877e-02\n",
      " -7.17645735e-02 -2.41142102e-02 -4.40940633e-02 -1.07346870e-01\n",
      "  6.79945573e-02  1.30466774e-01 -7.97030032e-02  6.79507293e-03\n",
      " -2.37511788e-02 -4.61636446e-02 -2.99650785e-02 -3.69410010e-33\n",
      "  7.30969608e-02 -2.20171511e-02 -8.61464366e-02 -7.14379549e-02\n",
      " -6.36741892e-02 -7.21863210e-02 -5.93045168e-03 -2.33641304e-02\n",
      " -2.83658206e-02  4.77434695e-02 -8.06176364e-02 -1.56476395e-03\n",
      "  1.38443531e-02 -2.86235940e-02 -3.35386544e-02 -1.13777518e-01\n",
      " -9.17634834e-03 -1.08101144e-02  3.23195867e-02  5.88380955e-02\n",
      "  3.34209017e-02  1.07987933e-01 -3.72713245e-02 -2.96770725e-02\n",
      "  5.17190322e-02 -2.25339010e-02 -6.96090907e-02 -2.14475319e-02\n",
      " -2.33410560e-02  4.82199639e-02 -3.58766392e-02 -4.68991175e-02\n",
      " -3.97873446e-02  1.10813215e-01 -1.43007059e-02 -1.18464500e-01\n",
      "  5.82915507e-02 -6.25889078e-02 -2.94040777e-02  6.03238493e-02\n",
      " -2.44415249e-03  1.60115883e-02  2.67233849e-02  2.49530282e-02\n",
      " -6.49318919e-02 -1.06802201e-02  2.81464476e-02  1.03564160e-02\n",
      " -6.63588231e-04  1.98186468e-02 -3.04288473e-02  6.28424389e-03\n",
      "  5.15268408e-02 -4.75375056e-02 -6.44421130e-02  9.55031961e-02\n",
      "  7.55858347e-02 -2.81574447e-02 -3.49966101e-02  1.01816393e-01\n",
      "  1.98732279e-02 -3.68036292e-02  2.93524913e-03 -5.00745066e-02\n",
      "  1.50932148e-01 -6.16079383e-02 -8.58812854e-02  7.13991933e-03\n",
      " -1.33065563e-02  7.80405179e-02  1.75250601e-02  4.21279445e-02\n",
      "  3.57939936e-02 -1.32950455e-01  3.56970318e-02 -2.03117039e-02\n",
      "  1.24909896e-02 -3.80355380e-02  4.91543300e-02 -1.56541094e-02\n",
      "  1.21418223e-01 -8.08644369e-02 -4.68781702e-02  4.10843194e-02\n",
      " -1.84318163e-02  6.69690669e-02  4.33594687e-03  2.27315016e-02\n",
      " -1.36428932e-02 -4.53238375e-02 -3.92829925e-02 -6.29889686e-03\n",
      "  5.29609881e-02 -3.69064547e-02  7.11676851e-02  2.33343416e-33\n",
      "  1.05231345e-01 -4.81874309e-02  6.95919171e-02  6.56976476e-02\n",
      " -4.65148576e-02  5.14492691e-02 -1.24475248e-02  3.20872180e-02\n",
      " -9.23356637e-02  5.00932932e-02 -3.28875929e-02  1.39138810e-02\n",
      " -8.70249700e-04 -4.90908930e-03  1.03946418e-01  3.21575440e-04\n",
      "  5.28110452e-02 -1.17990533e-02  2.31565759e-02  1.31768268e-02\n",
      " -5.25962599e-02  3.26702446e-02  3.08661431e-04  6.41128942e-02\n",
      "  3.88500877e-02  5.88008612e-02  8.29793215e-02 -1.88149698e-02\n",
      " -2.26377714e-02 -1.00473635e-01 -3.83752659e-02 -5.88081852e-02\n",
      "  1.82419305e-03 -4.26995531e-02  2.50195134e-02  6.40060008e-02\n",
      " -3.77482921e-02 -6.83899596e-03 -2.54607713e-03 -9.76042897e-02\n",
      "  1.88475624e-02 -8.83171742e-04  1.73611958e-02  7.10790828e-02\n",
      "  3.30393203e-02  6.93424093e-03 -5.60523421e-02  5.14634699e-02\n",
      " -4.29541990e-02  4.60077450e-02 -8.78834073e-03  3.17289047e-02\n",
      "  4.93965447e-02  2.95190178e-02 -5.05192764e-02 -5.43186888e-02\n",
      "  1.49943458e-04 -2.76614130e-02  3.46878618e-02 -2.10890323e-02\n",
      "  1.38059799e-02  2.99886353e-02  1.39744533e-02 -4.26468812e-03\n",
      " -1.50337061e-02 -8.76095295e-02 -6.85053691e-02 -4.28141691e-02\n",
      "  7.76945204e-02 -7.10285529e-02 -7.37687247e-03  2.13727243e-02\n",
      "  1.35562364e-02 -7.90464580e-02  5.47668152e-03  8.30663368e-02\n",
      "  1.14147991e-01  1.80762587e-03  8.75491202e-02 -4.16044779e-02\n",
      "  1.55416224e-02 -1.01206480e-02 -7.32435798e-03  1.07966419e-02\n",
      " -6.62816986e-02  3.98413576e-02 -1.16711512e-01  6.42993897e-02\n",
      "  4.02920060e-02 -6.54741600e-02  1.95052773e-02  8.09995756e-02\n",
      "  5.36463782e-02  7.67969489e-02 -1.34852575e-02 -1.76919048e-08\n",
      " -4.43935208e-02  9.20641702e-03 -8.79590437e-02  4.26921584e-02\n",
      "  7.31364936e-02  1.68427583e-02 -4.03262936e-02  1.85131785e-02\n",
      "  8.44172463e-02 -3.74476798e-02  3.02996188e-02  2.90641822e-02\n",
      "  6.36878684e-02  2.89750397e-02 -1.47270057e-02  1.77543126e-02\n",
      " -3.36895287e-02  1.73160955e-02  3.37875299e-02  1.76826045e-01\n",
      " -1.75533406e-02 -6.03077933e-02 -1.43394601e-02 -2.38536131e-02\n",
      " -4.45531197e-02 -2.89850049e-02 -8.96776244e-02 -1.75938255e-03\n",
      " -2.61485875e-02  5.93999913e-03 -5.18355519e-02  8.57279599e-02\n",
      " -8.18398744e-02  8.35439656e-03  4.00789790e-02  4.17764075e-02\n",
      "  1.04573548e-01 -2.86561972e-03  1.96691044e-02  5.81046520e-03\n",
      "  1.33253438e-02  4.51001301e-02 -2.17588190e-02 -1.39493123e-02\n",
      " -6.86992407e-02 -2.94114207e-03 -3.10764872e-02 -1.05854437e-01\n",
      "  6.91624135e-02 -4.24114801e-02 -4.67682108e-02 -3.64750996e-02\n",
      "  4.50399891e-02  6.09816350e-02 -6.56561777e-02 -5.45638567e-03\n",
      " -1.86226703e-02 -6.31484836e-02 -3.87436934e-02  3.46733704e-02\n",
      "  5.55458143e-02  5.21627963e-02  5.61065264e-02  1.02063894e-01]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "#Our sentences we like to encode\n",
    "sentences = ['This framework generates embeddings for each input sentence',\n",
    "    'Sentences are passed as a list of string.',\n",
    "    'The quick brown fox jumps over the lazy dog.']\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "#Print the embeddings\n",
    "for sentence, embedding in zip(sentences, embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia is a multilingual, free, online encyclopedia written and maintained by a community of volunteers, known as Wikipedians, through open collaboration and using a wiki-based editing system called MediaWiki. Wikipedia is the largest and most-read reference work in history. It is consistently one of the 10 most popular websites ranked by Similarweb and formerly Alexa; as of 2023, Wikipedia was ranked the 5th most popular site in the world according to Semrush. It is hosted by the Wikimedia Foundation, an American non-profit organization funded mainly through donations.\n"
     ]
    }
   ],
   "source": [
    "ex = \"Wikipedia is a multilingual, free, online encyclopedia written and maintained by a community of volunteers, known as Wikipedians, through open collaboration and using a wiki-based editing system called MediaWiki. Wikipedia is the largest and most-read reference work in history. It is consistently one of the 10 most popular websites ranked by Similarweb and formerly Alexa; as of 2023, Wikipedia was ranked the 5th most popular site in the world according to Semrush. It is hosted by the Wikimedia Foundation, an American non-profit organization funded mainly through donations.\"\n",
    "print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "def tokenizeCorpus(corpus, model=BertModel.from_pretrained('bert-base-uncased'), tokenizer = BertTokenizer.from_pretrained('bert-base-uncased'), delimiter=\".\"):\n",
    "    splited = [sentence+delimiter for sentence in corpus.split(\".\")]\n",
    "    SEPmarked = [sentence+\" [SEP]\" for sentence in splited[:-1]]\n",
    "    marked_text = \"[CLS] \"+\"\".join(SEPmarked)[:-6]\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segment_ids = []\n",
    "    i = 0\n",
    "    for token in tokenized_text:\n",
    "        segment_ids.append(i)\n",
    "        if token == \"[SEP]\":\n",
    "            i += 1\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensor = torch.tensor([segment_ids])\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensor)\n",
    "    hidden_states = outputs[-1]\n",
    "    \n",
    "    \n",
    "    return outputs\n",
    "\n",
    "o = tokenizeCorpus(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan]]]), pooler_output=tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]]), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
