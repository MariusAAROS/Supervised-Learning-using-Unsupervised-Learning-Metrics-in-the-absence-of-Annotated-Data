{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\myLibraries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from umap import UMAP\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from MARScore.utils import *\n",
    "import hdbscan\n",
    "from custom_score.utils import cleanString, get_git_root\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url=\"https://drive.google.com/file/d/1Wd0M3qepNF6B4YwFYrpo7CaSERpudAG_/view?usp=share_link\"\n",
    "dataset_url='https://drive.google.com/uc?export=download&id=' + dataset_url.split('/')[-2]\n",
    "dataset = pd.read_json(dataset_url, lines=True)\n",
    "dataset = dataset.loc[:, [\"text\", \"summary\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeCorpus(corpus, model=BertModel.from_pretrained('bert-base-uncased', \n",
    "                                                           output_hidden_states=True), \n",
    "                           tokenizer = BertTokenizer.from_pretrained('bert-base-uncased'), \n",
    "                           model_input_size=512):\n",
    "    def flatten(l):\n",
    "        return [item for sublist in l for item in sublist]\n",
    "    input_size = model_input_size - 1\n",
    "    corpusWords = corpus.split(\" \")\n",
    "    splited = [\" \".join(corpusWords[i:i+input_size]) for i in range(0, len(corpusWords), input_size)]\n",
    "\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for sentence in splited:\n",
    "        encoded = tokenizer.encode_plus(sentence, \n",
    "                                        add_special_tokens=True,\n",
    "                                        max_length=input_size+1,\n",
    "                                        padding=\"max_length\",\n",
    "                                        return_attention_mask=True,\n",
    "                                        return_tensors='pt',\n",
    "                                        truncation=True)\n",
    "        input_ids.append(encoded[\"input_ids\"])\n",
    "        attention_masks.append(encoded[\"attention_mask\"])\n",
    "\n",
    "    #inputs_ids = torch.Tensor(len(input_ids),1, max_len+1)\n",
    "    #torch.cat(input_ids, out=inputs_ids)\n",
    "    inputs_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    \n",
    "    temp = flatten([batch.tolist() for batch in input_ids])\n",
    "    labels = np.array(temp)\n",
    "    labels = labels.reshape((labels.shape[0]*labels.shape[1]))\n",
    "    labels = tokenizer.convert_ids_to_tokens(labels)\n",
    "    with torch.no_grad():\n",
    "        output = model(inputs_ids, attention_mask=attention_masks)\n",
    "    return output, labels\n",
    "\n",
    "def vectorizeCorpus(model_output, allStates=True):\n",
    "    if allStates==True:\n",
    "        hidden_states = model_output.hidden_states\n",
    "    else:\n",
    "        hidden_states = [model_output.last_hidden_state]\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "    token_embeddings = token_embeddings.permute(1,2,0,3)\n",
    "    embs = []\n",
    "    for batch in token_embeddings:\n",
    "        for token in batch:\n",
    "            emb = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "            embs.append(emb)\n",
    "    return embs\n",
    "\n",
    "def visualizeCorpus(embs, labels, embs_gold=None, labels_gold=None, dim=2):\n",
    "    comp_gold = True if embs_gold != None and labels_gold != None else False\n",
    "\n",
    "    formated_embs = [token.tolist() for token in embs]\n",
    "    formated_embs = np.array(formated_embs)\n",
    "    formated_embs_gold = [token.tolist() for token in embs_gold]\n",
    "    formated_embs_gold = np.array(formated_embs_gold)\n",
    "    token_indexes = [i for i in range(len(labels)) if labels[i] != \"[PAD]\" and labels[i] != \"[CLS]\" and labels[i] != \"[SEP]\" and len(labels[i])>2]\n",
    "\n",
    "    if dim == 1:\n",
    "        umap1D = UMAP(n_components=1, init='random', random_state=0)\n",
    "        proj1D = umap1D.fit_transform(formated_embs).T\n",
    "\n",
    "        data = {\"x\": proj1D[0],\n",
    "                \"labels\": labels}\n",
    "        \n",
    "        for k in data.keys():\n",
    "            data[k] = [data[k][i] for i in range(len(data[k])) if i in token_indexes]\n",
    "\n",
    "        if comp_gold:\n",
    "            token_indexes_gold = [i for i in range(len(labels_gold)) if labels_gold[i] != \"[PAD]\" and labels_gold[i] != \"[CLS]\" and labels_gold[i] != \"[SEP]\" and len(labels_gold[i])>2]\n",
    "            proj1D_gold = umap1D.fit_transform(formated_embs_gold).T\n",
    "            data_gold = {\"x\": proj1D_gold[0],\n",
    "                        \"labels\": labels_gold}\n",
    "            for k in data_gold.keys():\n",
    "                data_gold[k] = [data_gold[k][i] for i in range(len(data_gold[k])) if i in token_indexes_gold]\n",
    "\n",
    "        traces = []\n",
    "        for i in range(len(data['x'])):\n",
    "            if comp_gold:\n",
    "                color = 'green' if data[\"labels\"][i] in data_gold[\"labels\"] else 'red'\n",
    "            else:\n",
    "                color = 'red'\n",
    "            trace = go.Scatter(\n",
    "                x=[data['x'][i]],\n",
    "                mode='markers',\n",
    "                marker=dict(size=6, color=color),\n",
    "                text=[data['labels'][i]],\n",
    "                name=data['labels'][i]\n",
    "            )\n",
    "            traces.append(trace)\n",
    "        if comp_gold:\n",
    "            for i in range(len(data_gold['x'])):\n",
    "                trace = go.Scatter(\n",
    "                    x=[data_gold['x'][i]],\n",
    "                    mode='markers',\n",
    "                    marker=dict(size=6, color='gold'),\n",
    "                    text=[data_gold['labels'][i]],\n",
    "                    name=data_gold['labels'][i]\n",
    "                )\n",
    "                traces.append(trace)\n",
    "\n",
    "        layout = go.Layout(\n",
    "            title='1D Scatter Plot',\n",
    "            scene=dict(\n",
    "                xaxis=dict(title='X')\n",
    "            )\n",
    "        )\n",
    "        fig = go.Figure(data=traces, layout=layout)\n",
    "        fig.show()\n",
    "\n",
    "    elif dim == 2:\n",
    "        umap2D = UMAP(n_components=2, init='random', random_state=0)\n",
    "        proj2D = umap2D.fit_transform(formated_embs).T\n",
    "\n",
    "        data = {\"x\": proj2D[0],\n",
    "                \"y\": proj2D[1],\n",
    "                \"labels\": labels}\n",
    "        \n",
    "        for k in data.keys():\n",
    "            data[k] = [data[k][i] for i in range(len(data[k])) if i in token_indexes]\n",
    "\n",
    "        if comp_gold:\n",
    "            token_indexes_gold = [i for i in range(len(labels_gold)) if labels_gold[i] != \"[PAD]\" and labels_gold[i] != \"[CLS]\" and labels_gold[i] != \"[SEP]\" and len(labels_gold[i])>2]\n",
    "            proj2D_gold = umap2D.fit_transform(formated_embs_gold).T\n",
    "            data_gold = {\"x\": proj2D_gold[0],\n",
    "                        \"y\": proj2D_gold[1],\n",
    "                        \"labels\": labels_gold}\n",
    "            for k in data_gold.keys():\n",
    "                data_gold[k] = [data_gold[k][i] for i in range(len(data_gold[k])) if i in token_indexes_gold]\n",
    "\n",
    "        traces = []\n",
    "        for i in range(len(data['x'])):\n",
    "            if comp_gold:\n",
    "                color = 'green' if data[\"labels\"][i] in data_gold[\"labels\"] else 'red'\n",
    "            else:\n",
    "                color = 'red'\n",
    "            trace = go.Scatter(\n",
    "                x=[data['x'][i]],\n",
    "                y=[data['y'][i]],\n",
    "                mode='markers',\n",
    "                marker=dict(size=6, color=color),\n",
    "                text=[data['labels'][i]],\n",
    "                name=data['labels'][i]\n",
    "            )\n",
    "            traces.append(trace)\n",
    "        if comp_gold:\n",
    "            for i in range(len(data_gold['x'])):\n",
    "                trace = go.Scatter(\n",
    "                    x=[data_gold['x'][i]],\n",
    "                    y=[data_gold['y'][i]],\n",
    "                    mode='markers',\n",
    "                    marker=dict(size=6, color='gold'),\n",
    "                    text=[data_gold['labels'][i]],\n",
    "                    name=data_gold['labels'][i]\n",
    "                )\n",
    "                traces.append(trace)\n",
    "\n",
    "        layout = go.Layout(\n",
    "            title='2D Scatter Plot',\n",
    "            scene=dict(\n",
    "                xaxis=dict(title='X'),\n",
    "                yaxis=dict(title='Y')\n",
    "            )\n",
    "        )\n",
    "        fig = go.Figure(data=traces, layout=layout)\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states=True)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url=\"https://drive.google.com/file/d/1Wd0M3qepNF6B4YwFYrpo7CaSERpudAG_/view?usp=share_link\"\n",
    "dataset_url='https://drive.google.com/uc?export=download&id=' + dataset_url.split('/')[-2]\n",
    "dataset = pd.read_json(dataset_url, lines=True)\n",
    "dataset = dataset.loc[:, [\"text\", \"summary\"]]\n",
    "\n",
    "elem0 = dataset.iloc[0, 0]\n",
    "print(elem0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold0 = dataset.iloc[0, 1]\n",
    "print(gold0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o, l = tokenizeCorpus(elem0)\n",
    "ogold, lgold = tokenizeCorpus(gold0)\n",
    "v = vectorizeCorpus(o)\n",
    "vgold = vectorizeCorpus(ogold)\n",
    "visualizeCorpus(v, l, vgold, lgold, dim=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering with HDBScan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url=\"https://drive.google.com/file/d/1Wd0M3qepNF6B4YwFYrpo7CaSERpudAG_/view?usp=share_link\"\n",
    "dataset_url='https://drive.google.com/uc?export=download&id=' + dataset_url.split('/')[-2]\n",
    "dataset = pd.read_json(dataset_url, lines=True)\n",
    "dataset = dataset.loc[:, [\"text\", \"summary\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elem0 = dataset.iloc[0, 0]\n",
    "gold0 = dataset.iloc[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o, l = tokenizeCorpus(elem0)\n",
    "ogold, lgold = tokenizeCorpus(gold0)\n",
    "v = vectorizeCorpus(o)\n",
    "vgold = vectorizeCorpus(ogold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = cleanAll(v, l, \"emb\")\n",
    "vgold = cleanAll(vgold, lgold, \"emb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_values = tf(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = hdbscan.HDBSCAN()\n",
    "clusterer.fit(v)\n",
    "clabels = clusterer.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizeCorpus(v, l, vgold, lgold, clabels, tf_values, dim=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute TF of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(words):\n",
    "    text = ' '.join(words)\n",
    "    vectorizer = TfidfVectorizer(use_idf=False, norm=None)\n",
    "    tf_values = vectorizer.fit_transform([text]).toarray()[0]\n",
    "    tf_dict = {word: tf_values[index] for word, index in vectorizer.vocabulary_.items()}\n",
    "    return tf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_dictionary(words):\n",
    "    word_dict = {}\n",
    "    for word in words:\n",
    "        if word not in word_dict:\n",
    "            word_dict[word] = 1\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_word_dictionary([\"Banana\", \"Banana\", \"Apple\", \"Mango\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF des clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "elem0 = dataset.iloc[0, 0]\n",
    "gold0 = dataset.iloc[0, 1]\n",
    "o, l = tokenizeCorpus(elem0)\n",
    "ogold, lgold = tokenizeCorpus(gold0)\n",
    "v = vectorizeCorpus(o)\n",
    "vgold = vectorizeCorpus(ogold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "v, l = cleanAll(v, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tf_values = tf(l)\n",
    "clusterer = hdbscan.HDBSCAN()\n",
    "clusterer.fit(v)\n",
    "clabels = clusterer.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusters_tf(tf_values, labels, clabels):\n",
    "    clusters_tf_values = {}\n",
    "    for label, clabel in zip(labels, clabels):\n",
    "        if clabel in clusters_tf_values.keys():\n",
    "            clusters_tf_values[clabel] += tf_values[label]\n",
    "        else:\n",
    "            clusters_tf_values[clabel] = tf_values[label]\n",
    "    return clusters_tf_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_tf_values = clusters_tf(tf_values, l, clabels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ILP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_ilp_format(labels, clabels, clusters_tf_values, save=True):\n",
    "    #define scoring function\n",
    "    output = \"Maximize\\nscore:\"\n",
    "    for i, k in enumerate(sorted(clusters_tf_values.keys())):\n",
    "        if int(clusters_tf_values[k]) < 0:\n",
    "            output += f\" - {-int(clusters_tf_values[k])} c{i}\"\n",
    "        else:\n",
    "            output += f\" + {int(clusters_tf_values[k])} c{i}\"\n",
    "\n",
    "    #create sentences and sentence dictionnary\n",
    "    sentence_index = 0\n",
    "    sentences_map = {0: set()}\n",
    "    nb_sentences = labels.count(\".\")\n",
    "    for cluster_index, token in zip(clabels, labels):\n",
    "        if cluster_index in sentences_map.keys():\n",
    "            sentences_map[cluster_index].add(sentence_index)\n",
    "        else:\n",
    "            sentences_map[cluster_index] = {sentence_index}\n",
    "        \n",
    "        if token == \".\":\n",
    "            sentence_index += 1\n",
    "            \n",
    "    #define constraints\n",
    "    output += \"\\n\\nSubject To\\n\"\n",
    "    for i, k in enumerate(sorted(sentences_map.keys())):\n",
    "        output += f\"index_{i}:\"\n",
    "        for cluster_index in sorted(sentences_map[k]):\n",
    "            output += f\" s{cluster_index} +\"\n",
    "        output = output[:-2] + f\" - c{i} >= 0\" + \"\\n\"\n",
    "        \n",
    "    #define sentence length\n",
    "    output += \"length:\"\n",
    "    for i in range(nb_sentences):\n",
    "        output += f\" 1 s{i} +\"\n",
    "    output = output[:-2] + \" <= 2000\"\n",
    "\n",
    "    #declare cluster variables\n",
    "    output += \"\\n\\n\\nBinary\\n\"\n",
    "    for i in range(len(clusters_tf_values.keys())):\n",
    "        output += f\"c{i}\\n\"\n",
    "\n",
    "    #declare sentences variables\n",
    "    for i in range(nb_sentences):\n",
    "        output += f\"s{i}\\n\"\n",
    "    output = output[:-1]\n",
    "\n",
    "    #end file\n",
    "    output += \"\\nEnd\"\n",
    "\n",
    "    if save:\n",
    "        root = get_git_root()\n",
    "        path = os.path.join(root, \"myLibraries\\MARScore_output\\ilp_in.ilp\")\n",
    "        with open(path, \"w\") as text_file:\n",
    "            text_file.write(output)\n",
    "            text_file.close()\n",
    "            print(\"\\nSave successful\")\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['section', '.', 'short', 'title', '.', 'this', 'act', 'may', 'cited', 'the', 'national', 'science', 'education', 'tax', 'incentive', 'for', 'businesses', 'act', '2007', '.', 'sec', '.', '.', 'credits', 'for', 'certain', 'contributions', 'benefit', '##ing', 'science', 'technology', 'engineering', 'and', 'mathematics', 'education', 'the', 'elementary', 'and', 'secondary', 'school', 'level', '.', 'general', '.', 'sub', '##par', '##t', 'part', 'sub', '##cha', '##pt', '##er', 'chapter', 'the', 'internal', 'revenue', 'code', '1986', 'relating', 'business', 'related', 'credits', 'amended', 'adding', 'the', 'end', 'the', 'following', 'new', 'section', 'sec', '.', '##o', '.', 'contributions', 'benefit', '##ing', 'science', 'technology', 'engineering', 'and', 'mathematics', 'education', 'the', 'elementary', 'and', 'secondary', 'school', 'level', '.', 'general', '.', 'for', 'purposes', 'section', 'the', 'elementary', 'and', 'secondary', 'science', 'technology', 'engineering', 'and', 'mathematics', 'stem', 'contributions', 'credit', 'determined', 'under', 'this', 'section', 'for', 'the', 'taxa', '##ble', 'year', 'amount', 'equal', '100', 'percent', 'the', 'qualified', 'stem', 'contributions', 'the', 'taxpayer', 'for', 'such', 'taxa', '##ble', 'year', '.', 'qualified', 'stem', 'contributions', '.', 'for', 'purposes', 'this', 'section', 'the', 'term', 'qualified', 'stem', 'contributions', 'means', 'stem', 'school', 'contributions', 'stem', 'teacher', '##tern', '##ship', 'expenses', 'and', 'stem', 'teacher', 'training', 'expenses', '.', 'stem', 'school', 'contributions', '.', 'for', 'purposes', 'this', 'section', 'general', '.', 'the', 'term', 'stem', 'school', 'contributions', 'means', 'stem', 'property', 'contributions', 'and', 'stem', 'service', 'contributions', '.', 'stem', 'property', 'contributions', '.', 'the', 'term', 'stem', 'property', 'contributions', 'means', 'the', 'amount', 'which', 'would', 'but', 'for', 'sub', '##section', 'allowed', '##duction', 'under', 'section', '170', 'for', 'charitable', 'contribution', 'stem', 'inventory', 'property', 'the', 'done', '##e', 'elementary', 'secondary', 'school', 'described', 'section', '170', 'substantially', 'all', 'the', 'use', 'the', 'property', 'the', 'done', '##e', 'within', 'the', 'united', 'states', 'within', 'the', 'defense', 'dependent', '##s', 'education', 'system', 'for', 'educational', 'purposes', 'any', 'the', 'grades', 'that', 'are', 'related', 'the', 'purpose', 'function', 'the', 'done', '##e', 'the', 'original', 'use', 'the', 'property', 'begins', 'with', 'the', 'done', '##e', 'the', 'property', 'will', 'fit', 'productive', '##ly', 'into', 'the', 'done', '##e', 'education', 'plan', 'the', 'property', 'not', 'transferred', 'the', 'done', '##e', 'exchange', 'for', 'money', 'other', 'property', 'services', 'except', 'for', 'shipping', 'installation', 'and', 'transfer', 'costs', 'and', 'the', 'done', '##e', 'use', 'and', 'disposition', 'the', 'property', 'will', 'accordance', 'with', 'the', 'provisions', 'sub', '##para', '##graphs', 'and', '.', 'the', 'determination', 'the', 'amount', '##duction', 'under', 'section', '170', 'for', 'purposes', 'this', 'paragraph', 'shall', 'made', 'the', 'limitation', 'under', 'section', '170', 'applied', 'all', 'stem', 'inventory', 'property', '.', 'stem', 'service', 'contributions', '.', 'the', 'term', 'stem', 'service', 'contributions', 'means', 'the', 'amount', 'paid', 'incurred', 'during', 'the', 'taxa', '##ble', 'year', 'for', 'stem', 'services', 'provided', 'the', 'united', 'states', 'the', 'defense', 'dependent', '##s', 'education', 'system', 'for', 'the', 'exclusive', 'benefit', 'students', 'elementary', 'secondary', 'school', 'described', 'section', '170', 'but', 'only', 'the', 'taxpayer', 'engaged', 'the', 'trade', 'business', 'providing', 'such', 'services', 'commercial', 'basis', 'and', 'charge', 'imposed', 'for', 'providing', 'such', 'services', '.', 'stem', 'inventory', 'property', '.', 'the', 'term', 'stem', 'inventory', 'property', 'means', 'with', 'respect', 'any', 'contribution', 'school', 'any', 'property', 'which', 'described', 'paragraph', 'section', '122', '##1', 'with', 'respect', 'the', 'donor', 'and', 'which', 'determined', 'the', 'school', 'needed', 'the', 'school', 'providing', 'education', 'grades', 'the', 'areas', 'science', 'technology', 'engineering', 'mathematics', '.', 'stem', 'services', '.', 'the', 'term', 'stem', 'services', 'means', 'with', 'respect', 'any', 'contribution', 'school', 'any', 'service', 'determined', 'the', 'school', 'needed', 'the', 'school', 'providing', 'education', 'grades', 'the', 'areas', 'science', 'technology', 'engineering', 'mathematics', 'including', 'teaching', 'courses', 'instruction', 'such', 'school', 'any', 'such', 'area', '.', 'defense', 'dependent', '##s', 'education', 'system', '.', 'for', 'purposes', 'this', 'sub', '##section', 'the', 'term', 'defense', 'dependent', '##s', 'education', 'system', 'means', 'the', 'program', 'established', 'and', 'operated', 'under', 'the', 'defense', 'dependent', '##s', 'education', 'act', '1978', '.', '.', '.', '##1', '##q', '.', '.', 'stem', 'teacher', '##tern', '##ship', 'expenses', '.', 'for', 'purposes', 'this', 'section', 'general', '.', 'the', 'term', 'stem', 'teacher', '##tern', '##ship', 'expenses', 'means', 'any', 'amount', 'paid', 'incurred', 'carry', 'out', 'stem', '##tern', '##ship', 'program', 'the', 'taxpayer', 'but', 'only', 'the', 'extent', 'that', 'such', 'amount', '##tri', '##bu', '##table', 'the', 'participation', 'such', 'program', 'any', 'eligible', 'stem', 'teacher', 'including', 'amounts', 'paid', 'such', 'teacher', '##ipe', '##nd', 'while', 'participating', 'such', 'program', '.', 'stem', '##tern', '##ship', 'program', '.', 'the', 'term', 'stem', '##tern', '##ship', 'program', 'means', 'any', 'program', 'established', 'taxpayer', 'engaged', 'trade', 'business', 'within', 'area', 'science', 'technology', 'engineering', 'mathematics', 'and', 'under', 'which', 'eligible', 'stem', 'teachers', 'receive', 'training', 'enhance', 'their', 'teaching', 'skills', 'the', 'areas', 'science', 'technology', 'engineering', 'mathematics', 'otherwise', 'improve', 'their', 'knowledge', 'such', 'areas', '.', 'eligible', 'stem', 'teacher', '.', 'the', 'term', 'eligible', 'stem', 'teacher', 'means', 'any', 'individual', 'who', 'teacher', 'grades', 'educational', 'organization', 'described', 'section', '170', 'which', 'located', 'the', 'united', 'states', 'which', 'located', 'united', 'states', 'military', 'base', 'outside', 'the', 'united', 'states', 'and', 'whose', 'teaching', 'responsibilities', 'such', 'school', 'include', 'are', 'likely', 'include', 'any', 'course', 'the', 'areas', 'science', 'technology', 'engineering', 'mathematics', '.', 'stem', 'teacher', 'training', 'expenses', '.', 'the', 'term', 'stem', 'teacher', 'training', 'expenses', 'means', 'any', 'amount', 'paid', 'incurred', 'taxpayer', 'engaged', 'trade', 'business', 'within', 'area', 'science', 'technology', 'engineering', 'mathematics', 'which', '##tri', '##bu', '##table', 'the', 'participation', 'any', 'eligible', 'stem', 'teacher', 'regular', 'training', 'program', 'provided', 'employees', 'the', 'taxpayer', 'which', 'determined', 'such', 'teacher', 'school', 'enhancing', 'such', 'teacher', 'teaching', 'skills', 'the', 'areas', 'science', 'technology', 'engineering', 'mathematics', '.', 'denial', 'double', 'benefit', '.', '##duction', 'shall', 'allowed', 'under', 'this', 'chapter', 'for', 'any', 'amount', 'allowed', 'credit', 'under', 'this', 'section', '.', '.', 'conform', '##ing', 'amendments', '.', 'section', 'such', 'code', 'amended', 'striking', 'plus', 'the', 'end', 'paragraph', 'striking', 'the', 'period', 'the', 'end', 'paragraph', 'and', 'insert', '##ing', 'plus', 'and', 'adding', 'the', 'end', 'the', 'following', 'new', 'paragraph', 'the', 'elementary', 'and', 'secondary', 'science', 'technology', 'engineering', 'and', 'mathematics', 'stem', 'contributions', 'credit', 'determined', 'under', 'section', '##o', '.', '.', 'the', 'table', 'sections', 'for', 'sub', '##par', '##t', 'part', 'sub', '##cha', '##pt', '##er', 'chapter', 'such', 'code', 'amended', 'adding', 'the', 'end', 'the', 'following', 'new', 'item', 'sec', '.', '##o', '.', 'contributions', 'benefit', '##ing', 'science', 'technology', 'engineering', 'and', 'mathematics', 'education', 'the', 'elementary', 'and', 'secondary', 'school', 'level', '.', '.', 'effective', 'date', '.', 'the', 'amendments', 'made', 'this', 'section', 'shall', 'apply', 'taxa', '##ble', 'years', 'beginning', 'after', 'the', 'date', 'the', 'enactment', 'this', 'act', '.']\n"
     ]
    }
   ],
   "source": [
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"          \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Save successful\n",
      "Maximize\n",
      "score: + 5626 c0 + 49 c1 + 36 c2 + 49 c3 + 36 c4 + 46 c5 + 184 c6 + 276 c7 + 121 c8 + 121 c9 + 1225 c10 + 144 c11 + 144 c12 + 156 c13 + 144 c14 + 220 c15 + 33 c16 + 49 c17 + 19 c18 + 196 c19 + 84 c20 + 258 c21 + 36 c22 + 52 c23 + 377 c24 + 121 c25 + 225 c26 + 50 c27 + 64 c28 + 91 c29 + 638 c30 + 65 c31 + 63 c32 + 168 c33 + 1245 c34 + 1826 c35 + 1558 c36 + 689 c37\n",
      "\n",
      "Subject To\n",
      "index_0: s0 + s1 + s2 + s3 + s4 + s5 + s6 + s7 + s8 + s9 + s10 + s11 + s12 + s13 + s15 + s16 + s18 + s19 + s20 + s21 + s22 + s23 + s25 + s26 + s27 + s29 + s30 + s31 + s33 + s34 + s36 + s37 + s38 + s39 + s40 + s41 + s42 + s43 + s44 + s45 + s46 + s47 + s48 + s49 + s50 + s51 + s52 - c0 >= 0\n",
      "index_1: s18 - c1 >= 0\n",
      "index_2: s11 + s21 + s34 + s36 + s40 - c2 >= 0\n",
      "index_3: s11 + s13 + s15 + s18 + s19 + s27 + s33 - c3 >= 0\n",
      "index_4: s18 + s19 + s21 + s38 - c4 >= 0\n",
      "index_5: s23 + s25 + s36 + s38 + s40 - c5 >= 0\n",
      "index_6: s13 + s16 + s18 + s21 + s23 + s36 + s38 + s45 - c6 >= 0\n",
      "index_7: s5 + s9 + s11 + s18 + s45 + s49 - c7 >= 0\n",
      "index_8: s13 + s16 + s18 + s21 + s23 + s25 + s27 + s34 + s36 + s38 + s40 - c8 >= 0\n",
      "index_9: s13 + s16 + s18 + s21 + s23 + s25 + s27 + s34 + s36 + s38 + s40 - c9 >= 0\n",
      "index_10: s11 + s12 + s13 + s14 + s16 + s17 + s18 + s19 + s20 + s21 + s22 + s23 + s24 + s25 + s32 + s34 + s35 + s36 + s37 + s38 + s39 + s40 + s45 - c10 >= 0\n",
      "index_11: s5 + s9 + s11 + s23 + s25 + s36 + s38 + s40 + s45 + s49 - c11 >= 0\n",
      "index_12: s5 + s9 + s11 + s23 + s25 + s36 + s38 + s40 + s45 + s49 - c12 >= 0\n",
      "index_13: s5 + s9 + s11 + s23 + s25 + s36 + s38 + s40 + s45 + s49 - c13 >= 0\n",
      "index_14: s5 + s9 + s11 + s23 + s25 + s36 + s38 + s40 + s45 + s49 - c14 >= 0\n",
      "index_15: s5 + s11 + s13 + s15 + s18 + s19 + s21 + s27 + s33 + s42 - c15 >= 0\n",
      "index_16: s23 + s38 + s40 - c16 >= 0\n",
      "index_17: s18 - c17 >= 0\n",
      "index_18: s21 + s36 + s40 - c18 >= 0\n",
      "index_19: s16 + s17 + s18 + s19 + s22 + s23 - c19 >= 0\n",
      "index_20: s11 + s18 + s19 + s21 + s34 + s40 + s42 - c20 >= 0\n",
      "index_21: s5 + s11 + s12 + s13 + s14 + s16 + s17 + s18 + s20 + s21 + s49 - c21 >= 0\n",
      "index_22: s13 + s32 + s34 + s39 + s40 - c22 >= 0\n",
      "index_23: s18 + s21 + s24 + s25 - c23 >= 0\n",
      "index_24: s0 + s2 + s7 + s11 + s13 + s15 + s18 + s19 + s21 + s23 + s27 + s33 + s38 + s42 + s45 + s47 + s52 - c24 >= 0\n",
      "index_25: s2 + s11 + s13 + s15 + s19 + s27 + s33 + s42 + s52 - c25 >= 0\n",
      "index_26: s11 + s21 + s25 + s34 + s36 + s38 + s40 + s45 + s47 - c26 >= 0\n",
      "index_27: s18 + s21 + s38 - c27 >= 0\n",
      "index_28: s27 + s34 + s35 + s36 + s40 - c28 >= 0\n",
      "index_29: s5 + s9 + s11 + s18 + s21 + s45 + s49 - c29 >= 0\n",
      "index_30: s2 + s5 + s9 + s13 + s14 + s16 + s18 + s21 + s23 + s25 + s26 + s27 + s32 + s34 + s36 + s37 + s38 + s39 + s40 + s49 - c30 >= 0\n",
      "index_31: s13 + s32 + s34 + s35 - c31 >= 0\n",
      "index_32: s11 + s18 + s19 + s42 + s45 - c32 >= 0\n",
      "index_33: s23 + s25 + s34 + s36 + s38 + s40 + s42 - c33 >= 0\n",
      "index_34: s13 + s16 + s18 + s19 + s21 + s23 + s25 + s27 + s34 + s36 + s38 + s40 + s45 + s47 + s52 - c34 >= 0\n",
      "index_35: s18 + s21 + s23 + s25 + s34 + s40 - c35 >= 0\n",
      "index_36: s7 + s11 + s12 + s14 + s16 + s17 + s18 + s19 + s20 + s21 + s22 + s25 + s27 + s28 + s30 + s34 + s36 + s40 + s44 + s45 + s49 + s52 - c36 >= 0\n",
      "index_37: s6 + s9 + s10 + s13 + s15 + s23 + s24 + s26 + s32 + s35 + s39 + s41 + s51 - c37 >= 0\n",
      "length: 1 s0 + 1 s1 + 1 s2 + 1 s3 + 1 s4 + 1 s5 + 1 s6 + 1 s7 + 1 s8 + 1 s9 + 1 s10 + 1 s11 + 1 s12 + 1 s13 + 1 s14 + 1 s15 + 1 s16 + 1 s17 + 1 s18 + 1 s19 + 1 s20 + 1 s21 + 1 s22 + 1 s23 + 1 s24 + 1 s25 + 1 s26 + 1 s27 + 1 s28 + 1 s29 + 1 s30 + 1 s31 + 1 s32 + 1 s33 + 1 s34 + 1 s35 + 1 s36 + 1 s37 + 1 s38 + 1 s39 + 1 s40 + 1 s41 + 1 s42 + 1 s43 + 1 s44 + 1 s45 + 1 s46 + 1 s47 + 1 s48 + 1 s49 + 1 s50 + 1 s51 + 1 s52 <= 2000\n",
      "\n",
      "\n",
      "Binary\n",
      "c0\n",
      "c1\n",
      "c2\n",
      "c3\n",
      "c4\n",
      "c5\n",
      "c6\n",
      "c7\n",
      "c8\n",
      "c9\n",
      "c10\n",
      "c11\n",
      "c12\n",
      "c13\n",
      "c14\n",
      "c15\n",
      "c16\n",
      "c17\n",
      "c18\n",
      "c19\n",
      "c20\n",
      "c21\n",
      "c22\n",
      "c23\n",
      "c24\n",
      "c25\n",
      "c26\n",
      "c27\n",
      "c28\n",
      "c29\n",
      "c30\n",
      "c31\n",
      "c32\n",
      "c33\n",
      "c34\n",
      "c35\n",
      "c36\n",
      "c37\n",
      "s0\n",
      "s1\n",
      "s2\n",
      "s3\n",
      "s4\n",
      "s5\n",
      "s6\n",
      "s7\n",
      "s8\n",
      "s9\n",
      "s10\n",
      "s11\n",
      "s12\n",
      "s13\n",
      "s14\n",
      "s15\n",
      "s16\n",
      "s17\n",
      "s18\n",
      "s19\n",
      "s20\n",
      "s21\n",
      "s22\n",
      "s23\n",
      "s24\n",
      "s25\n",
      "s26\n",
      "s27\n",
      "s28\n",
      "s29\n",
      "s30\n",
      "s31\n",
      "s32\n",
      "s33\n",
      "s34\n",
      "s35\n",
      "s36\n",
      "s37\n",
      "s38\n",
      "s39\n",
      "s40\n",
      "s41\n",
      "s42\n",
      "s43\n",
      "s44\n",
      "s45\n",
      "s46\n",
      "s47\n",
      "s48\n",
      "s49\n",
      "s50\n",
      "s51\n",
      "s52\n",
      "End\n"
     ]
    }
   ],
   "source": [
    "res = to_ilp_format(l, clabels, clusters_tf_values)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readILP(rel_path=\"myLibraries\\MARScore_output\\ilp_out.sol\"):\n",
    "    root = get_git_root()\n",
    "    path = os.path.join(root, rel_path)\n",
    "    with open(path, \"r\") as f:\n",
    "        raw = \"\".join(f.readlines())\n",
    "        f.close()\n",
    "\n",
    "    pattern = r's\\d+\\s+\\*\\s+(\\d)'\n",
    "\n",
    "    matches = re.findall(pattern, raw)\n",
    "    result = [int(match) for match in matches]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "res = readILP()\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
