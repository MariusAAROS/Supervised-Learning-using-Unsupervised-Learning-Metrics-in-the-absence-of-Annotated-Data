{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    for i in range(10):\n",
    "        yield i\n",
    "\n",
    "arr = test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = [1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr[0:len(arr)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERTScore (custom and classic) output format analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bert_score\n",
    "import sys\n",
    "sys.path.append(r\"D:\\COURS\\A4\\S8 - ESILV\\Stage\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\myLibraries\")\n",
    "from custom_score.score import score\n",
    "from custom_score.utils import model_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = model_load(\"Word2Vec\", True)\n",
    "custom_o = score(w2v, [\"I am Marius\"], [\"My name is Marius\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classic_o = bert_score.score([\"I am Marius\"], [\"My name is Marius\"],  lang=\"en\", verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classic_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classic_o[0].item()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERTScore multi-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classic_o = bert_score.score([\"I am Marius\", \"I like trains.\"], [\"My name is Marius\", \"I enjoy vehicules.\"],  lang=\"en\", verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classic_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classic_o[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = model_load(\"Word2Vec\", True)\n",
    "custom_o = score(w2v, [\"I am Marius\", \"I like trains.\"], [\"My name is Marius\", \"I enjoy vehicules.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_o"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sets experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.add(\"a\")\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.add(\"a\")\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = {1}\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset file test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data.myLibraries\")\n",
    "from datasets_loaders.loaders import load_billsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_billsum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset_loaders test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"D:\\COURS\\A4\\S8 - ESILV\\Stage\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\myLibraries\")\n",
    "from datasets_loaders.loaders import load_billsum\n",
    "from datasets_loaders.utils import cleanDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "billsum = load_billsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(billsum.iloc[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "billsum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanDataset(billsum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(billsum.iloc[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "billsum.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "# Get memory information\n",
    "memory_info = psutil.virtual_memory()\n",
    "\n",
    "# Total physical memory\n",
    "total_memory = memory_info.total\n",
    "\n",
    "# Available memory\n",
    "available_memory = memory_info.available\n",
    "\n",
    "# Used memory\n",
    "used_memory = memory_info.used\n",
    "\n",
    "# Memory usage percentage\n",
    "memory_percent = memory_info.percent\n",
    "\n",
    "# Maximum memory that can be consumed by Python\n",
    "max_memory = memory_info.available\n",
    "\n",
    "print(\"Total memory:\", total_memory)\n",
    "print(\"Available memory:\", available_memory)\n",
    "print(\"Used memory:\", used_memory)\n",
    "print(\"Memory usage percentage:\", memory_percent)\n",
    "print(\"Maximum memory:\", max_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\myLibraries\")\n",
    "from datasets_loaders.loaders import load_billsum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "billsum = load_billsum()\n",
    "subset = billsum.iloc[:20, :]\n",
    "subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pytextrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"textrank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_rank_result = []\n",
    "for indiv in subset[\"text\"]:\n",
    "    doc = nlp(indiv)\n",
    "    text_rank_result.append(doc._.phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_rank_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mauve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mauve\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATA_URL = \"https://openaipublic.azureedge.net/gpt-2/output-dataset/v1-amazonfinetune/\"\n",
    "subdir = r'C:\\Pro\\Stages\\A4 - DVRC\\Work\\Datasets\\GPT2'\n",
    "\n",
    "\n",
    "if not os.path.exists(subdir):\n",
    "    os.makedirs(subdir)\n",
    "subdir = subdir.replace('\\\\','/') # needed for Windows\n",
    "\n",
    "for ds in [\n",
    "    'amazon',                   # human-written data\n",
    "    'amazon-xl-1542M',          # machine generation: sampled from GPT-2 XL\n",
    "]:\n",
    "    for split in ['valid']:\n",
    "        filename = ds + \".\" + split + '.jsonl'\n",
    "        r = requests.get(DATA_URL + filename, stream=True)\n",
    "\n",
    "        with open(os.path.join(subdir, filename), 'wb') as f:\n",
    "            file_size = int(r.headers[\"content-length\"])\n",
    "            chunk_size = 1000\n",
    "            with tqdm(ncols=100, desc=\"Fetching \" + filename, total=file_size, unit_scale=True) as pbar:\n",
    "                # 1k for chunk_size, since Ethernet packet size is around 1500 bytes\n",
    "                for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gpt2_dataset(json_file_name, num_examples=float('inf')):\n",
    "    texts = []\n",
    "    for i, line in enumerate(open(json_file_name)):\n",
    "        if i >= num_examples:\n",
    "            break\n",
    "        texts.append(json.loads(line)['text'])\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_text = load_gpt2_dataset(r'C:\\Pro\\Stages\\A4 - DVRC\\Work\\Datasets\\GPT2\\amazon.valid.jsonl'.replace(\"\\\\\", \"/\"), num_examples=100) # human\n",
    "q_text = load_gpt2_dataset(r'C:\\Pro\\Stages\\A4 - DVRC\\Work\\Datasets\\GPT2\\amazon-xl-1542M.valid.jsonl'.replace(\"\\\\\", \"/\"), num_examples=100) # machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = mauve.compute_mauve(p_text=p_text, q_text=q_text, device_id=0)\n",
    "print(out.mauve) # prints 0.9917"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = mauve.compute_mauve(p_text=subset[\"text\"].tolist(), q_text=subset[\"summary\"].tolist(), device_id=0)\n",
    "print(out.mauve) # prints 0.9917"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap = AffinityPropagation(random_state=0)\n",
    "\".\".join(ap.__module__.split(\".\")[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pytextrank\n",
    "\n",
    "# example text\n",
    "text = \"Compatibility of systems of linear constraints over the set of natural numbers. Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered. Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given. These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types systems and systems of mixed types.\"\n",
    "\n",
    "# load a spaCy model, depending on language, scale, etc.\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# add PyTextRank to the spaCy pipeline\n",
    "nlp.add_pipe(\"textrank\")\n",
    "doc = nlp(text)\n",
    "\n",
    "# examine the top-ranked phrases in the document\n",
    "for phrase in doc._.phrases:\n",
    "    print(phrase.text)\n",
    "    print(phrase.rank, phrase.count)\n",
    "    print(phrase.chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textrank_summarize(nlp, text, num_sentences=3):\n",
    "    # Process the input text using the spacy pipeline to get individual sentences\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Get the sentences and their TextRank scores\n",
    "    sentences = [sent.text.strip() for sent in doc.sents]\n",
    "    ranks = doc._.textrank_scores\n",
    "\n",
    "    # Sort sentences based on TextRank scores\n",
    "    ranked_sentences = sorted(sentences, key=lambda x: ranks[x], reverse=True)\n",
    "\n",
    "    # Select the top 'num_sentences' sentences for the summary\n",
    "    summary_sentences = ranked_sentences[:num_sentences]\n",
    "\n",
    "    # Join the selected sentences to form the summary\n",
    "    summary = \" \".join(summary_sentences)\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Compatibility of systems of linear constraints over the set of natural numbers. Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered. Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given. These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types systems and systems of mixed types.\"\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"textrank\")\n",
    "\n",
    "textrank_summarize(nlp, text, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from collections import Counter\n",
    "from pytextrank import TextRank\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Create a TextRank instance and add it to the pipeline\n",
    "textrank = TextRank()\n",
    "nlp.add_pipe(textrank.PipelineComponent, name=\"textrank\", last=True)\n",
    "\n",
    "def textrank_summarize(nlp, text, num_sentences=3):\n",
    "    # Process the input text using the spacy pipeline to get individual sentences\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Get the sentences and their TextRank scores\n",
    "    sentences = [sent.text.strip() for sent in doc.sents]\n",
    "    ranks = [sent._.tr_score for sent in doc.sents]\n",
    "\n",
    "    # Sort sentences based on TextRank scores\n",
    "    ranked_sentences = [sent for _, sent in sorted(zip(ranks, sentences), reverse=True)]\n",
    "\n",
    "    # Select the top 'num_sentences' sentences for the summary\n",
    "    summary_sentences = ranked_sentences[:num_sentences]\n",
    "\n",
    "    # Join the selected sentences to form the summary\n",
    "    summary = \" \".join(summary_sentences)\n",
    "\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extractive summarization testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pytextrank\n",
    "from icecream import ic\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"Compatibility of systems of linear constraints over the set of natural numbers. Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered. Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given. These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types systems and systems of mixed types.\"\n",
    "nlp.add_pipe(\"textrank\", last=True)\n",
    "doc = nlp(text)\n",
    "\n",
    "sent_bounds = [ [s.start, s.end, set([])] for s in doc.sents ]\n",
    "sent_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_phrases = 4\n",
    "\n",
    "phrase_id = 0\n",
    "unit_vector = []\n",
    "\n",
    "for p in doc._.phrases:\n",
    "    ic(phrase_id, p.text, p.rank)\n",
    "\n",
    "    unit_vector.append(p.rank)\n",
    "\n",
    "    for chunk in p.chunks:\n",
    "        ic(chunk.start, chunk.end)\n",
    "\n",
    "        for sent_start, sent_end, sent_vector in sent_bounds:\n",
    "            if chunk.start >= sent_start and chunk.end <= sent_end:\n",
    "                ic(sent_start, chunk.start, chunk.end, sent_end)\n",
    "                sent_vector.add(phrase_id)\n",
    "                break\n",
    "\n",
    "    phrase_id += 1\n",
    "\n",
    "    if phrase_id == limit_phrases:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in doc.sents:\n",
    "    ic(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_ranks = sum(unit_vector)\n",
    "\n",
    "unit_vector = [ rank/sum_ranks for rank in unit_vector ]\n",
    "unit_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "sent_rank = {}\n",
    "sent_id = 0\n",
    "\n",
    "for sent_start, sent_end, sent_vector in sent_bounds:\n",
    "    ic(sent_vector)\n",
    "    sum_sq = 0.0\n",
    "    ic\n",
    "    for phrase_id in range(len(unit_vector)):\n",
    "        ic(phrase_id, unit_vector[phrase_id])\n",
    "\n",
    "        if phrase_id not in sent_vector:\n",
    "            sum_sq += unit_vector[phrase_id]**2.0\n",
    "\n",
    "    sent_rank[sent_id] = sqrt(sum_sq)\n",
    "    sent_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic(sent_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "sorted(sent_rank.items(), key=itemgetter(1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_sentences = 2\n",
    "\n",
    "sent_text = {}\n",
    "sent_id = 0\n",
    "\n",
    "for sent in doc.sents:\n",
    "    sent_text[sent_id] = sent.text\n",
    "    sent_id += 1\n",
    "\n",
    "num_sent = 0\n",
    "\n",
    "for sent_id, rank in sorted(sent_rank.items(), key=itemgetter(1)):\n",
    "    ic(sent_id, sent_text[sent_id])\n",
    "    num_sent += 1\n",
    "\n",
    "    if num_sent == limit_sentences:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_text[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extractive summarization model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_rank(corpus, limit_phrases=4, limit_sentences=2):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    nlp.add_pipe(\"textrank\", last=True)\n",
    "    summaries = []\n",
    "\n",
    "    for text in corpus:\n",
    "        doc = nlp(text)\n",
    "        sent_bounds = [[s.start, s.end, set([])] for s in doc.sents]\n",
    "\n",
    "        phrase_id = 0\n",
    "        unit_vector = []\n",
    "\n",
    "        for p in doc._.phrases:\n",
    "            unit_vector.append(p.rank)\n",
    "            for chunk in p.chunks:\n",
    "                for sent_start, sent_end, sent_vector in sent_bounds:\n",
    "                    if chunk.start >= sent_start and chunk.end <= sent_end:\n",
    "                        sent_vector.add(phrase_id)\n",
    "                        break\n",
    "            phrase_id += 1\n",
    "            if phrase_id == limit_phrases:\n",
    "                break\n",
    "        \n",
    "        sum_ranks = sum(unit_vector)\n",
    "        unit_vector = [rank/sum_ranks for rank in unit_vector]\n",
    "        sent_rank = {}\n",
    "        sent_id = 0\n",
    "\n",
    "        for sent_start, sent_end, sent_vector in sent_bounds:\n",
    "            sum_sq = 0.0\n",
    "            for phrase_id in range(len(unit_vector)):\n",
    "                if phrase_id not in sent_vector:\n",
    "                    sum_sq += unit_vector[phrase_id]**2.0\n",
    "\n",
    "            sent_rank[sent_id] = sqrt(sum_sq)\n",
    "            sent_id += 1\n",
    "\n",
    "        sent_text = {}\n",
    "        sent_id = 0\n",
    "\n",
    "        for sent in doc.sents:\n",
    "            sent_text[sent_id] = sent.text\n",
    "            sent_id += 1\n",
    "\n",
    "        num_sent = 0\n",
    "\n",
    "        result = \"\"\n",
    "        for sent_id, rank in sorted(sent_rank.items(), key=itemgetter(1)):\n",
    "            result += sent_text[sent_id] + \" \"\n",
    "            num_sent += 1\n",
    "            if num_sent == limit_sentences:\n",
    "                break\n",
    "        summaries.append(result[:-1])\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_rank([text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Library testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\myLibraries\")\n",
    "from TextRanker.textrank import TextRanker\n",
    "from custom_score.utils import cleanString\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_json(r'C:\\Pro\\Stages\\A4 - DVRC\\Work\\Datasets\\pubmed\\test.json', lines=True)\n",
    "dataset = dataset[[\"article_text\", \"abstract_text\"]]\n",
    "cleaner = lambda x: \". \".join(x).replace(\"<S>\", \"\").strip()\n",
    "format_dot = lambda x: x.replace(\" .\", \".\")\n",
    "dataset.loc[:,\"abstract_text\"] = dataset[\"abstract_text\"].replace(regex=r\"\\[[^\\]]*\\]\", value=\"\")\n",
    "dataset.loc[:,\"article_text\"] = dataset[\"article_text\"].replace(regex=r\"\\[[^\\]]*\\]\", value=\"\")\n",
    "dataset.loc[:,\"abstract_text\"] = dataset[\"abstract_text\"].map(cleaner)\n",
    "dataset.loc[:,\"article_text\"] = dataset[\"article_text\"].map(cleaner)\n",
    "dataset.loc[:,\"abstract_text\"] = dataset[\"abstract_text\"].map(cleanString)\n",
    "dataset.loc[:,\"article_text\"] = dataset[\"article_text\"].map(cleanString)\n",
    "dataset.loc[:,\"abstract_text\"] = dataset[\"abstract_text\"].map(format_dot)\n",
    "dataset.loc[:,\"article_text\"] = dataset[\"article_text\"].map(format_dot)\n",
    "dataset = dataset.rename(columns={\"abstract_text\": \"summary\",\n",
    "                        \"article_text\": \"text\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = dataset.iloc[:2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = TextRanker(subset[\"text\"].to_list(), subset[\"summary\"].to_list())\n",
    "tr.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['although the neurobiology of anxiety in pd remains unknown , many researchers have postulated that anxiety disorders are related to neurochemical changes that occur during the early , premotor stages of pd related degeneration [ 37 , 38 ] such as nigrostriatal dopamine depletion , as well as cell loss within serotonergic and noradrenergic brainstem nuclei ( i.e. , raphe nuclei and locus coeruleus , resp. , which provide massive inputs to corticolimbic regions ). overall , very few neuroimaging studies have been conducted in pd in order to understand the neural correlates of pd anxiety and its underlying neural pathology.',\n",
       " 'enforced mir128 expression increased the sensitivity of breast cancer cells to doxorubicin induced apoptosis and dna damage. a specific subset of dysregulated mirnas in breast cancer cells may serve as targets for gene therapy either alone or as an adjuvant treatment to current clinical protocols for breast cancer patients.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr.summaries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
