{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\umap\\distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\umap\\distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\umap\\distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\umap\\umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\myLibraries\")\n",
    "\n",
    "from MARScore.utils import * \n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "import hdbscan\n",
    "from sklearn.metrics import make_scorer\n",
    "from random import seed\n",
    "from datasets_loaders.loaders import load_billsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "billsum = load_billsum()\n",
    "subset = billsum.iloc[:200, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DBCV(model, X, y=None):\n",
    "    preds = model.fit_predict(X)\n",
    "    return hdbscan.validity.validity_index(X, preds) if len(set(preds)) > 1 else float('nan')\n",
    "\n",
    "def HDBScanFinetune(vectors, \n",
    "                    min_samples=[10,30,50,60,100], \n",
    "                    min_cluster_size=[100,200,300,400,500,600],\n",
    "                    cluster_selection_method=['eom','leaf'],\n",
    "                    seed_num=0, \n",
    "                    verbose=True):\n",
    "    \n",
    "    #model setup\n",
    "    hdb = hdbscan.HDBSCAN(gen_min_span_tree=True)\n",
    "\n",
    "    # specify parameters and distributions to sample from\n",
    "    param_dist = {'min_samples': min_samples,\n",
    "                  'min_cluster_size': min_cluster_size,  \n",
    "                  'cluster_selection_method': cluster_selection_method\n",
    "                 }\n",
    "\n",
    "    #validity_scroer = \"hdbscan__hdbscan___HDBSCAN__validity_index\"\n",
    "    #validity_scorer = make_scorer(DBCV, greater_is_better=True)\n",
    "\n",
    "    #parameters research\n",
    "    \"\"\"\n",
    "    random_search = GridSearchCV(hdb,\n",
    "                                 param_grid=param_dist,\n",
    "                                 scoring=validity_scorer)\n",
    "    \"\"\"\n",
    "    \n",
    "    n_iter_search = 2\n",
    "    random_search = RandomizedSearchCV(hdb,\n",
    "                                       param_distributions=param_dist,\n",
    "                                       n_iter=n_iter_search,\n",
    "                                       scoring=DBCV,\n",
    "                                       random_state=seed(seed_num))\n",
    "    \n",
    "    random_search.fit(vectors)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Best Parameters {random_search.best_params_}\")\n",
    "        print(f\"DBCV score :{random_search.best_estimator_.relative_validity_}\")\n",
    "    return {\"best_params\": random_search.best_params_, \"dbcv_score\":random_search.best_estimator_.relative_validity_}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def to_shape(a, shape):\n",
    "    x_, y_, z_ = shape\n",
    "    x, y, z = len(a), len(a[0]), len(a[0][0])\n",
    "    x_pad = (x_-x)\n",
    "    y_pad = (y_-y)\n",
    "    z_pad = (z_-z)\n",
    "    return np.pad(a,((0, x_pad),\n",
    "                     (0, y_pad),\n",
    "                     (0, z_pad)),\n",
    "                  mode = 'constant')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_shape(a, shape):\n",
    "    x_, y_ = shape\n",
    "    x, y = len(a), len(a[0])\n",
    "    x_pad = (x_-x)\n",
    "    y_pad = (y_-y)\n",
    "    return np.pad(a,((0, x_pad),\n",
    "                     (0, y_pad)),\n",
    "                  mode = 'constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creation of embeddings\n",
    "all_v = []\n",
    "for indiv in subset[\"text\"].to_list():\n",
    "    o, l = tokenizeCorpus(indiv)\n",
    "    v = vectorizeCorpus(o)\n",
    "    v, l = cleanAll(v, l)\n",
    "    all_v.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_dim_1 = np.max([len(x) for x in all_v])\n",
    "all_v2 = np.array([to_shape(cur_v, (max_dim_1, len(all_v[0][0]))) for cur_v in all_v])\n",
    "s = all_v2.shape\n",
    "all_v2 = all_v2.reshape((s[0], s[1]*s[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(all_v[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_v2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HDBScanFinetune(all_v2,\n",
    "                min_samples=[1, 3, 5, 10],\n",
    "                min_cluster_size=[1, 5, 10, 20],\n",
    "                cluster_selection_method=['eom','leaf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_dim_1 = np.max([len(x) for x in all_v])\n",
    "all_v3 = [to_shape(cur_v, (max_dim_1, len(all_v[0][0]))) for cur_v in all_v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6898272642990181"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DBCV(hdbscan.HDBSCAN(), all_v3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters {'min_samples': 3, 'min_cluster_size': 7, 'cluster_selection_method': 'leaf'}\n",
      "DBCV score :0.67558658664357\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_params': {'min_samples': 3,\n",
       "  'min_cluster_size': 7,\n",
       "  'cluster_selection_method': 'leaf'},\n",
       " 'dbcv_score': 0.67558658664357}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HDBScanFinetune(all_v3[0],\n",
    "                min_samples=[1, 3, 5, 7],\n",
    "                min_cluster_size=[7, 10, 20],\n",
    "                cluster_selection_method=['eom','leaf'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
