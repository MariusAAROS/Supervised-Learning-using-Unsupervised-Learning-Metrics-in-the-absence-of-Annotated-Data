{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\umap\\distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\umap\\distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\umap\\distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\umap\\umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\myLibraries\")\n",
    "\n",
    "from MARScore.utils import * \n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "import hdbscan\n",
    "from sklearn.metrics import make_scorer\n",
    "from random import seed\n",
    "from datasets_loaders.loaders import load_billsum\n",
    "import numbers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "billsum = load_billsum()\n",
    "subset = billsum.iloc[:10, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DBCV(model, X, y=None):\n",
    "    preds = model.fit_predict(X)\n",
    "    return hdbscan.validity.validity_index(X, preds) if len(set(preds)) > 1 else float('nan')\n",
    "\n",
    "def DBCV2(model, X, y=None):\n",
    "    preds = [model.fit_predict(x) for x in X]\n",
    "    score = np.mean(hdbscan.validity.validity_index(x, pred) for x, pred in zip(X, preds))\n",
    "    return score if score != 0 else float('nan')\n",
    "\n",
    "def HDBScanFinetune(v_texts, \n",
    "                    min_samples=[10,30,50,60,100], \n",
    "                    min_cluster_size=[100,200,300,400,500,600],\n",
    "                    cluster_selection_method=['eom','leaf'],\n",
    "                    seed_num=0, \n",
    "                    verbose=True):\n",
    "    \n",
    "    #model setup\n",
    "    hdb = hdbscan.HDBSCAN(gen_min_span_tree=True)\n",
    "\n",
    "    # specify parameters and distributions to sample from\n",
    "    param_dist = {'min_samples': min_samples,\n",
    "                  'min_cluster_size': min_cluster_size,  \n",
    "                  'cluster_selection_method': cluster_selection_method\n",
    "                 }\n",
    "\n",
    "    #validity_scroer = \"hdbscan__hdbscan___HDBSCAN__validity_index\"\n",
    "    #validity_scorer = make_scorer(DBCV, greater_is_better=True)\n",
    "\n",
    "    #parameters research\n",
    "    \"\"\"\n",
    "    random_search = GridSearchCV(hdb,\n",
    "                                 param_grid=param_dist,\n",
    "                                 scoring=validity_scorer)\n",
    "    \"\"\"\n",
    "    \n",
    "    n_iter_search = 2\n",
    "    random_search = RandomizedSearchCV(hdb,\n",
    "                                       param_distributions=param_dist,\n",
    "                                       n_iter=n_iter_search,\n",
    "                                       scoring=DBCV,\n",
    "                                       random_state=seed(seed_num))\n",
    "    best_params = {}\n",
    "    best_scores = []\n",
    "    first_lap = True\n",
    "    for vectors in v_texts:\n",
    "        random_search.fit(vectors)\n",
    "        if first_lap:\n",
    "            for k in random_search.best_params_.keys():\n",
    "                best_params[k] = []\n",
    "            first_lap = False\n",
    "        for k, v in random_search.best_params_.items():\n",
    "            best_params[k].append(v)\n",
    "        best_scores.append(random_search.best_estimator_.relative_validity_)\n",
    "    \n",
    "    final_params = {k: None for k in best_params.keys()}\n",
    "    final_score = np.mean(best_scores)\n",
    "    for k, v in best_params.items():\n",
    "        if type(best_params[k][0]) == str:\n",
    "            final_params[k] = max(set(best_params[k]), key = best_params[k].count)\n",
    "        elif isinstance(best_params[k][0], numbers.Number):\n",
    "            final_params[k] = np.mean(best_params[k])\n",
    "        else:\n",
    "            final_params[k] = best_params[k][0]    \n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Best Parameters {final_params}\")\n",
    "        print(f\"DBCV score :{final_score}\")\n",
    "    return {\"best_params\": final_params, \"dbcv_score\": final_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_shape(a, shape):\n",
    "    x_, y_ = shape\n",
    "    x, y = len(a), len(a[0])\n",
    "    x_pad = (x_-x)\n",
    "    y_pad = (y_-y)\n",
    "    return np.pad(a,((0, x_pad),\n",
    "                    (0, y_pad)),\n",
    "                mode = 'constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (699 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "#creation of embeddings\n",
    "all_v = []\n",
    "for indiv in subset[\"text\"].to_list():\n",
    "    o, l = tokenizeCorpus(indiv)\n",
    "    v = vectorizeCorpus(o)\n",
    "    v, l = cleanAll(v, l)\n",
    "    all_v.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_dim_1 = np.max([len(x) for x in all_v])\n",
    "all_v3 = [to_shape(cur_v, (max_dim_1, len(all_v[0][0]))) for cur_v in all_v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DBCV2(hdbscan.HDBSCAN(), all_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [nan nan]\n",
      "  warnings.warn(\n",
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [nan nan]\n",
      "  warnings.warn(\n",
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [nan nan]\n",
      "  warnings.warn(\n",
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [nan nan]\n",
      "  warnings.warn(\n",
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [nan nan]\n",
      "  warnings.warn(\n",
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [nan nan]\n",
      "  warnings.warn(\n",
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [nan nan]\n",
      "  warnings.warn(\n",
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [nan nan]\n",
      "  warnings.warn(\n",
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters {'min_samples': 5.6, 'min_cluster_size': 11.4, 'cluster_selection_method': 'leaf'}\n",
      "DBCV score :0.227490788949255\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_params': {'min_samples': 5.6,\n",
       "  'min_cluster_size': 11.4,\n",
       "  'cluster_selection_method': 'leaf'},\n",
       " 'dbcv_score': 0.227490788949255}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HDBScanFinetune(all_v3,\n",
    "                min_samples=[3, 5, 7],\n",
    "                min_cluster_size=[7, 10, 20],\n",
    "                cluster_selection_method=['eom','leaf'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\myLibraries\")\n",
    "\n",
    "from MARScore.utils import * \n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "import hdbscan\n",
    "from sklearn.metrics import make_scorer\n",
    "from random import seed, randint\n",
    "from datasets_loaders.loaders import load_billsum\n",
    "import numbers\n",
    "import numpy as np\n",
    "from datetime import timedelta, datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "billsum = load_billsum()\n",
    "subset = billsum.iloc[:50, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DBCV(model, X, y=None):\n",
    "    preds = model.fit_predict(X)\n",
    "    return hdbscan.validity.validity_index(X, preds) if len(set(preds)) > 1 else float('nan')\n",
    "\n",
    "def HDBScanFinetune(v_texts, \n",
    "                    start = 1,\n",
    "                    stop = 3,\n",
    "                    epsilon = 1,\n",
    "                    epsilon_reduction_factor = 0.3,\n",
    "                    delta = 0.5,\n",
    "                    n_elements = 4,\n",
    "                    n_iter_search = 2,\n",
    "                    early_stopping = 3,\n",
    "                    seed_num=0,\n",
    "                    max_compile_time = timedelta(hours=2),\n",
    "                    verbose=True):\n",
    "    \n",
    "    \n",
    "    #params\n",
    "    first_iter = True\n",
    "    stable = 0\n",
    "    start_time = datetime.now()\n",
    "    compile_time = datetime.now() - start_time\n",
    "\n",
    "    #model setup\n",
    "    hdb = hdbscan.HDBSCAN(gen_min_span_tree=True)\n",
    "    \n",
    "    #tuning loop\n",
    "    while((start + delta) < stop or compile_time >= max_compile_time):\n",
    "        #search\n",
    "        min_samples = np.rint(np.linspace(np.exp(start), np.exp(stop), n_elements)).astype(int)\n",
    "        min_cluster_size = np.rint(np.linspace(np.exp(start), np.exp(stop), n_elements)).astype(int)\n",
    "        cluster_selection_method = ['eom', 'leaf']\n",
    "\n",
    "\n",
    "        #specify parameters and distributions to sample from\n",
    "        param_dist = {'min_samples': min_samples,\n",
    "                      'min_cluster_size': min_cluster_size,\n",
    "                      'cluster_selection_method': cluster_selection_method\n",
    "                     }\n",
    "\n",
    "        random_search = RandomizedSearchCV(hdb,\n",
    "                                       param_distributions=param_dist,\n",
    "                                       n_iter=n_iter_search,\n",
    "                                       scoring=DBCV,\n",
    "                                       random_state=seed(seed_num))\n",
    "        best_params = {}\n",
    "        best_scores = []\n",
    "        first_lap = True\n",
    "        better = False\n",
    "        for vectors in v_texts:\n",
    "            random_search.fit(vectors)\n",
    "            if first_lap:\n",
    "                for k in random_search.best_params_.keys():\n",
    "                    best_params[k] = []\n",
    "                first_lap = False\n",
    "            for k, v in random_search.best_params_.items():\n",
    "                best_params[k].append(v)\n",
    "            best_scores.append(random_search.best_estimator_.relative_validity_)\n",
    "        \n",
    "        cur_params = {k: None for k in best_params.keys()}\n",
    "        cur_score = np.mean(best_scores)\n",
    "        for k, v in best_params.items():\n",
    "            if type(best_params[k][0]) == str:\n",
    "                cur_params[k] = max(set(best_params[k]), key = best_params[k].count)\n",
    "            elif isinstance(best_params[k][0], numbers.Number):\n",
    "                cur_params[k] = round(np.mean(best_params[k]))\n",
    "            else:\n",
    "                cur_params[k] = best_params[k][0]\n",
    "        if first_iter:\n",
    "            global_params = cur_params\n",
    "            global_score = cur_score\n",
    "            first_iter = False\n",
    "        elif cur_score > global_score:\n",
    "            global_params = cur_params\n",
    "            global_score = cur_score\n",
    "            better = True\n",
    "\n",
    "        #adapting ranges\n",
    "        if better:\n",
    "            for k, v in cur_params.items():\n",
    "                if cur_params[k] == locals()[k][0]:\n",
    "                    start -= epsilon\n",
    "                    stop -= epsilon\n",
    "                elif cur_params[k] == locals()[k][-1]:\n",
    "                    start += epsilon\n",
    "                    stop += epsilon\n",
    "                else:\n",
    "                    side = np.round(randint(0, 1))\n",
    "                    if side:\n",
    "                        stop -= epsilon\n",
    "                    else:\n",
    "                        start += epsilon\n",
    "                    epsilon -= epsilon*epsilon_reduction_factor\n",
    "        elif not(better) and not(first_iter):\n",
    "            stable += 1\n",
    "        if stable >= early_stopping:\n",
    "            break\n",
    "        compile_time = datetime.now() - start_time\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Best Parameters {global_params}\")\n",
    "        print(f\"DBCV score :{global_score}\")\n",
    "    return {\"best_params\": global_params, \"dbcv_score\": global_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_shape(a, shape):\n",
    "    x_, y_ = shape\n",
    "    x, y = len(a), len(a[0])\n",
    "    x_pad = (x_-x)\n",
    "    y_pad = (y_-y)\n",
    "    return np.pad(a,((0, x_pad),\n",
    "                    (0, y_pad)),\n",
    "                mode = 'constant')\n",
    "\n",
    "#creation of embeddings\n",
    "all_v = []\n",
    "for indiv in subset[\"text\"].to_list():\n",
    "    o, l = tokenizeCorpus(indiv)\n",
    "    v = vectorizeCorpus(o)\n",
    "    v, l = cleanAll(v, l)\n",
    "    all_v.append(v)\n",
    "\n",
    "max_dim_1 = np.max([len(x) for x in all_v])\n",
    "all_v3 = [to_shape(cur_v, (max_dim_1, len(all_v[0][0]))) for cur_v in all_v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HDBScanFinetune(all_v3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
