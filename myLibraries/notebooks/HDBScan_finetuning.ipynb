{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\myLibraries\")\n",
    "\n",
    "from MARScore.utils import * \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import hdbscan\n",
    "from sklearn.metrics import make_scorer\n",
    "from random import seed\n",
    "from datasets_loaders.loaders import load_billsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "billsum = load_billsum()\n",
    "subset = billsum.iloc[:300, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HDBScanFinetune(corpus, \n",
    "                    min_samples=[10,30,50,60,100], \n",
    "                    min_cluster_size=[100,200,300,400,500,600],\n",
    "                    cluster_selection_method=['eom','leaf'],\n",
    "                    metric=['euclidean','manhattan'],\n",
    "                    seed=seed(0), \n",
    "                    verbose=True):\n",
    "    #creation of embeddings\n",
    "    for indiv in corpus:\n",
    "        o, l = tokenizeCorpus(indiv)\n",
    "        v = vectorizeCorpus(o)\n",
    "        v, l = cleanAll(v, l)\n",
    "    \n",
    "    #finetuning\n",
    "    hdb = hdbscan.HDBSCAN(gen_min_span_tree=True).fit(v)\n",
    "\n",
    "    # specify parameters and distributions to sample from\n",
    "    param_dist = {'min_samples': min_samples,\n",
    "                'min_cluster_size':min_cluster_size,  \n",
    "                'cluster_selection_method':cluster_selection_method,\n",
    "                'metric':metric  \n",
    "                }\n",
    "\n",
    "    #validity_scroer = \"hdbscan__hdbscan___HDBSCAN__validity_index\"\n",
    "    validity_scorer = make_scorer(hdbscan.validity.validity_index,greater_is_better=True)\n",
    "\n",
    "\n",
    "    n_iter_search = 20\n",
    "    random_search = RandomizedSearchCV(hdb\n",
    "                                    ,param_distributions=param_dist\n",
    "                                    ,n_iter=n_iter_search\n",
    "                                    ,scoring=validity_scorer\n",
    "                                    ,random_state=seed)\n",
    "    random_search.fit(v)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Best Parameters {random_search.best_params_}\")\n",
    "        print(f\"DBCV score :{random_search.best_estimator_.relative_validity_}\")\n",
    "    return {\"best_params\": random_search.best_params_, \"dbcv_score\":random_search.best_estimator_.relative_validity_}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HDBScanFinetune(billsum[\"text\"].to_list())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
