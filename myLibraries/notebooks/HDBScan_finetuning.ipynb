{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\umap\\distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\umap\\distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\umap\\distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\umap\\umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\myLibraries\")\n",
    "\n",
    "from MARScore.utils import * \n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "import hdbscan\n",
    "from sklearn.metrics import make_scorer\n",
    "from random import seed\n",
    "from datasets_loaders.loaders import load_billsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "billsum = load_billsum()\n",
    "subset = billsum.iloc[:200, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DBCV(model, X, y=None):\n",
    "    preds = model.fit_predict(X)\n",
    "    return hdbscan.validity.validity_index(X, preds) if len(set(preds)) > 1 else float('nan')\n",
    "\n",
    "def DBCV2(model, X, y=None):\n",
    "    preds = [model.fit_predict(x) for x in X]\n",
    "    score = np.mean(hdbscan.validity.validity_index(x, pred) for x, pred in zip(X, preds))\n",
    "    return score if score != 0 else float('nan')\n",
    "\n",
    "def HDBScanFinetune(vectors, \n",
    "                    min_samples=[10,30,50,60,100], \n",
    "                    min_cluster_size=[100,200,300,400,500,600],\n",
    "                    cluster_selection_method=['eom','leaf'],\n",
    "                    seed_num=0, \n",
    "                    verbose=True):\n",
    "    \n",
    "    #model setup\n",
    "    hdb = hdbscan.HDBSCAN(gen_min_span_tree=True)\n",
    "\n",
    "    # specify parameters and distributions to sample from\n",
    "    param_dist = {'min_samples': min_samples,\n",
    "                  'min_cluster_size': min_cluster_size,  \n",
    "                  'cluster_selection_method': cluster_selection_method\n",
    "                 }\n",
    "\n",
    "    #validity_scroer = \"hdbscan__hdbscan___HDBSCAN__validity_index\"\n",
    "    #validity_scorer = make_scorer(DBCV, greater_is_better=True)\n",
    "\n",
    "    #parameters research\n",
    "    \"\"\"\n",
    "    random_search = GridSearchCV(hdb,\n",
    "                                 param_grid=param_dist,\n",
    "                                 scoring=validity_scorer)\n",
    "    \"\"\"\n",
    "    \n",
    "    n_iter_search = 2\n",
    "    random_search = RandomizedSearchCV(hdb,\n",
    "                                       param_distributions=param_dist,\n",
    "                                       n_iter=n_iter_search,\n",
    "                                       scoring=DBCV,\n",
    "                                       random_state=seed(seed_num))\n",
    "    \n",
    "    random_search.fit(vectors)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Best Parameters {random_search.best_params_}\")\n",
    "        print(f\"DBCV score :{random_search.best_estimator_.relative_validity_}\")\n",
    "    return {\"best_params\": random_search.best_params_, \"dbcv_score\":random_search.best_estimator_.relative_validity_}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def to_shape(a, shape):\n",
    "    x_, y_, z_ = shape\n",
    "    x, y, z = len(a), len(a[0]), len(a[0][0])\n",
    "    x_pad = (x_-x)\n",
    "    y_pad = (y_-y)\n",
    "    z_pad = (z_-z)\n",
    "    return np.pad(a,((0, x_pad),\n",
    "                     (0, y_pad),\n",
    "                     (0, z_pad)),\n",
    "                  mode = 'constant')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_shape(a, shape):\n",
    "    x_, y_ = shape\n",
    "    x, y = len(a), len(a[0])\n",
    "    x_pad = (x_-x)\n",
    "    y_pad = (y_-y)\n",
    "    return np.pad(a,((0, x_pad),\n",
    "                     (0, y_pad)),\n",
    "                  mode = 'constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creation of embeddings\n",
    "all_v = []\n",
    "for indiv in subset[\"text\"].to_list():\n",
    "    o, l = tokenizeCorpus(indiv)\n",
    "    v = vectorizeCorpus(o)\n",
    "    v, l = cleanAll(v, l)\n",
    "    all_v.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_dim_1 = np.max([len(x) for x in all_v])\n",
    "all_v2 = np.array([to_shape(cur_v, (max_dim_1, len(all_v[0][0]))) for cur_v in all_v])\n",
    "s = all_v2.shape\n",
    "all_v2 = all_v2.reshape((s[0], s[1]*s[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(all_v[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_v2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HDBScanFinetune(all_v2,\n",
    "                min_samples=[1, 3, 5, 10],\n",
    "                min_cluster_size=[1, 5, 10, 20],\n",
    "                cluster_selection_method=['eom','leaf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_dim_1 = np.max([len(x) for x in all_v])\n",
    "all_v3 = [to_shape(cur_v, (max_dim_1, len(all_v[0][0]))) for cur_v in all_v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6898272642990181"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DBCV(hdbscan.HDBSCAN(), all_v3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'generator' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m DBCV2(hdbscan\u001b[39m.\u001b[39;49mHDBSCAN(), all_v3)\n",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m, in \u001b[0;36mDBCV2\u001b[1;34m(model, X, y)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mDBCV2\u001b[39m(model, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m      6\u001b[0m     preds \u001b[39m=\u001b[39m [model\u001b[39m.\u001b[39mfit_predict(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m X]\n\u001b[1;32m----> 7\u001b[0m     score \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mmean(hdbscan\u001b[39m.\u001b[39;49mvalidity\u001b[39m.\u001b[39;49mvalidity_index(x, pred) \u001b[39mfor\u001b[39;49;00m x, pred \u001b[39min\u001b[39;49;00m \u001b[39mzip\u001b[39;49m(X, preds))\n\u001b[0;32m      8\u001b[0m     \u001b[39mreturn\u001b[39;00m score \u001b[39mif\u001b[39;00m score \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mfloat\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mnan\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mmean\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3432\u001b[0m, in \u001b[0;36mmean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m   3429\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   3430\u001b[0m         \u001b[39mreturn\u001b[39;00m mean(axis\u001b[39m=\u001b[39maxis, dtype\u001b[39m=\u001b[39mdtype, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m-> 3432\u001b[0m \u001b[39mreturn\u001b[39;00m _methods\u001b[39m.\u001b[39;49m_mean(a, axis\u001b[39m=\u001b[39;49maxis, dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   3433\u001b[0m                       out\u001b[39m=\u001b[39;49mout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\numpy\\core\\_methods.py:192\u001b[0m, in \u001b[0;36m_mean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m    190\u001b[0m         ret \u001b[39m=\u001b[39m ret\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mtype(ret \u001b[39m/\u001b[39m rcount)\n\u001b[0;32m    191\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 192\u001b[0m     ret \u001b[39m=\u001b[39m ret \u001b[39m/\u001b[39;49m rcount\n\u001b[0;32m    194\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'generator' and 'int'"
     ]
    }
   ],
   "source": [
    "DBCV2(hdbscan.HDBSCAN(), all_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Pro\\Stages\\A4 - DVRC\\Work\\Supervised-Learning-using-Unsupervised-Learning-Metrics-in-the-absence-of-Annotated-Data\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters {'min_samples': 3, 'min_cluster_size': 7, 'cluster_selection_method': 'leaf'}\n",
      "DBCV score :0.67558658664357\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_params': {'min_samples': 3,\n",
       "  'min_cluster_size': 7,\n",
       "  'cluster_selection_method': 'leaf'},\n",
       " 'dbcv_score': 0.67558658664357}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HDBScanFinetune(all_v3[0],\n",
    "                min_samples=[3, 5, 7],\n",
    "                min_cluster_size=[7, 10, 20],\n",
    "                cluster_selection_method=['eom','leaf'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
